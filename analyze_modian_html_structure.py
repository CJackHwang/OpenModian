#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åˆ†ææ‘©ç‚¹ç½‘é¡µçš„çœŸå®HTMLç»“æ„
æ‰¾åˆ°å‡†ç¡®çš„æ•°æ®è·å–æ–¹å¼ï¼Œé¿å…åæ¨è®¡ç®—
"""

import sys
import re
import json
from pathlib import Path
from bs4 import BeautifulSoup

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

def analyze_modian_page_structure():
    """åˆ†ææ‘©ç‚¹é¡µé¢çš„çœŸå®HTMLç»“æ„"""
    print("ğŸ” åˆ†ææ‘©ç‚¹ç½‘é¡µçœŸå®HTMLç»“æ„")
    print("=" * 60)
    
    try:
        from spider.config import SpiderConfig
        from spider.utils import NetworkUtils
        
        config = SpiderConfig()
        network_utils = NetworkUtils(config)
        
        # æµ‹è¯•å‡ ä¸ªä¸åŒçš„é¡¹ç›®é¡µé¢
        test_urls = [
            "https://zhongchou.modian.com/item/2250000.html",  # æ¡Œæ¸¸é¡¹ç›®
            "https://zhongchou.modian.com/item/2249000.html",  # å¦ä¸€ä¸ªé¡¹ç›®
        ]
        
        for i, test_url in enumerate(test_urls):
            print(f"\nğŸŒ åˆ†æé¡µé¢ {i+1}: {test_url}")
            print("-" * 50)
            
            # è·å–é¡µé¢å†…å®¹
            html = network_utils.make_request(test_url)
            if not html:
                print(f"âŒ æ— æ³•è·å–é¡µé¢å†…å®¹")
                continue
            
            soup = BeautifulSoup(html, "html.parser")
            
            # åˆ†æé¡µé¢ç»“æ„
            analyze_funding_info_structure(soup, test_url)
            analyze_nav_info_structure(soup, test_url)
            analyze_author_info_structure(soup, test_url)
            
            if i == 0:  # åªåˆ†æç¬¬ä¸€ä¸ªé¡µé¢çš„è¯¦ç»†ç»“æ„
                save_html_structure_analysis(soup, test_url)
        
        return True
        
    except Exception as e:
        print(f"âŒ åˆ†æå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def analyze_funding_info_structure(soup: BeautifulSoup, url: str):
    """åˆ†æä¼—ç­¹ä¿¡æ¯çš„HTMLç»“æ„"""
    print(f"\nğŸ’° ä¼—ç­¹ä¿¡æ¯ç»“æ„åˆ†æ")
    print("-" * 30)
    
    # æŸ¥æ‰¾æ‰€æœ‰åŒ…å«é‡‘é¢çš„å…ƒç´ 
    money_elements = []
    
    # æŸ¥æ‰¾åŒ…å«Â¥ç¬¦å·çš„å…ƒç´ 
    for element in soup.find_all(text=re.compile(r'[Â¥ï¿¥]')):
        parent = element.parent
        if parent:
            text = element.strip()
            if re.search(r'[Â¥ï¿¥]\s*[\d,]+', text):
                money_elements.append({
                    'text': text,
                    'tag': parent.name,
                    'class': parent.get('class', []),
                    'id': parent.get('id', ''),
                    'parent_tag': parent.parent.name if parent.parent else '',
                    'parent_class': parent.parent.get('class', []) if parent.parent else []
                })
    
    print(f"ğŸ“Š æ‰¾åˆ° {len(money_elements)} ä¸ªé‡‘é¢ç›¸å…³å…ƒç´ :")
    for i, elem in enumerate(money_elements[:10]):  # æ˜¾ç¤ºå‰10ä¸ª
        print(f"   {i+1}. æ–‡æœ¬: '{elem['text']}'")
        print(f"      æ ‡ç­¾: <{elem['tag']} class='{elem['class']}' id='{elem['id']}'>")
        print(f"      çˆ¶çº§: <{elem['parent_tag']} class='{elem['parent_class']}'>")
        print()
    
    # æŸ¥æ‰¾åŒ…å«ç™¾åˆ†æ¯”çš„å…ƒç´ 
    percent_elements = []
    for element in soup.find_all(text=re.compile(r'\d+\.?\d*%')):
        parent = element.parent
        if parent:
            text = element.strip()
            percent_elements.append({
                'text': text,
                'tag': parent.name,
                'class': parent.get('class', []),
                'id': parent.get('id', ''),
            })
    
    print(f"ğŸ“ˆ æ‰¾åˆ° {len(percent_elements)} ä¸ªç™¾åˆ†æ¯”å…ƒç´ :")
    for i, elem in enumerate(percent_elements[:5]):  # æ˜¾ç¤ºå‰5ä¸ª
        print(f"   {i+1}. æ–‡æœ¬: '{elem['text']}' æ ‡ç­¾: <{elem['tag']} class='{elem['class']}'>")

def analyze_nav_info_structure(soup: BeautifulSoup, url: str):
    """åˆ†æå¯¼èˆªä¿¡æ¯çš„HTMLç»“æ„"""
    print(f"\nğŸ§­ å¯¼èˆªä¿¡æ¯ç»“æ„åˆ†æ")
    print("-" * 30)
    
    # æŸ¥æ‰¾å¯¼èˆªç›¸å…³çš„å®¹å™¨
    nav_containers = [
        soup.find('div', class_='nav-wrap-inner'),
        soup.find('div', class_='nav-wrap'),
        soup.find('ul', class_='nav-left'),
        soup.find('ul', class_='nav-right'),
    ]
    
    for i, container in enumerate(nav_containers):
        if container:
            print(f"ğŸ“¦ å¯¼èˆªå®¹å™¨ {i+1}: <{container.name} class='{container.get('class')}'>")
            
            # æŸ¥æ‰¾æ‰€æœ‰liå…ƒç´ 
            nav_items = container.find_all('li')
            for j, item in enumerate(nav_items):
                item_text = item.get_text().strip()
                item_class = item.get('class', [])
                
                # æŸ¥æ‰¾æ•°å­—
                numbers = re.findall(r'\d+', item_text)
                
                print(f"   é¡¹ç›® {j+1}: class='{item_class}' æ–‡æœ¬='{item_text}' æ•°å­—={numbers}")
                
                # æŸ¥æ‰¾spanå­å…ƒç´ 
                spans = item.find_all('span')
                for span in spans:
                    span_text = span.get_text().strip()
                    span_class = span.get('class', [])
                    if span_text:
                        print(f"      span: class='{span_class}' æ–‡æœ¬='{span_text}'")
            print()

def analyze_author_info_structure(soup: BeautifulSoup, url: str):
    """åˆ†æä½œè€…ä¿¡æ¯çš„HTMLç»“æ„"""
    print(f"\nğŸ‘¤ ä½œè€…ä¿¡æ¯ç»“æ„åˆ†æ")
    print("-" * 30)
    
    # æŸ¥æ‰¾ä½œè€…ç›¸å…³çš„å®¹å™¨
    author_containers = [
        soup.find('div', class_='sponsor-info'),
        soup.find('div', class_='author-info'),
        soup.find('a', class_='sponsor-link'),
    ]
    
    for i, container in enumerate(author_containers):
        if container:
            print(f"ğŸ“¦ ä½œè€…å®¹å™¨ {i+1}: <{container.name} class='{container.get('class')}'>")
            print(f"   æ–‡æœ¬å†…å®¹: '{container.get_text().strip()[:100]}...'")
            
            # æŸ¥æ‰¾é“¾æ¥
            links = container.find_all('a')
            for link in links:
                href = link.get('href', '')
                text = link.get_text().strip()
                print(f"   é“¾æ¥: href='{href}' æ–‡æœ¬='{text}'")
            
            # æŸ¥æ‰¾å›¾ç‰‡
            images = container.find_all('img')
            for img in images:
                src = img.get('src', '')
                alt = img.get('alt', '')
                print(f"   å›¾ç‰‡: src='{src}' alt='{alt}'")
            print()

def save_html_structure_analysis(soup: BeautifulSoup, url: str):
    """ä¿å­˜HTMLç»“æ„åˆ†æåˆ°æ–‡ä»¶"""
    print(f"\nğŸ’¾ ä¿å­˜HTMLç»“æ„åˆ†æ")
    print("-" * 30)
    
    try:
        # åˆ›å»ºåˆ†ææŠ¥å‘Š
        analysis = {
            "url": url,
            "analysis_time": str(Path(__file__).stat().st_mtime),
            "page_title": soup.title.string if soup.title else "",
            "structure_analysis": {}
        }
        
        # åˆ†æä¸»è¦å®¹å™¨ç»“æ„
        main_containers = [
            ('main-left', soup.find('div', class_='main-left')),
            ('main-right', soup.find('div', class_='main-right')),
            ('center', soup.find('div', class_='center')),
            ('sponsor-info', soup.find('div', class_='sponsor-info')),
            ('nav-wrap-inner', soup.find('div', class_='nav-wrap-inner')),
        ]
        
        for name, container in main_containers:
            if container:
                analysis["structure_analysis"][name] = {
                    "tag": container.name,
                    "classes": container.get('class', []),
                    "id": container.get('id', ''),
                    "children_count": len(container.find_all()),
                    "text_length": len(container.get_text().strip()),
                    "has_forms": len(container.find_all('form')) > 0,
                    "has_scripts": len(container.find_all('script')) > 0,
                }
        
        # æŸ¥æ‰¾æ‰€æœ‰åŒ…å«æ•°æ®å±æ€§çš„å…ƒç´ 
        data_elements = []
        for element in soup.find_all(attrs=lambda x: x and any(k.startswith('data-') for k in x.keys())):
            data_attrs = {k: v for k, v in element.attrs.items() if k.startswith('data-')}
            if data_attrs:
                data_elements.append({
                    "tag": element.name,
                    "classes": element.get('class', []),
                    "data_attributes": data_attrs,
                    "text": element.get_text().strip()[:50]
                })
        
        analysis["data_elements"] = data_elements[:20]  # ä¿å­˜å‰20ä¸ª
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        output_file = Path("data") / "modian_html_structure_analysis.json"
        output_file.parent.mkdir(exist_ok=True)
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(analysis, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… HTMLç»“æ„åˆ†æå·²ä¿å­˜åˆ°: {output_file}")
        
        # åŒæ—¶ä¿å­˜ç®€åŒ–çš„HTMLç»“æ„
        simplified_html = simplify_html_structure(soup)
        html_file = Path("data") / "modian_simplified_structure.html"
        
        with open(html_file, 'w', encoding='utf-8') as f:
            f.write(simplified_html)
        
        print(f"âœ… ç®€åŒ–HTMLç»“æ„å·²ä¿å­˜åˆ°: {html_file}")
        
    except Exception as e:
        print(f"âŒ ä¿å­˜åˆ†æå¤±è´¥: {e}")

def simplify_html_structure(soup: BeautifulSoup) -> str:
    """ç®€åŒ–HTMLç»“æ„ï¼Œåªä¿ç•™å…³é”®ä¿¡æ¯"""
    
    # ç§»é™¤ä¸å¿…è¦çš„å…ƒç´ 
    for element in soup(['script', 'style', 'meta', 'link']):
        element.decompose()
    
    # ç®€åŒ–æ–‡æœ¬å†…å®¹
    for element in soup.find_all(text=True):
        if len(element.strip()) > 100:
            element.replace_with(element.strip()[:100] + "...")
    
    # ç§»é™¤å¤§éƒ¨åˆ†å±æ€§ï¼Œåªä¿ç•™classå’Œid
    for element in soup.find_all():
        attrs_to_keep = {}
        if element.get('class'):
            attrs_to_keep['class'] = element.get('class')
        if element.get('id'):
            attrs_to_keep['id'] = element.get('id')
        if element.get('href'):
            attrs_to_keep['href'] = element.get('href')
        if element.get('src'):
            attrs_to_keep['src'] = element.get('src')
        
        element.attrs = attrs_to_keep
    
    return str(soup)

def extract_real_data_patterns():
    """æå–çœŸå®çš„æ•°æ®æ¨¡å¼"""
    print(f"\nğŸ¯ æå–çœŸå®æ•°æ®æ¨¡å¼")
    print("-" * 30)
    
    try:
        from spider.config import SpiderConfig
        from spider.utils import NetworkUtils
        
        config = SpiderConfig()
        network_utils = NetworkUtils(config)
        
        test_url = "https://zhongchou.modian.com/item/2250000.html"
        html = network_utils.make_request(test_url)
        
        if not html:
            print("âŒ æ— æ³•è·å–é¡µé¢å†…å®¹")
            return False
        
        soup = BeautifulSoup(html, "html.parser")
        page_text = soup.get_text()
        
        print("ğŸ“ é¡µé¢æ–‡æœ¬ä¸­çš„å…³é”®æ•°æ®æ¨¡å¼:")
        
        # æŸ¥æ‰¾æ‰€æœ‰æ•°å­—æ¨¡å¼
        patterns = {
            "é‡‘é¢æ¨¡å¼": [
                r'å·²ç­¹[Â¥ï¿¥]\s*([0-9,]+)',
                r'ç›®æ ‡é‡‘é¢[Â¥ï¿¥]\s*([0-9,]+)',
                r'[Â¥ï¿¥]\s*([0-9,]+)',
            ],
            "ç™¾åˆ†æ¯”æ¨¡å¼": [
                r'([0-9.]+)%',
                r'å®Œæˆ.*?([0-9.]+)%',
            ],
            "äººæ•°æ¨¡å¼": [
                r'(\d+)\s*äºº',
                r'(\d+)\s*æ”¯æŒè€…',
                r'æ”¯æŒè€…\s*(\d+)',
            ],
            "è®¡æ•°æ¨¡å¼": [
                r'æ›´æ–°\s*(\d+)',
                r'è¯„è®º\s*(\d+)',
                r'æ”¶è—\s*(\d+)',
            ]
        }
        
        for pattern_type, pattern_list in patterns.items():
            print(f"\n{pattern_type}:")
            for pattern in pattern_list:
                matches = re.findall(pattern, page_text)
                if matches:
                    print(f"   âœ… {pattern} -> {matches}")
                else:
                    print(f"   âŒ {pattern} -> æ— åŒ¹é…")
        
        return True
        
    except Exception as e:
        print(f"âŒ æå–æ•°æ®æ¨¡å¼å¤±è´¥: {e}")
        return False

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ æ‘©ç‚¹ç½‘é¡µHTMLç»“æ„åˆ†æå·¥å…·")
    print("ç›®æ ‡ï¼šæ‰¾åˆ°å‡†ç¡®çš„æ•°æ®è·å–æ–¹å¼ï¼Œé¿å…åæ¨è®¡ç®—")
    print("=" * 80)
    
    tasks = [
        ("é¡µé¢ç»“æ„åˆ†æ", analyze_modian_page_structure),
        ("æ•°æ®æ¨¡å¼æå–", extract_real_data_patterns),
    ]
    
    results = []
    
    for task_name, task_func in tasks:
        try:
            print(f"\n{'='*20} {task_name} {'='*20}")
            result = task_func()
            results.append((task_name, result))
            status = "âœ… æˆåŠŸ" if result else "âŒ å¤±è´¥"
            print(f"\n{task_name}: {status}")
        except Exception as e:
            print(f"\n{task_name}: âŒ å¼‚å¸¸ - {e}")
            results.append((task_name, False))
    
    # æ€»ç»“
    print(f"\n" + "=" * 80)
    print(f"ğŸ“Š HTMLç»“æ„åˆ†ææ€»ç»“")
    print("=" * 80)
    
    passed = sum(1 for _, result in results if result)
    total = len(results)
    
    for task_name, result in results:
        status = "âœ…" if result else "âŒ"
        print(f"{status} {task_name}")
    
    print(f"\næˆåŠŸç‡: {passed}/{total} ({passed/total*100:.1f}%)")
    
    if passed == total:
        print(f"ğŸ‰ HTMLç»“æ„åˆ†æå®Œæˆï¼")
        print(f"âœ… å·²è¯†åˆ«çœŸå®çš„æ•°æ®è·å–æ–¹å¼")
        print(f"âœ… å¯ä»¥é¿å…åæ¨è®¡ç®—")
        print(f"âœ… åˆ†ææŠ¥å‘Šå·²ä¿å­˜åˆ°dataç›®å½•")
        return True
    else:
        print(f"âš ï¸  éƒ¨åˆ†åˆ†æä»»åŠ¡éœ€è¦è¿›ä¸€æ­¥å®Œå–„")
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
