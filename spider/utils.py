# -*- coding: utf-8 -*-
"""
çˆ¬è™«å·¥å…·ç±»
æä¾›ç½‘ç»œè¯·æ±‚ã€æ•°æ®å¤„ç†ã€ç¼“å­˜ç­‰é€šç”¨åŠŸèƒ½
"""

import os
import re
import json
import time
import random
import hashlib
import pickle
import socket
import urllib.request
import urllib.error
import html
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple
from pathlib import Path
import requests
from bs4 import BeautifulSoup

from .config import SpiderConfig, RegexPatterns


class NetworkUtils:
    """ç½‘ç»œè¯·æ±‚å·¥å…·ç±»"""
    
    def __init__(self, config: SpiderConfig):
        self.config = config
        self.session = requests.Session()
        self.request_count = 0
        self.last_request_time = 0
        
    def get_headers(self, header_type: str = "desktop") -> Dict[str, str]:
        """è·å–è¯·æ±‚å¤´"""
        headers = self.config.REQUEST_HEADERS.get(header_type, {}).copy()
        
        # æ·»åŠ æ—¶é—´æˆ³é˜²æ­¢ç¼“å­˜
        if header_type == "mobile":
            headers["Timestamp"] = str(int(time.time()))
            
        return headers
    
    def _rate_limit(self):
        """é€Ÿç‡é™åˆ¶"""
        current_time = time.time()
        time_since_last = current_time - self.last_request_time
        
        min_delay, max_delay = self.config.REQUEST_DELAY
        required_delay = random.uniform(min_delay, max_delay)
        
        if time_since_last < required_delay:
            sleep_time = required_delay - time_since_last
            time.sleep(sleep_time)
        
        self.last_request_time = time.time()
        self.request_count += 1
    
    def make_request(self, url: str, method: str = "GET",
                    header_type: str = "desktop", use_urllib: bool = False, **kwargs) -> Optional[str]:
        """å‘é€HTTPè¯·æ±‚ - èåˆmain.pyçš„åŒé‡è¯·æ±‚ç­–ç•¥"""
        self._rate_limit()

        # ğŸ”§ èåˆmain.pyçš„åŒé‡è¯·æ±‚æ–¹æ³•
        if use_urllib or header_type == "mobile":
            return self._make_urllib_request(url, header_type)
        else:
            return self._make_requests_request(url, method, header_type, **kwargs)

    def _make_requests_request(self, url: str, method: str, header_type: str, **kwargs) -> Optional[str]:
        """ä½¿ç”¨requestsåº“å‘é€è¯·æ±‚"""
        headers = self.get_headers(header_type)
        timeout = random.randint(*self.config.TIMEOUT_RANGE)

        for attempt in range(self.config.MAX_RETRIES):
            try:
                if method.upper() == "GET":
                    response = self.session.get(
                        url, headers=headers, timeout=timeout, verify=False, **kwargs
                    )
                else:
                    response = self.session.request(
                        method, url, headers=headers, timeout=timeout, verify=False, **kwargs
                    )

                response.raise_for_status()
                response.encoding = 'utf-8'  # ğŸ”§ èåˆmain.pyçš„ç¼–ç è®¾ç½®
                return response.text

            except (requests.RequestException, socket.timeout,
                   urllib.error.URLError, ConnectionResetError) as e:
                print(f"Requestsè¯·æ±‚å¤±è´¥ (å°è¯• {attempt + 1}/{self.config.MAX_RETRIES}): {e}")
                print(f"URL: {url}")

                if attempt < self.config.MAX_RETRIES - 1:
                    # ğŸ”§ èåˆmain.pyçš„æŒ‡æ•°é€€é¿ç­–ç•¥
                    delay = self.config.RETRY_DELAY[0] * (attempt + 1) * 2
                    time.sleep(delay)
                else:
                    print(f"Requestsæ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†: {url}")
                    return None

        return None

    def _make_urllib_request(self, url: str, header_type: str = "desktop") -> Optional[str]:
        """ä½¿ç”¨urllibå‘é€è¯·æ±‚ - èåˆmain.pyçš„å®ç°"""
        import urllib.request
        import urllib.error
        import ssl

        # ğŸ”§ èåˆmain.pyçš„SSLä¸Šä¸‹æ–‡å¤„ç†
        ssl_context = ssl.create_default_context()
        ssl_context.check_hostname = False
        ssl_context.verify_mode = ssl.CERT_NONE

        headers = self.get_headers(header_type)
        timeout_range = self.config.TIMEOUT_RANGE

        for attempt in range(self.config.MAX_RETRIES):
            try:
                timeout = random.randint(*timeout_range)
                request = urllib.request.Request(url, headers=headers)
                response = urllib.request.urlopen(request, timeout=timeout, context=ssl_context)
                html = response.read().decode("utf-8")
                return html

            except (urllib.error.URLError, ConnectionResetError, socket.timeout, ssl.SSLError) as e:
                print(f"Urllibè¯·æ±‚å¤±è´¥ (å°è¯• {attempt + 1}/{self.config.MAX_RETRIES}): {e}")
                print(f"URL: {url}")

                if attempt < self.config.MAX_RETRIES - 1:
                    # ğŸ”§ èåˆmain.pyçš„æŒ‡æ•°é€€é¿ç­–ç•¥
                    wait_time = (attempt + 1) * 2
                    time.sleep(wait_time)
                else:
                    print(f"Urllibæ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†: {url}")
                    return None

        return None
    
    def make_api_request(self, endpoint: str, params: Dict = None) -> Optional[Dict]:
        """å‘é€APIè¯·æ±‚ - ğŸ”§ ä¿®å¤ï¼šç¡®ä¿APIè¯·æ±‚ä¹Ÿéµå¾ªå»¶è¿Ÿæ§åˆ¶"""
        # ğŸ”§ å…³é”®ä¿®å¤ï¼šAPIè¯·æ±‚ä¹Ÿè¦éµå¾ªé€Ÿç‡é™åˆ¶
        self._rate_limit()

        url = self.config.get_api_url(endpoint)

        try:
            response = self.session.get(
                url,
                params=params or {},
                headers=self.get_headers("mobile"),
                timeout=random.randint(*self.config.TIMEOUT_RANGE)
            )
            response.raise_for_status()
            return response.json()
        except Exception as e:
            print(f"APIè¯·æ±‚å¤±è´¥: {e}")
            return None
    
    def get_request_stats(self) -> Dict[str, Any]:
        """è·å–è¯·æ±‚ç»Ÿè®¡ä¿¡æ¯"""
        return {
            "total_requests": self.request_count,
            "last_request_time": self.last_request_time,
            "session_cookies": len(self.session.cookies)
        }


class DataUtils:
    """æ•°æ®å¤„ç†å·¥å…·ç±»"""
    
    @staticmethod
    def extract_number(text: str, default: str = "0") -> str:
        """æå–æ•°å­—"""
        if not text:
            return default
            
        # ç§»é™¤å¸¸è§çš„éæ•°å­—å­—ç¬¦
        cleaned = re.sub(r'[^\d.]', '', str(text))
        
        # éªŒè¯æ˜¯å¦ä¸ºæœ‰æ•ˆæ•°å­—
        try:
            float(cleaned) if '.' in cleaned else int(cleaned)
            return cleaned
        except ValueError:
            return default
    
    @staticmethod
    def extract_percentage(text: str, default: str = "0") -> str:
        """æå–ç™¾åˆ†æ¯”"""
        if not text:
            return default
            
        match = re.search(RegexPatterns.PERCENTAGE_PATTERN, str(text))
        return match.group(1) if match else default
    
    @staticmethod
    def extract_project_id(url: str) -> str:
        """ä»URLæå–é¡¹ç›®ID"""
        if not url:
            return ""
            
        match = re.search(RegexPatterns.PROJECT_ID_PATTERN, url)
        return match.group(1) if match else ""
    
    @staticmethod
    def extract_user_id(url: str) -> str:
        """ä»URLæå–ç”¨æˆ·ID"""
        if not url:
            return ""
            
        match = re.search(RegexPatterns.USER_ID_PATTERN, url)
        return match.group(1) if match else ""
    
    @staticmethod
    def clean_text(text: str, max_length: int = None) -> str:
        """æ¸…ç†æ–‡æœ¬"""
        if not text:
            return ""
            
        # ç§»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
        cleaned = re.sub(r'\s+', ' ', str(text)).strip()
        
        # é™åˆ¶é•¿åº¦
        if max_length and len(cleaned) > max_length:
            cleaned = cleaned[:max_length] + "..."
            
        return cleaned
    
    @staticmethod
    def parse_time(time_str: str) -> str:
        """è§£ææ—¶é—´å­—ç¬¦ä¸²"""
        if not time_str or time_str in ["none", "åˆ›æ„ä¸­", "é¢„çƒ­ä¸­", "ä¼—ç­¹ä¸­"]:
            return time_str
            
        # å°è¯•ä¸åŒçš„æ—¶é—´æ ¼å¼
        for pattern in RegexPatterns.TIME_PATTERNS:
            match = re.search(pattern, str(time_str))
            if match:
                return match.group(0)
        
        return time_str
    
    @staticmethod
    def validate_url(url: str) -> str:
        """éªŒè¯å’Œä¿®æ­£URL"""
        if not url or url == "none":
            return "none"
            
        url = str(url).strip()
        
        # è¡¥å…¨ç›¸å¯¹URL
        if url.startswith("/"):
            url = SpiderConfig.BASE_URL + url
        elif not url.startswith("http"):
            url = "https://" + url
            
        return url
    
    @staticmethod
    def safe_json_loads(json_str: str, default: Any = None) -> Any:
        """å®‰å…¨çš„JSONè§£æ"""
        try:
            return json.loads(json_str)
        except (json.JSONDecodeError, TypeError):
            return default or {}
    
    @staticmethod
    def format_money(amount: str) -> str:
        """æ ¼å¼åŒ–é‡‘é¢"""
        if not amount or amount == "0":
            return "0"

        # ç§»é™¤è´§å¸ç¬¦å·å’Œé€—å·
        cleaned = re.sub(r'[ï¿¥Â¥,]', '', str(amount))

        try:
            # è½¬æ¢ä¸ºæµ®ç‚¹æ•°å†è½¬å›å­—ç¬¦ä¸²ï¼Œç¡®ä¿æ ¼å¼ä¸€è‡´
            return str(float(cleaned))
        except ValueError:
            return "0"

    @staticmethod
    def fix_encoding(text: str) -> str:
        """ä¿®å¤ç¼–ç é—®é¢˜"""
        if not text:
            return ""

        try:
            # å°è¯•ä¿®å¤å¸¸è§çš„ç¼–ç é—®é¢˜
            if isinstance(text, bytes):
                text = text.decode('utf-8', errors='ignore')

            # æ¸…ç†ç‰¹æ®Šå­—ç¬¦
            text = re.sub(r'[\x00-\x1f\x7f-\x9f]', '', str(text))

            return text.strip()
        except Exception:
            return str(text).strip()

    @staticmethod
    def clean_reward_text(text: str) -> str:
        """æ¸…ç†å›æŠ¥æ–‡æœ¬"""
        if not text:
            return "none"

        # ç§»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
        cleaned = re.sub(r'\s+', ' ', str(text)).strip()

        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦
        cleaned = re.sub(r'[^\w\s\u4e00-\u9fffï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š""''ï¼ˆï¼‰ã€ã€‘ã€Šã€‹ã€]', '', cleaned)

        # é™åˆ¶é•¿åº¦
        if len(cleaned) > 200:
            cleaned = cleaned[:200] + "..."

        return cleaned if cleaned else "none"

    @staticmethod
    def fix_encoding(text: str) -> str:
        """ä¿®å¤ç¼–ç é—®é¢˜"""
        if not text:
            return text

        try:
            # æ–¹æ³•1: å¤„ç†å¸¸è§çš„ç¼–ç é”™è¯¯æ¨¡å¼
            encoding_map = {
                'Ã§Â¥Ã¥Â¥Ã¦Â¨Ã¤ÂºÂº': 'ç¥å¥‡æœ¨äºº',
                'Ã¤Â¸Ã¦ÂµÂ·Ã¤Â¼Ã¦Â©Ã¦Ã¥': 'ä¸Šæµ·ä¼Ÿæ©æ–‡åŒ–',
                'Ã©Â¢Ã¨Â®Â¡Ã¥Ã¦Â¥Ã¥Ã¦Â¾Ã¦Â¶Ã©Â´': 'é¢„è®¡å‘è´§å‘æ”¾æ—¶é—´',
                'Ã¤Â¸\\x8dÃ©\\x9c\\x80Ã¨Â¦\\x81Ã¥\\x9b\\x9eÃ¦\\x8aÂ¥Ã¯Â¼\\x8cÃ¦\\x88\\x91Ã¥\\x': 'ä¸éœ€è¦å›æŠ¥ï¼Œæˆ‘åªæ˜¯æ”¯æŒæœ‰æ¢¦æƒ³çš„äººã€‚',
                'Ã§Â®Ã¦ Ã©Ã©Â¢': 'ç›®æ ‡é‡‘é¢',
                'Ã¥Â·Â²Ã§Â­Â¹': 'å·²ç­¹',
                'Ã¦Â¯Ã¦Ã¤ÂºÂºÃ¦Â°': 'æ”¯æŒäººæ•°',
                'Ã¥Â©Ã¤Â½Ã¦Â¶Ã©Â´': 'å‰©ä½™æ—¶é—´'
            }

            # æ£€æŸ¥æ˜¯å¦æœ‰ç›´æ¥æ˜ å°„
            if text in encoding_map:
                return encoding_map[text]

            # æ–¹æ³•2: HTMLè§£ç 
            decoded = html.unescape(text)
            if decoded != text:
                return decoded

            # æ–¹æ³•3: å¤„ç†Unicodeè½¬ä¹‰åºåˆ—
            if '\\x' in text:
                try:
                    # å°†\\xè½¬ä¹‰åºåˆ—è½¬æ¢ä¸ºå®é™…å­—ç¬¦
                    fixed = text.encode().decode('unicode_escape')
                    return fixed
                except:
                    pass

            # æ–¹æ³•4: å¤„ç†UTF-8ç¼–ç é—®é¢˜
            if any(ord(c) > 127 for c in text):
                try:
                    # å°è¯•é‡æ–°ç¼–ç 
                    fixed = text.encode('latin1').decode('utf-8')
                    return fixed
                except:
                    pass

            return text
        except Exception:
            return text

    @staticmethod
    def clean_reward_text(text: str) -> str:
        """æ¸…ç†å›æŠ¥æ–‡æœ¬"""
        if not text:
            return "none"

        # ä¿®å¤ç¼–ç 
        fixed = DataUtils.fix_encoding(text)

        # æ¸…ç†æ–‡æœ¬
        cleaned = re.sub(r'\s+', ' ', fixed).strip()

        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦
        cleaned = re.sub(r'[^\w\s\u4e00-\u9fff.,!?()ï¼ˆï¼‰ï¼Œã€‚ï¼ï¼Ÿ]', '', cleaned)

        return cleaned if cleaned else "none"


class CacheUtils:
    """ç¼“å­˜å·¥å…·ç±»"""
    
    def __init__(self, config: SpiderConfig):
        self.config = config
        self.cache_dir = Path(config.CACHE_DIR)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
    def _get_cache_key(self, url: str) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        return hashlib.md5(url.encode()).hexdigest()
    
    def _get_cache_path(self, cache_key: str) -> Path:
        """è·å–ç¼“å­˜æ–‡ä»¶è·¯å¾„"""
        return self.cache_dir / f"{cache_key}.cache"
    
    def _is_cache_valid(self, cache_path: Path) -> bool:
        """æ£€æŸ¥ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ"""
        if not cache_path.exists():
            return False
            
        # æ£€æŸ¥ç¼“å­˜æ—¶é—´
        cache_time = datetime.fromtimestamp(cache_path.stat().st_mtime)
        expire_time = cache_time + timedelta(hours=self.config.CACHE_EXPIRE_HOURS)
        
        return datetime.now() < expire_time
    
    def get_cache(self, url: str) -> Optional[str]:
        """è·å–ç¼“å­˜å†…å®¹"""
        if not self.config.ENABLE_CACHE:
            return None
            
        cache_key = self._get_cache_key(url)
        cache_path = self._get_cache_path(cache_key)
        
        if self._is_cache_valid(cache_path):
            try:
                with open(cache_path, 'r', encoding='utf-8') as f:
                    return f.read()
            except Exception as e:
                print(f"è¯»å–ç¼“å­˜å¤±è´¥: {e}")
                
        return None
    
    def set_cache(self, url: str, content: str):
        """è®¾ç½®ç¼“å­˜å†…å®¹"""
        if not self.config.ENABLE_CACHE:
            return
            
        cache_key = self._get_cache_key(url)
        cache_path = self._get_cache_path(cache_key)
        
        try:
            with open(cache_path, 'w', encoding='utf-8') as f:
                f.write(content)
        except Exception as e:
            print(f"å†™å…¥ç¼“å­˜å¤±è´¥: {e}")
    
    def clear_cache(self):
        """æ¸…ç©ºç¼“å­˜"""
        try:
            for cache_file in self.cache_dir.glob("*.cache"):
                cache_file.unlink()
            print("ç¼“å­˜å·²æ¸…ç©º")
        except Exception as e:
            print(f"æ¸…ç©ºç¼“å­˜å¤±è´¥: {e}")
    
    def get_cache_stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        cache_files = list(self.cache_dir.glob("*.cache"))
        total_size = sum(f.stat().st_size for f in cache_files)
        
        return {
            "cache_count": len(cache_files),
            "total_size_mb": round(total_size / 1024 / 1024, 2),
            "cache_dir": str(self.cache_dir)
        }


class FileUtils:
    """æ–‡ä»¶å¤„ç†å·¥å…·ç±»"""
    
    @staticmethod
    def ensure_directory(path: str):
        """ç¡®ä¿ç›®å½•å­˜åœ¨"""
        Path(path).mkdir(parents=True, exist_ok=True)
    
    @staticmethod
    def backup_file(file_path: str) -> str:
        """å¤‡ä»½æ–‡ä»¶"""
        if not os.path.exists(file_path):
            return ""
            
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_path = f"{file_path}.backup_{timestamp}"
        
        try:
            import shutil
            shutil.copy2(file_path, backup_path)
            return backup_path
        except Exception as e:
            print(f"å¤‡ä»½æ–‡ä»¶å¤±è´¥: {e}")
            return ""
    
    @staticmethod
    def get_file_size(file_path: str) -> int:
        """è·å–æ–‡ä»¶å¤§å°ï¼ˆå­—èŠ‚ï¼‰"""
        try:
            return os.path.getsize(file_path)
        except OSError:
            return 0
    
    @staticmethod
    def safe_filename(filename: str) -> str:
        """ç”Ÿæˆå®‰å…¨çš„æ–‡ä»¶å"""
        # ç§»é™¤æˆ–æ›¿æ¢ä¸å®‰å…¨çš„å­—ç¬¦
        safe_chars = re.sub(r'[<>:"/\\|?*]', '_', filename)
        return safe_chars[:255]  # é™åˆ¶æ–‡ä»¶åé•¿åº¦


class ParserUtils:
    """è§£æå·¥å…·ç±»"""
    
    @staticmethod
    def safe_find(soup: BeautifulSoup, *args, **kwargs) -> Optional[Any]:
        """å®‰å…¨çš„å…ƒç´ æŸ¥æ‰¾"""
        try:
            return soup.find(*args, **kwargs)
        except Exception:
            return None
    
    @staticmethod
    def safe_find_all(soup: BeautifulSoup, *args, **kwargs) -> List[Any]:
        """å®‰å…¨çš„å¤šå…ƒç´ æŸ¥æ‰¾"""
        try:
            return soup.find_all(*args, **kwargs)
        except Exception:
            return []
    
    @staticmethod
    def safe_get_text(element, default: str = "") -> str:
        """å®‰å…¨è·å–å…ƒç´ æ–‡æœ¬"""
        try:
            return element.get_text(strip=True) if element else default
        except Exception:
            return default
    
    @staticmethod
    def safe_get_attr(element, attr: str, default: str = "") -> str:
        """å®‰å…¨è·å–å…ƒç´ å±æ€§"""
        try:
            return element.get(attr, default) if element else default
        except Exception:
            return default
