# -*- coding: utf-8 -*-
"""
ç½‘ç»œè¯·æ±‚ä¼˜åŒ–æ¨¡å—
æä¾›è¿æ¥æ± ã€è¯·æ±‚ç¼“å­˜ã€æ™ºèƒ½é‡è¯•ç­‰ç½‘ç»œä¼˜åŒ–åŠŸèƒ½
"""

import time
import threading
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from urllib3.poolmanager import PoolManager
import hashlib
import json
from typing import Dict, Any, Optional, List, Tuple
from dataclasses import dataclass
import pickle
import os
from pathlib import Path


@dataclass
class NetworkStats:
    """ç½‘ç»œç»Ÿè®¡ä¿¡æ¯"""
    total_requests: int = 0
    successful_requests: int = 0
    failed_requests: int = 0
    cached_requests: int = 0
    retry_requests: int = 0
    total_bytes_downloaded: int = 0
    average_response_time: float = 0.0
    connection_pool_hits: int = 0
    connection_pool_misses: int = 0


class SmartCache:
    """æ™ºèƒ½ç¼“å­˜ç³»ç»Ÿ"""
    
    def __init__(self, cache_dir: str = "data/cache/network", max_size_mb: int = 500):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        self.max_size_mb = max_size_mb
        self.cache_index = {}
        self._lock = threading.RLock()
        
        # åŠ è½½ç¼“å­˜ç´¢å¼•
        self._load_cache_index()
    
    def _load_cache_index(self):
        """åŠ è½½ç¼“å­˜ç´¢å¼•"""
        index_file = self.cache_dir / "cache_index.json"
        try:
            if index_file.exists():
                with open(index_file, 'r', encoding='utf-8') as f:
                    self.cache_index = json.load(f)
        except Exception as e:
            print(f"âš ï¸ åŠ è½½ç¼“å­˜ç´¢å¼•å¤±è´¥: {e}")
            self.cache_index = {}
    
    def _save_cache_index(self):
        """ä¿å­˜ç¼“å­˜ç´¢å¼•"""
        index_file = self.cache_dir / "cache_index.json"
        try:
            with open(index_file, 'w', encoding='utf-8') as f:
                json.dump(self.cache_index, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜ç¼“å­˜ç´¢å¼•å¤±è´¥: {e}")
    
    def _get_cache_key(self, url: str, headers: Dict = None, params: Dict = None) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        cache_data = {
            'url': url,
            'headers': headers or {},
            'params': params or {}
        }
        cache_str = json.dumps(cache_data, sort_keys=True)
        return hashlib.md5(cache_str.encode()).hexdigest()
    
    def get(self, url: str, headers: Dict = None, params: Dict = None, max_age: int = 3600) -> Optional[Dict]:
        """è·å–ç¼“å­˜"""
        with self._lock:
            cache_key = self._get_cache_key(url, headers, params)
            
            if cache_key not in self.cache_index:
                return None
            
            cache_info = self.cache_index[cache_key]
            
            # æ£€æŸ¥æ˜¯å¦è¿‡æœŸ
            if time.time() - cache_info['timestamp'] > max_age:
                self._remove_cache_entry(cache_key)
                return None
            
            # è¯»å–ç¼“å­˜æ–‡ä»¶
            cache_file = self.cache_dir / f"{cache_key}.pkl"
            try:
                if cache_file.exists():
                    with open(cache_file, 'rb') as f:
                        return pickle.load(f)
            except Exception as e:
                print(f"âš ï¸ è¯»å–ç¼“å­˜å¤±è´¥: {e}")
                self._remove_cache_entry(cache_key)
            
            return None
    
    def set(self, url: str, response_data: Dict, headers: Dict = None, params: Dict = None):
        """è®¾ç½®ç¼“å­˜"""
        with self._lock:
            cache_key = self._get_cache_key(url, headers, params)
            cache_file = self.cache_dir / f"{cache_key}.pkl"
            
            try:
                # ä¿å­˜ç¼“å­˜æ•°æ®
                with open(cache_file, 'wb') as f:
                    pickle.dump(response_data, f)
                
                # æ›´æ–°ç´¢å¼•
                self.cache_index[cache_key] = {
                    'url': url,
                    'timestamp': time.time(),
                    'size': cache_file.stat().st_size
                }
                
                # æ£€æŸ¥ç¼“å­˜å¤§å°
                self._cleanup_if_needed()
                
                # ä¿å­˜ç´¢å¼•
                self._save_cache_index()
                
            except Exception as e:
                print(f"âš ï¸ ä¿å­˜ç¼“å­˜å¤±è´¥: {e}")
    
    def _remove_cache_entry(self, cache_key: str):
        """åˆ é™¤ç¼“å­˜æ¡ç›®"""
        try:
            cache_file = self.cache_dir / f"{cache_key}.pkl"
            if cache_file.exists():
                cache_file.unlink()
            
            if cache_key in self.cache_index:
                del self.cache_index[cache_key]
        except Exception as e:
            print(f"âš ï¸ åˆ é™¤ç¼“å­˜æ¡ç›®å¤±è´¥: {e}")
    
    def _cleanup_if_needed(self):
        """æ ¹æ®éœ€è¦æ¸…ç†ç¼“å­˜"""
        total_size = sum(info.get('size', 0) for info in self.cache_index.values())
        max_size_bytes = self.max_size_mb * 1024 * 1024
        
        if total_size > max_size_bytes:
            print(f"ğŸ§¹ ç¼“å­˜å¤§å°è¶…é™ ({total_size/1024/1024:.1f}MB)ï¼Œå¼€å§‹æ¸…ç†...")
            
            # æŒ‰æ—¶é—´æˆ³æ’åºï¼Œåˆ é™¤æœ€æ—§çš„ç¼“å­˜
            sorted_entries = sorted(
                self.cache_index.items(),
                key=lambda x: x[1]['timestamp']
            )
            
            for cache_key, _ in sorted_entries:
                self._remove_cache_entry(cache_key)
                total_size = sum(info.get('size', 0) for info in self.cache_index.values())
                
                if total_size <= max_size_bytes * 0.8:  # æ¸…ç†åˆ°80%
                    break
            
            print(f"ğŸ§¹ ç¼“å­˜æ¸…ç†å®Œæˆï¼Œå½“å‰å¤§å°: {total_size/1024/1024:.1f}MB")


class OptimizedHTTPAdapter(HTTPAdapter):
    """ä¼˜åŒ–çš„HTTPé€‚é…å™¨"""
    
    def __init__(self, pool_connections=20, pool_maxsize=50, max_retries=3, **kwargs):
        self.pool_connections = pool_connections
        self.pool_maxsize = pool_maxsize
        
        # é…ç½®é‡è¯•ç­–ç•¥
        retry_strategy = Retry(
            total=max_retries,
            backoff_factor=1,
            status_forcelist=[429, 500, 502, 503, 504],
            allowed_methods=["HEAD", "GET", "OPTIONS"]
        )
        
        super().__init__(max_retries=retry_strategy, **kwargs)
    
    def init_poolmanager(self, *args, **kwargs):
        """åˆå§‹åŒ–è¿æ¥æ± ç®¡ç†å™¨"""
        kwargs['num_pools'] = self.pool_connections
        kwargs['maxsize'] = self.pool_maxsize
        kwargs['block'] = False  # éé˜»å¡æ¨¡å¼
        return super().init_poolmanager(*args, **kwargs)


class NetworkOptimizer:
    """ç½‘ç»œä¼˜åŒ–å™¨"""
    
    def __init__(self, config=None):
        self.config = config
        self.stats = NetworkStats()
        self.cache = SmartCache()
        self._lock = threading.RLock()
        
        # åˆ›å»ºä¼˜åŒ–çš„ä¼šè¯
        self.session = self._create_optimized_session()
        
        # è¯·æ±‚å†å²ï¼ˆç”¨äºåˆ†æï¼‰
        self.request_history = []
        self.max_history_size = 1000
    
    def _create_optimized_session(self) -> requests.Session:
        """åˆ›å»ºä¼˜åŒ–çš„è¯·æ±‚ä¼šè¯"""
        session = requests.Session()
        
        # é…ç½®è¿æ¥æ± é€‚é…å™¨
        http_adapter = OptimizedHTTPAdapter(
            pool_connections=20,
            pool_maxsize=50,
            max_retries=3
        )
        https_adapter = OptimizedHTTPAdapter(
            pool_connections=20,
            pool_maxsize=50,
            max_retries=3
        )
        
        session.mount("http://", http_adapter)
        session.mount("https://", https_adapter)
        
        # è®¾ç½®é»˜è®¤è¶…æ—¶
        session.timeout = 30
        
        # è®¾ç½®é»˜è®¤å¤´éƒ¨
        session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        })
        
        return session
    
    def make_request(self, url: str, method: str = 'GET', use_cache: bool = True, 
                    cache_max_age: int = 3600, **kwargs) -> Optional[requests.Response]:
        """å‘èµ·ä¼˜åŒ–çš„ç½‘ç»œè¯·æ±‚"""
        start_time = time.time()
        
        with self._lock:
            self.stats.total_requests += 1
        
        # å°è¯•ä»ç¼“å­˜è·å–
        if use_cache and method.upper() == 'GET':
            cached_data = self.cache.get(
                url, 
                headers=kwargs.get('headers'),
                params=kwargs.get('params'),
                max_age=cache_max_age
            )
            
            if cached_data:
                with self._lock:
                    self.stats.cached_requests += 1
                    self.stats.successful_requests += 1
                
                # åˆ›å»ºæ¨¡æ‹Ÿå“åº”å¯¹è±¡
                response = requests.Response()
                response._content = cached_data['content'].encode() if isinstance(cached_data['content'], str) else cached_data['content']
                response.status_code = cached_data['status_code']
                response.headers.update(cached_data['headers'])
                response.url = url
                
                return response
        
        # å‘èµ·å®é™…è¯·æ±‚
        try:
            response = self.session.request(method, url, **kwargs)
            response.raise_for_status()
            
            request_time = time.time() - start_time
            
            with self._lock:
                self.stats.successful_requests += 1
                self.stats.total_bytes_downloaded += len(response.content)
                self._update_average_response_time(request_time)
            
            # ç¼“å­˜GETè¯·æ±‚çš„å“åº”
            if use_cache and method.upper() == 'GET' and response.status_code == 200:
                cache_data = {
                    'content': response.text,
                    'status_code': response.status_code,
                    'headers': dict(response.headers)
                }
                self.cache.set(url, cache_data, kwargs.get('headers'), kwargs.get('params'))
            
            # è®°å½•è¯·æ±‚å†å²
            self._record_request_history(url, method, request_time, True)
            
            return response
            
        except Exception as e:
            request_time = time.time() - start_time
            
            with self._lock:
                self.stats.failed_requests += 1
            
            self._record_request_history(url, method, request_time, False, str(e))
            print(f"âŒ ç½‘ç»œè¯·æ±‚å¤±è´¥: {url} - {e}")
            return None
    
    def _update_average_response_time(self, request_time: float):
        """æ›´æ–°å¹³å‡å“åº”æ—¶é—´"""
        if self.stats.successful_requests == 1:
            self.stats.average_response_time = request_time
        else:
            # ä½¿ç”¨ç§»åŠ¨å¹³å‡
            alpha = 0.1  # å¹³æ»‘å› å­
            self.stats.average_response_time = (
                alpha * request_time + 
                (1 - alpha) * self.stats.average_response_time
            )
    
    def _record_request_history(self, url: str, method: str, request_time: float, 
                               success: bool, error: str = None):
        """è®°å½•è¯·æ±‚å†å²"""
        history_entry = {
            'timestamp': time.time(),
            'url': url,
            'method': method,
            'request_time': request_time,
            'success': success,
            'error': error
        }
        
        self.request_history.append(history_entry)
        
        # é™åˆ¶å†å²è®°å½•å¤§å°
        if len(self.request_history) > self.max_history_size:
            self.request_history.pop(0)
    
    def get_network_stats(self) -> Dict[str, Any]:
        """è·å–ç½‘ç»œç»Ÿè®¡ä¿¡æ¯"""
        with self._lock:
            success_rate = (
                self.stats.successful_requests / self.stats.total_requests * 100
                if self.stats.total_requests > 0 else 0
            )
            
            cache_hit_rate = (
                self.stats.cached_requests / self.stats.total_requests * 100
                if self.stats.total_requests > 0 else 0
            )
            
            return {
                'total_requests': self.stats.total_requests,
                'successful_requests': self.stats.successful_requests,
                'failed_requests': self.stats.failed_requests,
                'cached_requests': self.stats.cached_requests,
                'success_rate': success_rate,
                'cache_hit_rate': cache_hit_rate,
                'total_bytes_downloaded': self.stats.total_bytes_downloaded,
                'average_response_time': self.stats.average_response_time,
                'recent_requests': len(self.request_history)
            }
    
    def analyze_performance(self) -> Dict[str, Any]:
        """åˆ†æç½‘ç»œæ€§èƒ½"""
        if not self.request_history:
            return {'message': 'æš‚æ— è¯·æ±‚å†å²æ•°æ®'}
        
        recent_requests = self.request_history[-100:]  # æœ€è¿‘100ä¸ªè¯·æ±‚
        
        successful_requests = [r for r in recent_requests if r['success']]
        failed_requests = [r for r in recent_requests if not r['success']]
        
        if successful_requests:
            response_times = [r['request_time'] for r in successful_requests]
            avg_response_time = sum(response_times) / len(response_times)
            min_response_time = min(response_times)
            max_response_time = max(response_times)
        else:
            avg_response_time = min_response_time = max_response_time = 0
        
        return {
            'recent_requests_count': len(recent_requests),
            'success_rate': len(successful_requests) / len(recent_requests) * 100,
            'average_response_time': avg_response_time,
            'min_response_time': min_response_time,
            'max_response_time': max_response_time,
            'common_errors': self._get_common_errors(failed_requests)
        }
    
    def _get_common_errors(self, failed_requests: List[Dict]) -> Dict[str, int]:
        """è·å–å¸¸è§é”™è¯¯ç»Ÿè®¡"""
        error_counts = {}
        for request in failed_requests:
            error = request.get('error', 'Unknown')
            error_counts[error] = error_counts.get(error, 0) + 1
        
        # è¿”å›å‰5ä¸ªæœ€å¸¸è§çš„é”™è¯¯
        return dict(sorted(error_counts.items(), key=lambda x: x[1], reverse=True)[:5])
    
    def clear_cache(self):
        """æ¸…ç©ºç¼“å­˜"""
        try:
            import shutil
            shutil.rmtree(self.cache.cache_dir)
            self.cache.cache_dir.mkdir(parents=True, exist_ok=True)
            self.cache.cache_index = {}
            print("ğŸ§¹ ç½‘ç»œç¼“å­˜å·²æ¸…ç©º")
        except Exception as e:
            print(f"âŒ æ¸…ç©ºç¼“å­˜å¤±è´¥: {e}")
    
    def close(self):
        """å…³é—­ç½‘ç»œä¼˜åŒ–å™¨"""
        if hasattr(self, 'session'):
            self.session.close()


# å…¨å±€ç½‘ç»œä¼˜åŒ–å™¨å®ä¾‹
_global_network_optimizer = None
_optimizer_lock = threading.Lock()


def get_network_optimizer():
    """è·å–å…¨å±€ç½‘ç»œä¼˜åŒ–å™¨å®ä¾‹ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰"""
    global _global_network_optimizer
    
    if _global_network_optimizer is None:
        with _optimizer_lock:
            if _global_network_optimizer is None:
                _global_network_optimizer = NetworkOptimizer()
    
    return _global_network_optimizer
