# -*- coding: utf-8 -*-
"""
çˆ¬è™«æ ¸å¿ƒæ¨¡å—
ä¼˜åŒ–ç‰ˆçš„æ‘©ç‚¹ä¼—ç­¹çˆ¬è™«ï¼Œé›†æˆç›‘æ§ã€éªŒè¯ã€ç¼“å­˜ç­‰åŠŸèƒ½
"""

import time
import re
import json
from typing import List, Dict, Any, Optional, Tuple
from bs4 import BeautifulSoup
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading

from .config import SpiderConfig
from .utils import NetworkUtils, DataUtils, CacheUtils, ParserUtils
from .monitor import SpiderMonitor
from .validator import DataValidator
from .exporter import DataExporter


class AdaptiveParser:
    """æ™ºèƒ½é€‚é…è§£æå™¨ - é‡æ„ç‰ˆæœ¬ï¼Œä½¿ç”¨æ¨¡å—åŒ–çš„æå–å™¨"""

    def __init__(self, config: SpiderConfig, network_utils: NetworkUtils, web_monitor=None, stop_flag=None):
        self.config = config
        self.network_utils = network_utils
        self.data_utils = DataUtils()
        self.web_monitor = web_monitor
        self._stop_flag = stop_flag

        # åˆå§‹åŒ–å„ä¸ªæå–å™¨æ¨¡å—ï¼ˆä¿ç•™å¿…è¦çš„æ¨¡å—ï¼‰
        from .extractors.list_extractor import ListExtractor

        self.list_extractor = ListExtractor(config, web_monitor)
        # ContentExtractorå·²ç§»é™¤ - åŠŸèƒ½å®Œå…¨è¢«APIæ›¿ä»£

        # å·²åˆ é™¤çš„å†—ä½™æå–å™¨ï¼š
        # - detail_extractor (ä¾èµ–åŠ¨æ€è·å–ï¼Œå·²å¼ƒç”¨)
        # - author_extractor (APIå·²åŒ…å«ä½œè€…ä¿¡æ¯)
        # - funding_extractor (APIå·²åŒ…å«é‡‘é¢ä¿¡æ¯)

        # åˆå§‹åŒ–å„ä¸ªå¤„ç†å™¨æ¨¡å—ï¼ˆç®€åŒ–ç‰ˆï¼‰
        from .processors.data_processor import DataProcessor
        from .processors.validation_processor import ValidationProcessor

        self.data_processor = DataProcessor(config, self.data_utils, web_monitor)
        self.validation_processor = ValidationProcessor(config, web_monitor)

        # å·²ç§»é™¤çš„å¤„ç†å™¨ï¼ˆAPIæ—¶ä»£ä¸å†éœ€è¦ï¼‰ï¼š
        # - status_processor (APIç›´æ¥æä¾›å‡†ç¡®çŠ¶æ€)
        # - time_processor (APIç›´æ¥æä¾›å‡†ç¡®æ—¶é—´)

    def _log(self, level: str, message: str):
        """ç»Ÿä¸€æ—¥å¿—è¾“å‡º"""
        print(message)
        if self.web_monitor:
            self.web_monitor.add_log(level, message)

    def try_multiple_selectors(self, soup: BeautifulSoup, selectors: list, element_type: str = "element") -> any:
        """å°è¯•å¤šä¸ªé€‰æ‹©å™¨ï¼Œè¿”å›ç¬¬ä¸€ä¸ªæˆåŠŸçš„ç»“æœ"""
        for selector in selectors:
            try:
                if element_type == "text":
                    element = soup.select_one(selector)
                    if element:
                        return ParserUtils.safe_get_text(element)
                elif element_type == "attr":
                    element = soup.select_one(selector)
                    if element:
                        return element
                else:
                    result = soup.select(selector) if element_type == "all" else soup.select_one(selector)
                    if result:
                        return result
            except Exception:
                continue
        return None

    def adaptive_parse_project_list(self, html: str) -> List[Tuple[str, str, str, str, Dict[str, str]]]:
        """æ™ºèƒ½é€‚é…è§£æé¡¹ç›®åˆ—è¡¨ - ä½¿ç”¨ListExtractoræ¨¡å—"""
        return self.list_extractor.extract_project_list(html)

    # è¿™äº›æ–¹æ³•å·²ç»ç§»åŠ¨åˆ°ListExtractoræ¨¡å—ä¸­ï¼Œä¸å†éœ€è¦

    def _extract_js_data(self, soup: BeautifulSoup) -> Dict[str, Any]:
        """ä»JavaScriptä»£ç ä¸­æå–é¡¹ç›®æ•°æ® - ä½¿ç”¨DataProcessoræ¨¡å—"""
        return self.data_processor.extract_js_data(soup)
    
    def parse_project_status(self, soup: BeautifulSoup) -> Dict[str, Any]:
        """å·²å¼ƒç”¨ï¼šé¡¹ç›®çŠ¶æ€è§£æï¼Œç°åœ¨ä½¿ç”¨APIè·å–"""
        # APIç›´æ¥æä¾›å‡†ç¡®çš„é¡¹ç›®çŠ¶æ€ï¼Œæ— éœ€HTMLè§£æ
        return {
            "item_class": "æœªçŸ¥æƒ…å†µ",
            "status": "unknown",
            "is_crowdfunding": False,
            "is_success": False,
            "is_failed": False,
            "is_finished": False
        }
    
    # åŸºç¡€ä¿¡æ¯è§£ææ–¹æ³•å·²å¼ƒç”¨ - ç°åœ¨ä½¿ç”¨APIè·å–å®Œæ•´æ•°æ®
    # def parse_basic_info(self, soup: BeautifulSoup, project_status: Dict) -> List[Any]:
    #     """å·²å¼ƒç”¨ï¼šåŸºç¡€ä¿¡æ¯è§£æï¼Œç°åœ¨ä½¿ç”¨APIè·å–"""
    #     pass

    # æ—¶é—´è§£ææ–¹æ³•å·²ç§»åŠ¨åˆ°TimeProcessoræ¨¡å—
    
    # ä½œè€…ä¿¡æ¯è§£ææ–¹æ³•å·²ç§»åŠ¨åˆ°AuthorExtractoræ¨¡å—

    # ä½œè€…ä¿¡æ¯è§£ææ–¹æ³•å·²å®Œå…¨ç§»åŠ¨åˆ°AuthorExtractoræ¨¡å—
    
    # ä½œè€…è¯¦ç»†ä¿¡æ¯è·å–æ–¹æ³•å·²ç§»åŠ¨åˆ°AuthorExtractoræ¨¡å—
    
    # ä½œè€…é¡µé¢è§£ææ–¹æ³•å·²ç§»åŠ¨åˆ°AuthorExtractoræ¨¡å—
    
    # ä¼—ç­¹ä¿¡æ¯è§£ææ–¹æ³•å·²ç§»åŠ¨åˆ°FundingExtractoræ¨¡å—

    def _validate_extracted_data(self, money: str, percent: str, goal_money: str, sponsor_num: str):
        """éªŒè¯æå–çš„æ•°æ®åˆç†æ€§ - ä½¿ç”¨DataProcessoræ¨¡å—"""
        self.data_processor.validate_extracted_data(money, percent, goal_money, sponsor_num)

    # ä¼—ç­¹ä¿¡æ¯è§£ææ–¹æ³•å·²ç§»åŠ¨åˆ°FundingExtractoræ¨¡å—
    
    # é¡¹ç›®å†…å®¹è§£ææ–¹æ³•å·²å¼ƒç”¨ - ç°åœ¨ä½¿ç”¨APIè·å–å®Œæ•´æ•°æ®
    # def parse_project_content(self, soup: BeautifulSoup) -> List[Any]:
    #     """å·²å¼ƒç”¨ï¼šé¡¹ç›®å†…å®¹è§£æï¼Œç°åœ¨ä½¿ç”¨APIè·å–"""
    #     pass
    
    # å›æŠ¥ä¿¡æ¯è§£ææ–¹æ³•å·²ç§»åŠ¨åˆ°DetailExtractoræ¨¡å—
    
    # å•ä¸ªå›æŠ¥è§£ææ–¹æ³•å·²ç§»åŠ¨åˆ°DetailExtractoræ¨¡å—
    
    # å¯¼èˆªä¿¡æ¯è§£ææ–¹æ³•å·²ç§»åŠ¨åˆ°ContentExtractoræ¨¡å—

    # JavaScriptå¯¼èˆªæ•°æ®æå–æ–¹æ³•å·²ç§»åŠ¨åˆ°ContentExtractoræ¨¡å—



    # å…³é”®å¯¼èˆªæ•°æ®æå–æ–¹æ³•å·²ç§»åŠ¨åˆ°ContentExtractoræ¨¡å—



    # é¡¹ç›®IDæå–æ–¹æ³•å·²ç§»åŠ¨åˆ°ContentExtractoræ¨¡å—



    # æ•°æ®æå–æ–¹æ³•å·²ç§»åŠ¨åˆ°ç›¸åº”çš„æå–å™¨æ¨¡å—ï¼š
    # - _extract_update_count_only -> ContentExtractor
    # - _validate_nav_data -> ContentExtractor
    # - _parse_content_media -> DetailExtractor


class SpiderCore:
    """çˆ¬è™«æ ¸å¿ƒç±»"""

    def __init__(self, config: SpiderConfig = None, web_monitor=None, db_manager=None):
        # ğŸ”§ ä¼˜å…ˆä»YAMLé…ç½®æ–‡ä»¶åŠ è½½é…ç½®
        self.config = config or SpiderConfig.load_from_yaml()
        self.config.create_directories()

        # Web UIç›‘æ§å™¨
        self.web_monitor = web_monitor

        # æ•°æ®åº“ç®¡ç†å™¨ï¼ˆç”¨äºå¢é‡ä¿å­˜ï¼‰
        self.db_manager = db_manager

        # çº¿ç¨‹é”å’Œåœæ­¢æ ‡å¿—ï¼ˆéœ€è¦åœ¨åˆå§‹åŒ–ç»„ä»¶ä¹‹å‰å®šä¹‰ï¼‰
        self._lock = threading.Lock()
        self._stop_flag = threading.Event()
        self._is_running = False

        # åˆå§‹åŒ–ç»„ä»¶
        self.network_utils = NetworkUtils(self.config)
        self.cache_utils = CacheUtils(self.config)
        self.monitor = SpiderMonitor(self.config)
        self.validator = DataValidator(self.config)
        self.exporter = DataExporter(self.config)
        self.parser = AdaptiveParser(self.config, self.network_utils, self.web_monitor, self._stop_flag)

        # åˆå§‹åŒ–APIè·å–å™¨ï¼ˆæ–°çš„äº’è¡¥æ¶æ„ï¼‰
        from spider.api_data_fetcher import ModianAPIFetcher
        self.api_fetcher = ModianAPIFetcher(self.config)

        # æ•°æ®å­˜å‚¨
        self.projects_data = []
        self.failed_urls = []

        # è¿›åº¦å›è°ƒ
        self._progress_callback = None

        # ğŸ”§ åŠ¨æ€ä¿å­˜é…ç½®ï¼šæ ¹æ®çº¿ç¨‹æ•°è°ƒæ•´ä¿å­˜é—´éš”
        base_save_interval = getattr(self.config, 'SAVE_INTERVAL', 3)
        self.save_interval = max(1, min(base_save_interval, self.config.MAX_CONCURRENT_REQUESTS))
        self.current_task_id = None
        self.saved_count = 0  # å·²ä¿å­˜çš„é¡¹ç›®æ•°é‡

        self._log("info", f"çˆ¬è™«åˆå§‹åŒ–å®Œæˆï¼Œè¾“å‡ºç›®å½•: {self.config.OUTPUT_DIR}")
        self._log("info", f"å¹¶å‘çº¿ç¨‹æ•°: {self.config.MAX_CONCURRENT_REQUESTS}")
        self._log("info", f"è¯·æ±‚å»¶è¿ŸèŒƒå›´: {self.config.REQUEST_DELAY[0]}-{self.config.REQUEST_DELAY[1]}ç§’")
        self._log("info", f"åŠ¨æ€ä¿å­˜é—´éš”: æ¯{self.save_interval}ä¸ªé¡¹ç›®ï¼ˆåŸºäº{self.config.MAX_CONCURRENT_REQUESTS}çº¿ç¨‹ä¼˜åŒ–ï¼‰")

    # æ¸…ç†æ–¹æ³•å·²ç§»é™¤ - ç°åœ¨ä½¿ç”¨è½»é‡çº§APIè·å–ï¼Œæ— éœ€å¤æ‚çš„èµ„æºç®¡ç†

    def _log(self, level: str, message: str):
        """ç»Ÿä¸€æ—¥å¿—è¾“å‡º"""
        print(message)
        if self.web_monitor:
            self.web_monitor.add_log(level, message)

    def set_progress_callback(self, callback):
        """è®¾ç½®è¿›åº¦å›è°ƒå‡½æ•°"""
        self._progress_callback = callback

    def stop_crawling(self):
        """åœæ­¢çˆ¬è™«"""
        print("æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œæ­£åœ¨åœæ­¢çˆ¬è™«...")
        self._stop_flag.set()
        self._is_running = False

    def is_stopped(self):
        """æ£€æŸ¥æ˜¯å¦å·²åœæ­¢"""
        return self._stop_flag.is_set()

    def is_running(self):
        """æ£€æŸ¥æ˜¯å¦æ­£åœ¨è¿è¡Œ"""
        return self._is_running

    def start_crawling(self, start_page: int = 1, end_page: int = 50,
                      category: str = "all", task_id: str = None) -> bool:
        """å¼€å§‹çˆ¬å–"""
        try:
            self._is_running = True
            self._stop_flag.clear()
            self.current_task_id = task_id
            self.saved_count = 0

            self._log("info", f"å¼€å§‹çˆ¬å–æ‘©ç‚¹ä¼—ç­¹æ•°æ®...")
            self._log("info", f"é¡µé¢èŒƒå›´: {start_page}-{end_page}")
            self._log("info", f"åˆ†ç±»: {category}")
            self._log("info", f"ä»»åŠ¡ID: {task_id}")

            # å¯åŠ¨ç›‘æ§
            self.monitor.start_monitoring()

            # çˆ¬å–é¡¹ç›®åˆ—è¡¨
            project_urls = self._crawl_project_lists(start_page, end_page, category)

            if self.is_stopped():
                self._log("warning", "çˆ¬å–å·²è¢«ç”¨æˆ·åœæ­¢")
                # å³ä½¿è¢«åœæ­¢ï¼Œä¹Ÿè¦ä¿å­˜å·²è·å–çš„æ•°æ®
                self._save_remaining_data()
                return False

            if not project_urls:
                self._log("warning", "æœªæ‰¾åˆ°ä»»ä½•é¡¹ç›®URL")
                return False

            self._log("info", f"å‘ç° {len(project_urls)} ä¸ªé¡¹ç›®ï¼Œå¼€å§‹è¯¦ç»†çˆ¬å–...")

            # æ›´æ–°è¿›åº¦
            if self._progress_callback:
                total_pages = end_page - start_page + 1
                self._progress_callback(current_page=total_pages, total_pages=total_pages, total_projects=len(project_urls), completed_projects=0)

            # çˆ¬å–é¡¹ç›®è¯¦æƒ…
            success = self._crawl_project_details(project_urls)

            # åœæ­¢ç›‘æ§
            self.monitor.stop_monitoring()

            # ä¿å­˜å‰©ä½™æ•°æ®
            self._save_remaining_data()

            # æ•°æ®å¯¼å‡ºï¼ˆå¦‚æœæœ‰æ•°æ®ä¸”æœªè¢«åœæ­¢ï¼‰
            if self.projects_data and not self.is_stopped():
                self._export_data()

            # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
            self.monitor.print_stats()

            return success

        except KeyboardInterrupt:
            print("\nç”¨æˆ·ä¸­æ–­çˆ¬å–")
            self._is_running = False
            self.monitor.stop_monitoring()
            return False
        except Exception as e:
            print(f"çˆ¬å–è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            self.monitor.record_error("crawling_error", str(e))
            self._is_running = False
            self.monitor.stop_monitoring()
            return False
        finally:
            self._is_running = False
            # åŠ¨æ€æ•°æ®ç®¡ç†å™¨å·²å¼ƒç”¨ï¼Œæ— éœ€æ¸…ç†

    def _crawl_project_lists(self, start_page: int, end_page: int,
                           category: str) -> List[Tuple[str, str, str, str]]:
        """çˆ¬å–é¡¹ç›®åˆ—è¡¨é¡µé¢"""
        project_urls = []

        for page in range(start_page, end_page + 1):
            # æ£€æŸ¥åœæ­¢æ ‡å¿—
            if self.is_stopped():
                print("æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œåœæ­¢çˆ¬å–é¡µé¢åˆ—è¡¨")
                break

            try:
                self._log("info", f"æ­£åœ¨çˆ¬å–ç¬¬ {page} é¡µ...")

                url = self.config.get_full_url(category, page)
                page_projects = self._parse_project_list_page(url, page)

                if page_projects:
                    project_urls.extend(page_projects)
                    self.monitor.record_page(True)
                    self._log("success", f"ç¬¬ {page} é¡µå‘ç° {len(page_projects)} ä¸ªé¡¹ç›®")

                    # æ›´æ–°è¿›åº¦
                    if self._progress_callback:
                        current_progress = page - start_page + 1
                        total_pages = end_page - start_page + 1
                        self._progress_callback(current_page=current_progress, total_pages=total_pages, total_projects=len(project_urls), completed_projects=0)
                else:
                    self.monitor.record_page(False)
                    self._log("warning", f"ç¬¬ {page} é¡µæœªå‘ç°é¡¹ç›®")

                # æ£€æŸ¥æ˜¯å¦éœ€è¦åœæ­¢
                if self.monitor.stats.consecutive_errors > self.config.MAX_CONSECUTIVE_ERRORS:
                    print("è¿ç»­é”™è¯¯è¿‡å¤šï¼Œåœæ­¢çˆ¬å–")
                    break

            except Exception as e:
                print(f"çˆ¬å–ç¬¬ {page} é¡µå¤±è´¥: {e}")
                self.monitor.record_error("page_crawl_error", str(e))
                self.monitor.record_page(False)

        return project_urls

    def _parse_project_list_page(self, url: str, page: int) -> List[Tuple[str, str, str, str, Dict[str, str]]]:
        """è§£æé¡¹ç›®åˆ—è¡¨é¡µé¢"""
        start_time = time.time()

        # æ£€æŸ¥ç¼“å­˜
        cached_content = self.cache_utils.get_cache(url)
        if cached_content:
            html = cached_content
            self.monitor.record_request(True, 0, cached=True)
        else:
            # å‘é€è¯·æ±‚
            html = self.network_utils.make_request(url)
            request_time = time.time() - start_time

            if html:
                self.cache_utils.set_cache(url, html)
                self.monitor.record_request(True, request_time)
            else:
                self.monitor.record_request(False, request_time)
                return []

        # è§£æé¡µé¢
        parse_start = time.time()
        projects = self._extract_projects_from_list(html)
        parse_time = time.time() - parse_start
        self.monitor.record_parse(parse_time)

        return projects

    def _extract_projects_from_list(self, html: str) -> List[Tuple[str, str, str, str, Dict[str, str]]]:
        """ä»åˆ—è¡¨é¡µé¢æå–é¡¹ç›®ä¿¡æ¯ - ä½¿ç”¨æ™ºèƒ½é€‚é…è§£æ"""
        try:
            # ä½¿ç”¨æ™ºèƒ½é€‚é…è§£æå™¨
            projects = self.parser.adaptive_parse_project_list(html)

            # è¿‡æ»¤å’ŒéªŒè¯é¡¹ç›®
            filtered_projects = []
            for project_data in projects:
                try:
                    # è§£åŒ…é¡¹ç›®æ•°æ®ï¼ˆå…¼å®¹æ–°æ—§æ ¼å¼ï¼‰
                    if len(project_data) == 5:
                        project_url, project_id, project_name, project_image, list_data = project_data
                    else:
                        project_url, project_id, project_name, project_image = project_data
                        list_data = {}

                    # æ£€æŸ¥æ˜¯å¦è·³è¿‡
                    if self._should_skip_project(project_name):
                        self.monitor.record_project("skipped")
                        continue

                    # è¿”å›å®Œæ•´çš„5ä¸ªå­—æ®µï¼ŒåŒ…å«åˆ—è¡¨æ•°æ®ï¼ˆç‰¹åˆ«æ˜¯ä½œè€…ä¿¡æ¯ï¼‰
                    filtered_projects.append((project_url, project_id, project_name, project_image, list_data))
                    self.monitor.record_project("found")

                    # è®°å½•åˆ—è¡¨æ•°æ®ç”¨äºè°ƒè¯•
                    if list_data and any(v != "0" and v != "none" for v in list_data.values()):
                        author_name = list_data.get('list_author_name', 'none')
                        print(f"ğŸ“Š åˆ—è¡¨æ•°æ®: {project_name[:20]}... -> ä½œè€…:{author_name}, æ”¯æŒè€…{list_data.get('list_backer_count', '0')}äºº")

                except Exception as e:
                    print(f"éªŒè¯é¡¹ç›®å¤±è´¥: {e}")
                    self.monitor.record_error("project_validation_error", str(e))
                    continue

            print(f"âœ… æ™ºèƒ½è§£æå®Œæˆ: å‘ç° {len(projects)} ä¸ªé¡¹ç›®ï¼Œè¿‡æ»¤å {len(filtered_projects)} ä¸ª")
            return filtered_projects

        except Exception as e:
            print(f"æ™ºèƒ½è§£æå¤±è´¥ï¼Œä½¿ç”¨ä¼ ç»Ÿè§£æ: {e}")
            self.monitor.record_error("adaptive_parse_error", str(e))
            return self._fallback_extract_projects(html)

    def _fallback_extract_projects(self, html: str) -> List[Tuple[str, str, str, str, Dict[str, str]]]:
        """ä¼ ç»Ÿè§£ææ–¹æ³•ä½œä¸ºå›é€€"""
        projects = []

        try:
            soup = BeautifulSoup(html, "html.parser")

            # æŸ¥æ‰¾é¡¹ç›®åˆ—è¡¨
            project_list = ParserUtils.safe_find(soup, 'div', {'class': 'pro_field'})
            if not project_list:
                return projects

            project_items = ParserUtils.safe_find_all(project_list, 'li')

            for item in project_items:
                try:
                    # é¡¹ç›®é“¾æ¥
                    link_tag = ParserUtils.safe_find(item, 'a', {'class': 'pro_name ga'})
                    if not link_tag:
                        continue

                    project_url = ParserUtils.safe_get_attr(link_tag, 'href')
                    if not project_url:
                        continue

                    project_url = self.data_utils.validate_url(project_url)

                    # é¡¹ç›®ID
                    project_id = self.data_utils.extract_project_id(project_url)
                    if not project_id:
                        continue

                    # é¡¹ç›®åç§°
                    title_tag = ParserUtils.safe_find(link_tag, 'h3', {'class': 'pro_title'})
                    project_name = ParserUtils.safe_get_text(title_tag) if title_tag else "æœªçŸ¥é¡¹ç›®"
                    project_name = self.data_utils.clean_text(project_name, self.config.MAX_TITLE_LENGTH)

                    # æ£€æŸ¥æ˜¯å¦è·³è¿‡
                    if self._should_skip_project(project_name):
                        self.monitor.record_project("skipped")
                        continue

                    # é¡¹ç›®å›¾ç‰‡
                    img_tag = ParserUtils.safe_find(item, 'img')
                    project_image = ParserUtils.safe_get_attr(img_tag, 'src') if img_tag else "none"
                    project_image = self.data_utils.validate_url(project_image)

                    # åˆ›å»ºç©ºçš„åˆ—è¡¨æ•°æ®ï¼ˆfallbackæ–¹æ³•æ²¡æœ‰é¢å¤–æ•°æ®ï¼‰
                    list_data = {
                        "list_backer_money": "0",
                        "list_rate": "0",
                        "list_backer_count": "0",
                        "list_author_name": "none"
                    }
                    projects.append((project_url, project_id, project_name, project_image, list_data))
                    self.monitor.record_project("found")

                except Exception as e:
                    print(f"è§£æé¡¹ç›®é¡¹å¤±è´¥: {e}")
                    continue

        except Exception as e:
            print(f"è§£æé¡¹ç›®åˆ—è¡¨å¤±è´¥: {e}")
            self.monitor.record_error("parse_list_error", str(e))

        return projects

    def _should_skip_project(self, project_name: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦åº”è¯¥è·³è¿‡é¡¹ç›®"""
        if not project_name or len(project_name) < self.config.MIN_TITLE_LENGTH:
            return True

        for keyword in self.config.SKIP_KEYWORDS:
            if keyword in project_name:
                return True

        return False

    def _crawl_project_details(self, project_urls: List[Tuple[str, str, str, str, Dict[str, str]]]) -> bool:
        """çˆ¬å–é¡¹ç›®è¯¦æƒ…ï¼ˆå¢å¼ºè¿›åº¦æ˜¾ç¤ºç‰ˆæœ¬ï¼‰"""
        if not project_urls:
            return False

        total_projects = len(project_urls)
        self._log("info", f"å¼€å§‹å¹¶å‘çˆ¬å– {total_projects} ä¸ªé¡¹ç›®è¯¦æƒ…ï¼Œå¹¶å‘æ•°: {self.config.MAX_CONCURRENT_REQUESTS}")

        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘çˆ¬å–
        with ThreadPoolExecutor(max_workers=self.config.MAX_CONCURRENT_REQUESTS) as executor:
            # æäº¤ä»»åŠ¡
            future_to_project = {
                executor.submit(self._crawl_single_project, i, project_info): (i, project_info)
                for i, project_info in enumerate(project_urls)
            }

            # å¤„ç†ç»“æœ
            completed = 0
            for future in as_completed(future_to_project):
                # åœ¨å¤„ç†æ¯ä¸ªç»“æœå‰æ£€æŸ¥åœæ­¢æ ‡å¿—
                if self.is_stopped():
                    self._log("warning", "æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œæ­£åœ¨ä¿å­˜å·²å¤„ç†çš„æ•°æ®...")
                    # å–æ¶ˆå‰©ä½™çš„ä»»åŠ¡
                    for remaining_future in future_to_project:
                        if not remaining_future.done():
                            remaining_future.cancel()
                    self._save_remaining_data()
                    break

                _, project_info = future_to_project[future]

                try:
                    result = future.result(timeout=1)  # æ·»åŠ è¶…æ—¶ï¼Œé¿å…é•¿æ—¶é—´é˜»å¡
                    if result:
                        with self._lock:
                            self.projects_data.append(result)
                        self.monitor.record_project("processed")
                        self._log("success", f"é¡¹ç›® {project_info[2]} å¤„ç†æˆåŠŸ")

                        # ğŸ”§ å¢é‡ä¿å­˜ï¼šæ¯å¤„ç†å®ŒæŒ‡å®šæ•°é‡çš„é¡¹ç›®å°±ä¿å­˜ä¸€æ¬¡
                        if len(self.projects_data) % self.save_interval == 0:
                            self._save_incremental_data()
                    else:
                        self.monitor.record_project("failed")
                        self.failed_urls.append(project_info[0])
                        self._log("warning", f"é¡¹ç›® {project_info[2]} å¤„ç†å¤±è´¥")

                except TimeoutError:
                    self._log("warning", f"é¡¹ç›® {project_info[2]} å¤„ç†è¶…æ—¶")
                    self.monitor.record_project("failed")
                    self.failed_urls.append(project_info[0])
                except Exception as e:
                    self._log("error", f"å¤„ç†é¡¹ç›®å¤±è´¥ {project_info[2]}: {e}")
                    self.monitor.record_error("project_process_error", str(e))
                    self.monitor.record_project("failed")
                    self.failed_urls.append(project_info[0])

                completed += 1

                # æ›´æ–°è¿›åº¦åˆ°Web UI
                if self._progress_callback:
                    # è®¡ç®—æ€»ä½“è¿›åº¦ï¼šé¡µé¢çˆ¬å– + é¡¹ç›®è¯¦æƒ…çˆ¬å–
                    project_progress = (completed / total_projects) * 100
                    self._progress_callback(current_page=0, total_pages=0, total_projects=total_projects, completed_projects=completed, project_progress=project_progress)

                # å®šæœŸè¾“å‡ºè¿›åº¦
                if completed % 5 == 0 or completed == total_projects:
                    progress_percent = (completed / total_projects) * 100
                    self._log("info", f"é¡¹ç›®è¯¦æƒ…è¿›åº¦: {completed}/{total_projects} ({progress_percent:.1f}%)")

            # å¦‚æœè¢«åœæ­¢ï¼Œå¼ºåˆ¶å…³é—­çº¿ç¨‹æ± 
            if self.is_stopped():
                self._log("warning", "å¼ºåˆ¶å…³é—­çº¿ç¨‹æ± ...")
                executor.shutdown(wait=False)

        self._log("info", f"é¡¹ç›®è¯¦æƒ…çˆ¬å–å®Œæˆï¼ŒæˆåŠŸ: {len(self.projects_data)}, å¤±è´¥: {len(self.failed_urls)}")
        return len(self.projects_data) > 0

    def _crawl_single_project(self, index: int, project_info: Tuple[str, str, str, str, Dict[str, str]]) -> Optional[List[Any]]:
        """çˆ¬å–å•ä¸ªé¡¹ç›®è¯¦æƒ…"""
        # è§£åŒ…é¡¹ç›®ä¿¡æ¯ï¼ˆæ”¯æŒ5ä¸ªå­—æ®µï¼‰
        if len(project_info) == 5:
            project_url, project_id, project_name, project_image, list_data = project_info
        else:
            project_url, project_id, project_name, project_image = project_info
            list_data = {}

        # æ£€æŸ¥åœæ­¢æ ‡å¿—
        if self.is_stopped():
            self._log("warning", f"â¹ï¸ æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œè·³è¿‡é¡¹ç›® {project_name}")
            return None

        try:
            start_time = time.time()

            # æ£€æŸ¥ç¼“å­˜
            cached_content = self.cache_utils.get_cache(project_url)
            if cached_content:
                html = cached_content
                self.monitor.record_request(True, 0, cached=True)
            else:
                # å‘é€è¯·æ±‚
                html = self.network_utils.make_request(project_url)
                request_time = time.time() - start_time

                if html:
                    self.cache_utils.set_cache(project_url, html)
                    self.monitor.record_request(True, request_time)
                else:
                    self.monitor.record_request(False, request_time)
                    return None

            # é€šè¿‡APIè·å–é¡¹ç›®å®Œæ•´æ•°æ®ï¼ˆæ–°çš„äº’è¡¥æ¶æ„ï¼‰
            api_start = time.time()
            project_data = self._get_project_data_via_api(index + 1, project_url, project_id, project_name, project_image, list_data)
            api_time = time.time() - api_start
            self.monitor.record_parse(api_time)

            return project_data

        except Exception as e:
            print(f"çˆ¬å–é¡¹ç›®è¯¦æƒ…å¤±è´¥ {project_name}: {e}")
            self.monitor.record_error("project_detail_error", str(e))
            return None

    def _get_project_data_via_api(self, index: int, project_url: str,
                                 project_id: str, project_name: str, project_image: str, list_data: Dict[str, str] = None) -> List[Any]:
        """é€šè¿‡APIè·å–é¡¹ç›®å®Œæ•´æ•°æ® - æ–°çš„äº’è¡¥æ¶æ„"""
        try:
            # ä½¿ç”¨APIè·å–å®Œæ•´é¡¹ç›®æ•°æ®
            api_data = self.api_fetcher.get_project_data(project_id)

            if not api_data or api_data.get("like_count", "0") == "0":
                self._log("warning", f"é¡¹ç›® {project_id} APIè·å–å¤±è´¥ï¼Œä½¿ç”¨åŸºç¡€æ•°æ®")
                # è¿”å›åŸºç¡€æ•°æ®
                return self._create_basic_project_data(index, project_url, project_id, project_name, project_image, list_data)

            # è½¬æ¢APIæ•°æ®ä¸ºæ•°æ®åº“æ ¼å¼ï¼Œä½¿ç”¨åˆ—è¡¨æ•°æ®è¡¥å……ä½œè€…ä¿¡æ¯
            project_data = self._convert_api_to_db_format(api_data, index, project_url, project_id, project_name, project_image, list_data)

            self._log("info", f"âœ… é¡¹ç›® {project_id} APIæ•°æ®è·å–æˆåŠŸ")
            return project_data

        except Exception as e:
            self._log("error", f"é¡¹ç›® {project_id} APIè·å–å¼‚å¸¸: {e}")
            return self._create_basic_project_data(index, project_url, project_id, project_name, project_image, list_data)

    def _convert_api_to_db_format(self, api_data: dict, index: int, project_url: str,
                                 project_id: str, project_name: str, project_image: str, list_data: Dict[str, str] = None) -> List[Any]:
        """å°†APIæ•°æ®è½¬æ¢ä¸ºæ•°æ®åº“æ ¼å¼ï¼Œä½¿ç”¨åˆ—è¡¨æ•°æ®è¡¥å……ä½œè€…ä¿¡æ¯"""
        from spider.config import FieldMapping

        # è·å–ä½œè€…ä¿¡æ¯ï¼šä¼˜å…ˆä½¿ç”¨åˆ—è¡¨æ•°æ®ï¼ŒAPIæ•°æ®ä½œä¸ºå¤‡é€‰
        if list_data and list_data.get("list_author_name") and list_data.get("list_author_name") != "none":
            author_name = list_data.get("list_author_name", "")
        else:
            author_name = api_data.get("author_name", "")

        # è·å–ä½œè€…å¤´åƒï¼šä¼˜å…ˆä½¿ç”¨åˆ—è¡¨æ•°æ®ï¼Œç„¶åAPIæ•°æ®ï¼Œæœ€åé»˜è®¤å¤´åƒ
        author_image = ""
        if list_data and list_data.get("list_author_avatar") and list_data.get("list_author_avatar") != "none":
            author_image = list_data.get("list_author_avatar", "")
        else:
            author_image = api_data.get("author_image", "")

        # å¦‚æœä»ç„¶æ²¡æœ‰å¤´åƒï¼Œä½¿ç”¨é»˜è®¤å¤´åƒ
        if not author_image:
            author_image = "https://s.moimg.net/new/images/headPic.png"

        # æŒ‰ç…§æ•°æ®åº“å­—æ®µé¡ºåºæ„å»ºæ•°æ®
        project_data = [
            index,                                          # åºå·
            project_url,                                    # é¡¹ç›®link
            project_id,                                     # é¡¹ç›®6ä½id
            project_name,                                   # é¡¹ç›®åç§°
            project_image,                                  # é¡¹ç›®å›¾
            api_data.get("start_time", ""),                # å¼€å§‹æ—¶é—´
            api_data.get("end_time", ""),                  # ç»“æŸæ—¶é—´
            api_data.get("project_status", ""),           # é¡¹ç›®ç»“æœ
            api_data.get("author_link", ""),               # ç”¨æˆ·ä¸»é¡µ(é“¾æ¥)
            author_image,                                  # ç”¨æˆ·å¤´åƒ(å›¾ç‰‡é“¾æ¥)
            api_data.get("category", ""),                  # åˆ†ç±»
            author_name,                                   # ç”¨æˆ·åï¼ˆä¼˜å…ˆä½¿ç”¨åˆ—è¡¨æ•°æ®ï¼‰
            "",                                            # ç”¨æˆ·UID(data-username) - APIæ— æ­¤å­—æ®µ
            api_data.get("raised_amount", 0),              # å·²ç­¹é‡‘é¢
            api_data.get("completion_rate", 0),            # ç™¾åˆ†æ¯”
            api_data.get("target_amount", 0),              # ç›®æ ‡é‡‘é¢
            api_data.get("backer_count", 0),               # æ”¯æŒè€…(æ•°é‡)
            "",                                            # çœŸå®ç”¨æˆ·ID(é“¾æ¥æå–) - å¯ä»author_linkæå–
            "",                                            # ä½œè€…é¡µ-ç²‰ä¸æ•° - APIæ— æ­¤å­—æ®µ
            "",                                            # ä½œè€…é¡µ-å…³æ³¨æ•° - APIæ— æ­¤å­—æ®µ
            "",                                            # ä½œè€…é¡µ-è·èµæ•° - APIæ— æ­¤å­—æ®µ
            "",                                            # ä½œè€…é¡µ-è¯¦æƒ… - APIæ— æ­¤å­—æ®µ
            "",                                            # ä½œè€…é¡µ-å…¶ä»–ä¿¡æ¯ - APIæ— æ­¤å­—æ®µ
            "",                                            # ä½œè€…é¡µ-ä¸»é¡µç¡®è®¤ - APIæ— æ­¤å­—æ®µ
            str(api_data.get("rewards_data", [])),         # å›æŠ¥åˆ—è¡¨ä¿¡æ¯(å­—ç¬¦ä¸²)
            len(api_data.get("rewards_data", [])),         # å›æŠ¥åˆ—è¡¨é¡¹ç›®æ•°
            api_data.get("update_count", 0),               # é¡¹ç›®æ›´æ–°æ•°
            api_data.get("comment_count", 0),              # è¯„è®ºæ•°
            api_data.get("like_count", 0),                 # çœ‹å¥½æ•°
            0,                                             # é¡¹ç›®è¯¦æƒ…-å›¾ç‰‡æ•°é‡ - APIæ— æ­¤å­—æ®µ
            "[]",                                          # é¡¹ç›®è¯¦æƒ…-å›¾ç‰‡(åˆ—è¡¨å­—ç¬¦ä¸²) - APIæ— æ­¤å­—æ®µ
            0,                                             # é¡¹ç›®è¯¦æƒ…-è§†é¢‘æ•°é‡ - APIæ— æ­¤å­—æ®µ
            "[]",                                          # é¡¹ç›®è¯¦æƒ…-è§†é¢‘(åˆ—è¡¨å­—ç¬¦ä¸²) - APIæ— æ­¤å­—æ®µ
        ]

        # ç¡®ä¿å­—æ®µæ•°é‡æ­£ç¡®
        expected_length = len(FieldMapping.EXCEL_COLUMNS)
        while len(project_data) < expected_length:
            project_data.append("")

        return project_data[:expected_length]

    def _create_basic_project_data(self, index: int, project_url: str,
                                  project_id: str, project_name: str, project_image: str, list_data: Dict[str, str] = None) -> List[Any]:
        """åˆ›å»ºåŸºç¡€é¡¹ç›®æ•°æ®ï¼ˆAPIè·å–å¤±è´¥æ—¶çš„åå¤‡æ–¹æ¡ˆï¼‰ï¼Œä½¿ç”¨åˆ—è¡¨æ•°æ®è¡¥å……"""
        from spider.config import FieldMapping
        expected_length = len(FieldMapping.EXCEL_COLUMNS)

        # è·å–ä½œè€…ä¿¡æ¯
        author_name = ""
        if list_data and list_data.get("list_author_name") and list_data.get("list_author_name") != "none":
            author_name = list_data.get("list_author_name", "")

        # è·å–ä½œè€…å¤´åƒï¼šä¼˜å…ˆä½¿ç”¨åˆ—è¡¨æ•°æ®ï¼Œå¦åˆ™ä½¿ç”¨é»˜è®¤å¤´åƒ
        author_avatar = ""
        if list_data and list_data.get("list_author_avatar") and list_data.get("list_author_avatar") != "none":
            author_avatar = list_data.get("list_author_avatar", "")
        else:
            author_avatar = "https://s.moimg.net/new/images/headPic.png"

        # åˆ›å»ºåŸºç¡€æ•°æ®ï¼Œåœ¨ç¬¬11ä½ï¼ˆç”¨æˆ·åï¼‰å’Œç¬¬9ä½ï¼ˆç”¨æˆ·å¤´åƒï¼‰å¡«å…¥ä½œè€…ä¿¡æ¯
        basic_data = [index, project_url, project_id, project_name, project_image]

        # å¡«å……å‰©ä½™å­—æ®µä¸ºç©ºå€¼ï¼Œä½†åœ¨ç‰¹å®šä½ç½®å¡«å…¥ä½œè€…ä¿¡æ¯
        while len(basic_data) < expected_length:
            if len(basic_data) == 9:  # ç”¨æˆ·å¤´åƒå­—æ®µä½ç½®
                basic_data.append(author_avatar)
            elif len(basic_data) == 11:  # ç”¨æˆ·åå­—æ®µä½ç½®
                basic_data.append(author_name)
            else:
                basic_data.append("")

        return basic_data

    def _export_data(self):
        """å¯¼å‡ºæ•°æ®ï¼ˆç§»é™¤éªŒè¯æ­¥éª¤ï¼ŒAPIæ•°æ®æ— éœ€éªŒè¯ï¼‰"""
        print("å¼€å§‹å¯¼å‡ºæ•°æ®...")

        try:
            # å¯¼å‡ºExcel
            excel_file = self.exporter.export_to_excel(self.projects_data, self.config.EXCEL_FILENAME)

            # å¯¼å‡ºJSON
            json_file = self.exporter.export_to_json(self.projects_data, self.config.JSON_FILENAME)

            # å¯¼å‡ºæ‘˜è¦æŠ¥å‘Š
            stats = self.monitor.get_current_stats()
            summary_file = self.exporter.export_summary_report(self.projects_data, stats)

            # ä¿å­˜ç»Ÿè®¡æŠ¥å‘Šåˆ°ç»Ÿä¸€çš„æŠ¥å‘Šç›®å½•
            stats_file = f"data/reports/stats/spider_stats_{time.strftime('%Y%m%d_%H%M%S')}.json"
            self.monitor.save_stats(stats_file)

            print(f"æ•°æ®å¯¼å‡ºå®Œæˆ:")
            print(f"  Excelæ–‡ä»¶: {excel_file}")
            print(f"  JSONæ–‡ä»¶: {json_file}")
            print(f"  æ‘˜è¦æŠ¥å‘Š: {summary_file}")
            print(f"  ç»Ÿè®¡æŠ¥å‘Š: {stats_file}")

        except Exception as e:
            print(f"æ•°æ®å¯¼å‡ºå¤±è´¥: {e}")
            self.monitor.record_error("export_error", str(e))

    def get_crawl_stats(self) -> Dict[str, Any]:
        """è·å–çˆ¬å–ç»Ÿè®¡ä¿¡æ¯"""
        stats = self.monitor.get_current_stats()
        stats.update({
            "projects_data_count": len(self.projects_data),
            "failed_urls_count": len(self.failed_urls),
            "cache_stats": self.cache_utils.get_cache_stats(),
            "network_stats": self.network_utils.get_request_stats(),
            "export_stats": self.exporter.get_export_stats()
        })
        return stats

    def retry_failed_projects(self) -> bool:
        """é‡è¯•å¤±è´¥çš„é¡¹ç›®"""
        if not self.failed_urls:
            print("æ²¡æœ‰å¤±è´¥çš„é¡¹ç›®éœ€è¦é‡è¯•")
            return True

        print(f"å¼€å§‹é‡è¯• {len(self.failed_urls)} ä¸ªå¤±è´¥çš„é¡¹ç›®...")

        # é‡æ–°æ„é€ é¡¹ç›®ä¿¡æ¯
        retry_projects = []
        for url in self.failed_urls:
            project_id = self.data_utils.extract_project_id(url)
            retry_projects.append((url, project_id, "é‡è¯•é¡¹ç›®", "none"))

        # æ¸…ç©ºå¤±è´¥åˆ—è¡¨
        self.failed_urls.clear()

        # é‡æ–°çˆ¬å–
        return self._crawl_project_details(retry_projects)

    def clear_cache(self):
        """æ¸…ç©ºç¼“å­˜"""
        self.cache_utils.clear_cache()

    def _save_incremental_data(self):
        """å¢é‡ä¿å­˜æ•°æ®åˆ°æ•°æ®åº“"""
        if not self.db_manager or not self.projects_data:
            return

        try:
            # è·å–æœªä¿å­˜çš„æ•°æ®
            unsaved_data = self.projects_data[self.saved_count:]

            if unsaved_data:
                # ä¿å­˜åˆ°æ•°æ®åº“
                saved_count = self.db_manager.save_projects(unsaved_data, self.current_task_id)
                self.saved_count += saved_count

                self._log("success", f"ğŸ“¦ å¢é‡ä¿å­˜: æœ¬æ¬¡ä¿å­˜ {saved_count} æ¡ï¼Œç´¯è®¡å·²ä¿å­˜ {self.saved_count} æ¡åˆ°æ•°æ®åº“")

                # ğŸ”§ ä¿®å¤ï¼šæ›´æ–°Webç›‘æ§å™¨ç»Ÿè®¡ï¼ˆæ”¯æŒå®šæ—¶ä»»åŠ¡ç›‘æ§å™¨ï¼‰
                if self.web_monitor:
                    self.web_monitor.update_stats(
                        projects_processed=self.saved_count,
                        projects_found=len(self.projects_data)
                    )

                    # ğŸ”§ ä¿®å¤ï¼šå¦‚æœæ˜¯å®šæ—¶ä»»åŠ¡ç›‘æ§å™¨ï¼Œè°ƒç”¨ä¸“é—¨çš„æ–¹æ³•
                    if hasattr(self.web_monitor, 'increment_saved_count'):
                        # è¿™æ˜¯å®šæ—¶ä»»åŠ¡ç›‘æ§å™¨ï¼Œéœ€è¦ç‰¹æ®Šå¤„ç†
                        self.web_monitor.set_final_stats(
                            projects_found=len(self.projects_data),
                            projects_saved=self.saved_count
                        )

        except Exception as e:
            self._log("error", f"å¢é‡ä¿å­˜å¤±è´¥: {e}")

    def _save_remaining_data(self):
        """ä¿å­˜å‰©ä½™çš„æœªä¿å­˜æ•°æ®"""
        if not self.db_manager or not self.projects_data:
            return

        try:
            # è·å–æœªä¿å­˜çš„æ•°æ®
            unsaved_data = self.projects_data[self.saved_count:]

            if unsaved_data:
                # ä¿å­˜åˆ°æ•°æ®åº“
                saved_count = self.db_manager.save_projects(unsaved_data, self.current_task_id)
                self.saved_count += saved_count

                self._log("success", f"ğŸ”„ æœ€ç»ˆæ£€æŸ¥: è¡¥å……ä¿å­˜ {saved_count} æ¡é—æ¼æ•°æ®ï¼Œç´¯è®¡å·²ä¿å­˜ {self.saved_count} æ¡åˆ°æ•°æ®åº“")

                # ğŸ”§ ä¿®å¤ï¼šæ›´æ–°Webç›‘æ§å™¨ç»Ÿè®¡ï¼ˆæ”¯æŒå®šæ—¶ä»»åŠ¡ç›‘æ§å™¨ï¼‰
                if self.web_monitor:
                    self.web_monitor.update_stats(
                        projects_processed=self.saved_count,
                        projects_found=len(self.projects_data)
                    )

                    # ğŸ”§ ä¿®å¤ï¼šå¦‚æœæ˜¯å®šæ—¶ä»»åŠ¡ç›‘æ§å™¨ï¼Œè°ƒç”¨ä¸“é—¨çš„æ–¹æ³•
                    if hasattr(self.web_monitor, 'set_final_stats'):
                        # è¿™æ˜¯å®šæ—¶ä»»åŠ¡ç›‘æ§å™¨ï¼Œéœ€è¦ç‰¹æ®Šå¤„ç†
                        # ğŸ”§ ä¿®å¤ï¼šç»Ÿè®¡ä¿¡æ¯åº”è¯¥æ˜¾ç¤ºå¤„ç†çš„é¡¹ç›®æ•°ï¼Œä¸æ˜¯ä¿å­˜çš„é¡¹ç›®æ•°
                        processed_count = len(self.projects_data)  # å®é™…å¤„ç†çš„é¡¹ç›®æ•°
                        self.web_monitor.set_final_stats(
                            projects_found=processed_count,
                            projects_saved=processed_count  # å¯¹äºå®šæ—¶ä»»åŠ¡ï¼Œå¤„ç†å³ä¸ºä¿å­˜
                        )
                        print(f"ğŸ“Š å®šæ—¶ä»»åŠ¡ç»Ÿè®¡æ›´æ–°: å¤„ç†{processed_count}ä¸ªé¡¹ç›®ï¼Œæ•°æ®åº“æ–°å¢{self.saved_count}ä¸ª")
            else:
                self._log("success", f"âœ… æ•°æ®ä¿å­˜å®Œæ•´æ€§æ£€æŸ¥: æ‰€æœ‰æ•°æ®å·²é€šè¿‡å¢é‡ä¿å­˜æœºåˆ¶ä¿å­˜å®Œæ¯•ï¼Œç´¯è®¡ {self.saved_count} æ¡")

        except Exception as e:
            self._log("error", f"æœ€ç»ˆä¿å­˜å¤±è´¥: {e}")

    def save_progress(self):
        """ä¿å­˜è¿›åº¦"""
        if self.projects_data:
            timestamp = time.strftime('%Y%m%d_%H%M%S')
            progress_file = f"{self.config.OUTPUT_DIR}/progress_{timestamp}.json"

            progress_data = {
                "timestamp": timestamp,
                "projects_count": len(self.projects_data),
                "saved_count": self.saved_count,
                "failed_urls": self.failed_urls,
                "stats": self.monitor.get_current_stats()
            }

            try:
                import json
                with open(progress_file, 'w', encoding='utf-8') as f:
                    json.dump(progress_data, f, ensure_ascii=False, indent=2)
                print(f"è¿›åº¦å·²ä¿å­˜åˆ°: {progress_file}")
            except Exception as e:
                print(f"ä¿å­˜è¿›åº¦å¤±è´¥: {e}")
