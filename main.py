#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ¡Œæ¸¸å¸‚åœºè°ƒç ”å·¥å…· - èåˆç‰ˆçˆ¬è™«
æ•´åˆäº†åŸå§‹ç‰ˆæœ¬çš„å®Œæ•´æ•°æ®æå–åŠŸèƒ½å’Œå¢å¼ºç‰ˆçš„ä¼˜ç§€æ¶æ„è®¾è®¡

ä¸»è¦åŠŸèƒ½ï¼š
1. å®Œæ•´çš„æ‘©ç‚¹ä¼—ç­¹æ•°æ®çˆ¬å–ï¼ˆ34ä¸ªæ•°æ®å­—æ®µï¼‰
2. è¯¦ç»†çš„é¡¹ç›®çŠ¶æ€åˆ†æå’Œä½œè€…ä¿¡æ¯æå–
3. å¤šæ ¼å¼è¾“å‡ºæ”¯æŒï¼ˆExcelã€JSONã€CSVï¼‰
4. å®Œå–„çš„é”™è¯¯å¤„ç†å’Œæ—¥å¿—è®°å½•
5. é…ç½®åŒ–ç®¡ç†å’Œç»Ÿè®¡ç›‘æ§
6. ç°ä»£åŒ–çš„ä»£ç æ¶æ„å’Œç½‘ç»œè¯·æ±‚å¤„ç†
"""

import random
import re
import socket
import time
import ssl
import json
import csv
import logging
import datetime
import urllib.request
import urllib.error
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple

import requests
import xlwt
import pandas as pd
from bs4 import BeautifulSoup
from openpyxl import Workbook
from openpyxl.utils.exceptions import IllegalCharacterError

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('modian_spider.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


class ModianSpiderConfig:
    """æ‘©ç‚¹çˆ¬è™«é…ç½®ç±»"""
    
    def __init__(self, category: str = "all"):
        # åŸºç¡€URLé…ç½®
        self.BASE_DOMAIN = "https://zhongchou.modian.com"
        self.AUTHOR_API_URL = "https://apim.modian.com/apis/comm/user/user_info"

        # åˆ†ç±»URLæ˜ å°„ï¼ˆåŸºäºæ‘©ç‚¹ç½‘ç«™å®é™…åˆ†ç±»ï¼‰
        self.CATEGORY_URLS = {
            # åŸºç¡€åˆ†ç±»
            "all": "/all/top_time/all/",
            "success": "/all/top_time/success/",
            "going": "/all/top_time/going/",
            "preheat": "/all/top_time/preheat/",
            "idea": "/all/top_time/idea/",

            # å…·ä½“é¡¹ç›®åˆ†ç±»
            "games": "/games/top_time/all/",
            "publishing": "/publishing/top_time/all/",
            "tablegames": "/tablegames/top_time/all/",
            "toys": "/toys/top_time/all/",
            "cards": "/cards/top_time/all/",
            "technology": "/technology/top_time/all/",
            "film-video": "/film-video/top_time/all/",
            "music": "/music/top_time/all/",
            "activities": "/activities/top_time/all/",
            "design": "/design/top_time/all/",
            "curio": "/curio/top_time/all/",
            "home": "/home/top_time/all/",
            "food": "/food/top_time/all/",
            "comics": "/comics/top_time/all/",
            "charity": "/charity/top_time/all/",
            "animals": "/animals/top_time/all/",
            "wishes": "/wishes/top_time/all/",
            "others": "/others/top_time/all/"
        }

        # è®¾ç½®å½“å‰åˆ†ç±»
        self.category = category if category in self.CATEGORY_URLS else "all"
        self.BASE_URL = f"{self.BASE_DOMAIN}{self.CATEGORY_URLS[self.category]}"
        
        # è¾“å‡ºé…ç½®
        self.OUTPUT_DIR = Path("output")
        self.CACHE_DIR = Path("cache")
        self.EXCEL_FILENAME = "æ‘©ç‚¹ä¼—ç­¹-ä¸»è¦ä¿¡æ¯.xls"
        
        # è¯·æ±‚é…ç½®
        self.MAX_RETRIES = 5
        self.RETRY_DELAY = 2
        self.REQUEST_TIMEOUT = (10, 20)
        self.MAX_PAGES = 3  # é»˜è®¤æµ‹è¯•èŒƒå›´ï¼Œå¯ä¿®æ”¹ä¸ºæ›´å¤§å€¼å¦‚833
        self.SAVE_INTERVAL = 5  # æ¯5é¡µä¿å­˜ä¸€æ¬¡
        
        # è¯·æ±‚å¤´é…ç½®
        self.HEADERS = {
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
            "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
            "Accept-Encoding": "gzip, deflate, br",
            "Connection": "keep-alive",
            "Upgrade-Insecure-Requests": "1"
        }
        
        # ç§»åŠ¨ç«¯è¯·æ±‚å¤´ï¼ˆç”¨äºä½œè€…é¡µé¢ï¼‰
        self.MOBILE_HEADERS = {
            'User-Agent': 'Mozilla/5.0 (iPhone; CPU iPhone OS 15_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.0 Mobile/15E148 Safari/604.1',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
        }
        
        # APIè¯·æ±‚å¤´
        self.API_HEADERS = {
            'Accept': 'application/json, text/plain, */*',
            'User-Agent': 'Mozilla/5.0 (Linux; Android 10; SM-G975F) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.82 Mobile Safari/537.36',
            'Origin': 'https://m.modian.com',
            'Referer': 'https://m.modian.com/',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8,en-GB;q=0.7,en-US;q=0.6',
        }
        
        # æ­£åˆ™è¡¨è¾¾å¼
        self.USER_ID_PATTERN = re.compile(r'https://me.modian.com/u/detail\?uid=(\d+)')
        self.LINK_ID_PATTERN = re.compile(r'https://zhongchou.modian.com/item/(\d+).html')
        
        # åˆ›å»ºå¿…è¦ç›®å½•
        self.OUTPUT_DIR.mkdir(exist_ok=True)
        self.CACHE_DIR.mkdir(exist_ok=True)

    def get_page_url(self, page: int) -> str:
        """è·å–æŒ‡å®šé¡µé¢çš„URL"""
        return f"{self.BASE_URL}{page}"

    def set_category(self, category: str):
        """è®¾ç½®çˆ¬å–åˆ†ç±»"""
        if category in self.CATEGORY_URLS:
            self.category = category
            self.BASE_URL = f"{self.BASE_DOMAIN}{self.CATEGORY_URLS[category]}"
        else:
            logger.warning(f"æœªçŸ¥åˆ†ç±»: {category}ï¼Œä½¿ç”¨é»˜è®¤åˆ†ç±» 'all'")
            self.category = "all"
            self.BASE_URL = f"{self.BASE_DOMAIN}{self.CATEGORY_URLS['all']}"


class ModianSpiderStats:
    """çˆ¬è™«ç»Ÿè®¡ç±»"""
    
    def __init__(self):
        self.start_time = time.time()
        self.total_requests = 0
        self.successful_requests = 0
        self.failed_requests = 0
        self.projects_found = 0
        self.projects_processed = 0
        self.pages_processed = 0
        self.errors = {}
    
    def record_request(self, success: bool):
        """è®°å½•è¯·æ±‚ç»“æœ"""
        self.total_requests += 1
        if success:
            self.successful_requests += 1
        else:
            self.failed_requests += 1
    
    def record_error(self, error_type: str, message: str):
        """è®°å½•é”™è¯¯"""
        if error_type not in self.errors:
            self.errors[error_type] = []
        self.errors[error_type].append(message)
    
    def get_summary(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡æ‘˜è¦"""
        elapsed_time = time.time() - self.start_time
        return {
            "elapsed_time": elapsed_time,
            "total_requests": self.total_requests,
            "successful_requests": self.successful_requests,
            "failed_requests": self.failed_requests,
            "success_rate": (self.successful_requests / max(self.total_requests, 1)) * 100,
            "projects_found": self.projects_found,
            "projects_processed": self.projects_processed,
            "pages_processed": self.pages_processed,
            "avg_time_per_page": elapsed_time / max(self.pages_processed, 1),
            "errors": self.errors
        }


class ModianSpider:
    """æ‘©ç‚¹ä¼—ç­¹çˆ¬è™«ä¸»ç±»"""
    
    def __init__(self, config: Optional[ModianSpiderConfig] = None):
        self.config = config or ModianSpiderConfig()
        self.stats = ModianSpiderStats()
        self.session = requests.Session()
        self.session.headers.update(self.config.HEADERS)
        
        # SSLé…ç½®
        self.ssl_context = ssl.create_default_context()
        self.ssl_context.check_hostname = False
        self.ssl_context.verify_mode = ssl.CERT_NONE
        
        logger.info("æ‘©ç‚¹ä¼—ç­¹çˆ¬è™«åˆå§‹åŒ–å®Œæˆ")
    
    def make_request(self, url: str, use_mobile: bool = False) -> Optional[str]:
        """å‘èµ·ç½‘ç»œè¯·æ±‚"""
        headers = self.config.MOBILE_HEADERS if use_mobile else self.config.HEADERS
        
        for attempt in range(self.config.MAX_RETRIES):
            try:
                timeout = random.randint(*self.config.REQUEST_TIMEOUT)
                
                # ä½¿ç”¨requestsè¿›è¡Œè¯·æ±‚
                response = self.session.get(url, headers=headers, timeout=timeout, verify=False)
                response.raise_for_status()
                response.encoding = 'utf-8'
                
                self.stats.record_request(True)
                return response.text
                
            except Exception as e:
                logger.warning(f"ç¬¬{attempt + 1}æ¬¡è¯·æ±‚å¤±è´¥ {url}: {e}")
                self.stats.record_request(False)
                self.stats.record_error("network_error", str(e))
                
                if attempt == self.config.MAX_RETRIES - 1:
                    logger.error(f"è¯·æ±‚æœ€ç»ˆå¤±è´¥ {url}")
                    return None
                
                wait_time = self.config.RETRY_DELAY * (attempt + 1)
                logger.info(f"ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                time.sleep(wait_time)
        
        return None
    
    def askURL(self, url):
        """å…¼å®¹æ—§ç‰ˆæœ¬çš„è¯·æ±‚æ–¹æ³•ï¼ˆä½¿ç”¨urllibï¼‰"""
        head = {
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        }
        timeout_range = (10, 20)

        html = ""
        for i in range(5):  # Increased retry attempts
            try:
                timeout = random.randint(*timeout_range)
                request = urllib.request.Request(url, headers=head)
                response = urllib.request.urlopen(request, timeout=timeout, context=self.ssl_context)
                html = response.read().decode("utf-8")
                self.stats.record_request(True)
                break
            except (urllib.error.URLError, ConnectionResetError, socket.timeout, ssl.SSLError) as e:
                logger.warning(f'ç¬¬{i + 1}æ¬¡å°è¯•å¤±è´¥ï¼ŒåŸå› ï¼š{e} URL: {url}')
                self.stats.record_request(False)
                self.stats.record_error("network_error", str(e))
                if i == 4:  # Last attempt
                    logger.error(f'é‡è¯•å¤šæ¬¡ä»ç„¶å¤±è´¥ï¼URL: {url}')
                    break
                # Exponential backoff
                wait_time = (i + 1) * 2
                logger.info(f'ç­‰å¾… {wait_time} ç§’åé‡è¯•...')
                time.sleep(wait_time)
        return html

    def askURL2(self, url):
        """ç§»åŠ¨ç«¯è¯·æ±‚æ–¹æ³•ï¼ˆç”¨äºä½œè€…é¡µé¢ï¼‰"""
        head = {
            'User-Agent': 'Mozilla/5.0 (iPhone; CPU iPhone OS 15_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.0 Mobile/15E148 Safari/604.1'
        }
        timeout_range = (10, 20)

        html = ""
        for i in range(5):  # Increased retry attempts
            try:
                timeout = random.randint(*timeout_range)
                request = urllib.request.Request(url, headers=head)
                response = urllib.request.urlopen(request, timeout=timeout, context=self.ssl_context)
                html = response.read().decode("utf-8")
                self.stats.record_request(True)
                break
            except (urllib.error.URLError, ConnectionResetError, socket.timeout, ssl.SSLError) as e:
                logger.warning(f'ç¬¬{i + 1}æ¬¡å°è¯•(askURL2)å¤±è´¥ï¼ŒåŸå› ï¼š{e} URL: {url}')
                self.stats.record_request(False)
                self.stats.record_error("network_error", str(e))
                if i == 4:  # Last attempt
                    logger.error(f'é‡è¯•å¤šæ¬¡(askURL2)ä»ç„¶å¤±è´¥ï¼URL: {url}')
                    break
                # Exponential backoff
                wait_time = (i + 1) * 2
                logger.info(f'ç­‰å¾… {wait_time} ç§’åé‡è¯•...')
                time.sleep(wait_time)
        return html

    def get_author_info_from_api(self, uid):
        """ä»APIè·å–ä½œè€…ä¿¡æ¯"""
        params = {"json_type": 1, "to_user_id": uid, "user_id": uid}
        headers = self.config.API_HEADERS.copy()
        headers['Timestamp'] = str(int(time.time()))

        try:
            response = requests.get(self.config.AUTHOR_API_URL, params=params, headers=headers, timeout=10)
            response.raise_for_status()
            self.stats.record_request(True)
            return response.json()
        except requests.exceptions.RequestException as e:
            logger.error(f"Error fetching author API for UID {uid}: {e}")
            self.stats.record_request(False)
            self.stats.record_error("api_error", str(e))
            return {}

    def parse_author_page_info(self, html, author_id_from_url):
        """è§£æä½œè€…é¡µé¢ä¿¡æ¯"""
        soup = BeautifulSoup(html, "html.parser")
        author_data = []

        fans_num = 0
        notice_number = 0
        love_number = 0

        banner_div = soup.find('div', {'class': 'banner'})
        if banner_div:
            cont_div = banner_div.find('div', {'class': 'cont'})
            if cont_div:
                fans_span = cont_div.find('span', {'class': 'go_span fans'})
                if fans_span and fans_span.find('i'):
                    fans_num_text = fans_span.find('i').text.strip()
                    if fans_num_text.isdigit():
                        fans_num = int(fans_num_text)

                notice_span = cont_div.select_one('span.go_span:not(.fans)')
                if notice_span:
                    text = notice_span.text.strip()
                    parts = text.split()
                    if parts and parts[0].isdigit():
                        notice_number = int(parts[0])

                all_span = cont_div.find('span', {'id': 'ALL'})
                if all_span:
                    text2 = all_span.text.strip()
                    parts2 = text2.split()
                    if parts2 and parts2[0].isdigit():
                        love_number = int(parts2[0])

        author_data.extend([fans_num, notice_number, love_number])

        detail_result = {}
        detail_div = soup.find('div', {'class': 'detail'})
        if detail_div:
            for item_div in detail_div.find_all('div', class_='item'):
                label_tag = item_div.find('label')
                p_tag = item_div.find('p')
                if label_tag and p_tag:
                    detail_result[label_tag.text.strip()] = p_tag.text.strip()
        author_data.append(str(detail_result))

        other_result = {}
        other_info_div = soup.find('div', {'class': 'other_info'})
        if other_info_div:
            for item_div in other_info_div.find_all('div', class_='item'):
                p_tags = item_div.find_all('p')
                if len(p_tags) == 2 and p_tags[1].text.strip().isdigit():
                    value = int(p_tags[1].text.strip())
                    key = p_tags[0].text.strip()
                    other_result[key] = value
        author_data.append(str(other_result))

        userhome_url = f"https://m.modian.com/user/homePage/{author_id_from_url}"
        author_data.append(userhome_url)

        return author_data

    def get_project_status_info(self, soup_detail_page):
        """è·å–é¡¹ç›®çŠ¶æ€ä¿¡æ¯"""
        status_info = {
            "item_class": "æœªçŸ¥æƒ…å†µ",
            "is_idea": False,
            "is_preheat": False,
            "is_going": False,
            "is_success": False,
            "is_fail": False # Added for clarity
        }
        buttons_div = soup_detail_page.find('div', {'class': 'buttons clearfloat'})
        if buttons_div:
            button_a = buttons_div.select_one('a')
            if button_a:
                class_result_text = button_a.text.strip()
                status_info["raw_status_text"] = class_result_text # Store raw text for debugging
                if class_result_text == "çœ‹å¥½":
                    status_info["item_class"] = "åˆ›æ„"
                    status_info["is_idea"] = True
                elif class_result_text == "çœ‹å¥½é¡¹ç›®":
                    status_info["item_class"] = "é¢„çƒ­"
                    status_info["is_preheat"] = True
                elif class_result_text == "ç«‹å³è´­ä¹°æ”¯æŒ":
                    status_info["item_class"] = "ä¼—ç­¹ä¸­"
                    status_info["is_going"] = True
                elif class_result_text == "ä¼—ç­¹æˆåŠŸ":
                    status_info["item_class"] = "ä¼—ç­¹æˆåŠŸ"
                    status_info["is_success"] = True
                elif class_result_text == "é¡¹ç›®ç»ˆæ­¢":
                    status_info["item_class"] = "é¡¹ç›®ç»ˆæ­¢" # Often implies success or specific end state
                    status_info["is_success"] = True # Or a different flag if needed
                elif class_result_text == "ä¼—ç­¹ç»“æŸ": # This usually means failed if not successful
                    status_info["item_class"] = "ä¼—ç­¹å¤±è´¥"
                    status_info["is_fail"] = True
                    status_info["is_going"] = True # Was likely 'going' before ending as fail
                elif class_result_text == "ä¼—ç­¹å–æ¶ˆ":
                    status_info["item_class"] = "ä¼—ç­¹å–æ¶ˆ"
                    status_info["is_fail"] = True # Or a different flag
                    status_info["is_going"] = True # Was likely 'going' before cancel
        return status_info

    def parse_upper_items(self, soup_detail_page, project_status):
        """è§£æé¡¹ç›®ä¸Šéƒ¨ä¿¡æ¯ï¼ˆæ—¶é—´ã€ä½œè€…ã€åŸºç¡€èµ„é‡‘ä¿¡æ¯ï¼‰- ä¿®å¤ç‰ˆ"""
        data = []
        starttime = "none"
        endtime = "none"
        itemreal_class = project_status["item_class"]

        # ğŸ”§ ä¼˜åŒ–æ—¶é—´ä¿¡æ¯æå– - ä»é¡µé¢æ–‡æœ¬ä¸­æå–
        page_text = soup_detail_page.get_text()

        # æå–å¼€å§‹æ—¶é—´
        start_time_patterns = [
            r'å¼€å§‹æ—¶é—´.*?(\d{4}-\d{1,2}-\d{1,2}\s+\d{1,2}:\d{2})',
            r'(\d{4}-\d{1,2}-\d{1,2}\s+\d{1,2}:\d{2}).*?å¼€å§‹',
            r'å¼€å§‹.*?(\d{4}-\d{1,2}-\d{1,2})',
            r'(\d{4}-\d{1,2}-\d{1,2}).*?å¼€å§‹'
        ]

        for pattern in start_time_patterns:
            start_match = re.search(pattern, page_text)
            if start_match:
                starttime = start_match.group(1)
                logger.info(f"æ‰¾åˆ°å¼€å§‹æ—¶é—´: {starttime}")
                break

        # æå–ç»“æŸæ—¶é—´
        end_time_patterns = [
            r'ç»“æŸæ—¶é—´.*?(\d{4}-\d{1,2}-\d{1,2}\s+\d{1,2}:\d{2})',
            r'(\d{4}-\d{1,2}-\d{1,2}\s+\d{1,2}:\d{2}).*?ç»“æŸ',
            r'å‰©ä½™æ—¶é—´.*?(\d{4}-\d{1,2}-\d{1,2})',
            r'(\d{4}-\d{1,2}-\d{1,2}).*?å‰è¾¾åˆ°'
        ]

        for pattern in end_time_patterns:
            end_match = re.search(pattern, page_text)
            if end_match:
                endtime = end_match.group(1)
                logger.info(f"æ‰¾åˆ°ç»“æŸæ—¶é—´: {endtime}")
                break

        # æ ¹æ®é¡¹ç›®çŠ¶æ€è®¾ç½®é»˜è®¤æ—¶é—´
        if project_status["is_preheat"]:
            if starttime == "none":
                starttime = "é¢„çƒ­ä¸­"
            if endtime == "none":
                endtime = "é¢„çƒ­ä¸­"
        elif project_status["is_idea"]:
            starttime = "åˆ›æ„ä¸­"
            endtime = "åˆ›æ„ä¸­"

        data.extend([starttime, endtime, itemreal_class])

        # ğŸ”§ ä¼˜åŒ–ä½œè€…ä¿¡æ¯æå– - ä»é¡µé¢æ–‡æœ¬ä¸­æå–
        sponsor_href = "none"
        true_authorid_from_re = "none"
        author_image = "none"
        category = "none"
        author_name = "none"
        author_uid_attr = "0"
        parsed_author_page_details = ["0", "0", "0", "{}", "{}", "none"]

        # ä»é¡µé¢æ–‡æœ¬ä¸­æå–ä½œè€…åç§° - æŸ¥æ‰¾"å‘èµ·äº†è¿™ä¸ªé¡¹ç›®"å‰çš„æ–‡æœ¬
        author_match = re.search(r'([^\n]+)\s*å‘èµ·äº†è¿™ä¸ªé¡¹ç›®', page_text)
        if author_match:
            author_name = author_match.group(1).strip()
            logger.info(f"æ‰¾åˆ°ä½œè€…åç§°: {author_name}")

        # ä»é¡µé¢æ–‡æœ¬ä¸­æå–é¡¹ç›®åˆ†ç±» - "é¡¹ç›®ç±»åˆ«ï¼šæ¡Œæ¸¸"
        category_match = re.search(r'é¡¹ç›®ç±»åˆ«[ï¼š:]\s*([^\n\r]+)', page_text)
        if category_match:
            category = category_match.group(1).strip()
            logger.info(f"æ‰¾åˆ°é¡¹ç›®åˆ†ç±»: {category}")

        # æŸ¥æ‰¾ä½œè€…é“¾æ¥ - æŸ¥æ‰¾åŒ…å«uidçš„é“¾æ¥
        author_links = soup_detail_page.find_all('a', href=re.compile(r'uid=\d+'))
        if author_links:
            sponsor_href = author_links[0].get('href')
            if not sponsor_href.startswith('http'):
                sponsor_href = 'https://me.modian.com' + sponsor_href

            # æå–ç”¨æˆ·ID
            uid_match = re.search(r'uid=(\d+)', sponsor_href)
            if uid_match:
                true_authorid_from_re = uid_match.group(1)
                author_uid_attr = true_authorid_from_re
                logger.info(f"æ‰¾åˆ°ä½œè€…UID: {true_authorid_from_re}")

                # è·å–ä½œè€…é¡µé¢ä¿¡æ¯
                try:
                    author_page_html = self.askURL2(sponsor_href)
                    if author_page_html:
                        parsed_author_page_details = self.parse_author_page_info(author_page_html, true_authorid_from_re)
                except Exception as e:
                    logger.warning(f"è·å–ä½œè€…é¡µé¢å¤±è´¥: {e}")

        # æŸ¥æ‰¾ä½œè€…å¤´åƒ
        author_imgs = soup_detail_page.find_all('img')
        for img in author_imgs:
            src = img.get('src')
            if src and ('avatar' in src or 'dst_avatar' in src):
                author_image = src
                if not author_image.startswith('http'):
                    author_image = 'https:' + author_image
                logger.info(f"æ‰¾åˆ°ä½œè€…å¤´åƒ: {author_image[:50]}...")
                break


        data.append(sponsor_href) # User homepage link
        data.append(author_image)
        data.append(category)
        data.append(author_name)
        data.append(author_uid_attr) # This is the one from data-username, often the same as true_authorid_from_re

        # ğŸ”§ ä¼˜åŒ–ä¼—ç­¹ä¿¡æ¯æå– - ä»é¡µé¢æ–‡æœ¬ä¸­æå–
        money = "0"
        percent = "0"
        goal_money = "0"
        sponsor_num = "0"

        # ä»é¡µé¢æ–‡æœ¬ä¸­æå–å·²ç­¹é‡‘é¢ - å¤„ç†ç¼–ç é—®é¢˜
        money_patterns = [
            r'å·²ç­¹[Â¥ï¿¥Ã‚Â¥]([0-9,]+)',  # æ­£å¸¸ç¼–ç 
            r'Ã¥Â·Â²Ã§Â­Â¹[Â¥ï¿¥Ã‚Â¥]([0-9,]+)',  # ç¼–ç åçš„ä¸­æ–‡
            r'å·²ç­¹.*?[Â¥ï¿¥Ã‚Â¥]\s*([0-9,]+)',  # å®½æ¾åŒ¹é…
            r'Ã¥Â·Â²Ã§Â­Â¹.*?[Â¥ï¿¥Ã‚Â¥]\s*([0-9,]+)'   # ç¼–ç åå®½æ¾åŒ¹é…
        ]

        for pattern in money_patterns:
            money_match = re.search(pattern, page_text)
            if money_match:
                money = money_match.group(1).replace(',', '')
                logger.info(f"æ‰¾åˆ°å·²ç­¹é‡‘é¢: Â¥{money}")
                break

        # ä»é¡µé¢æ–‡æœ¬ä¸­æå–ç›®æ ‡é‡‘é¢
        goal_patterns = [
            r'ç›®æ ‡é‡‘é¢\s*[Â¥ï¿¥Ã‚Â¥]([0-9,]+)',  # æ­£å¸¸ç¼–ç 
            r'Ã§Â®Ã¦ Ã©Ã©Â¢\s*[Â¥ï¿¥Ã‚Â¥]([0-9,]+)',  # ç¼–ç åçš„ä¸­æ–‡
            r'ç›®æ ‡é‡‘é¢.*?[Â¥ï¿¥Ã‚Â¥]\s*([0-9,]+)',  # å®½æ¾åŒ¹é…
            r'Ã§Â®Ã¦ Ã©Ã©Â¢.*?[Â¥ï¿¥Ã‚Â¥]\s*([0-9,]+)',   # ç¼–ç åå®½æ¾åŒ¹é…
            r'ç›®æ ‡[Â¥ï¿¥Ã‚Â¥]([0-9,]+)',  # ç®€åŒ–æ ¼å¼
            r'Ã§Â®Ã¦[Â¥ï¿¥Ã‚Â¥]([0-9,]+)',  # ç¼–ç åç®€åŒ–æ ¼å¼
            r'[Â¥ï¿¥Ã‚Â¥]([0-9,]+).*?ç›®æ ‡',  # åå‘åŒ¹é…
            r'([0-9,]+).*?ç›®æ ‡é‡‘é¢'  # æ•°å­—åœ¨å‰
        ]

        for pattern in goal_patterns:
            goal_match = re.search(pattern, page_text)
            if goal_match:
                goal_money = goal_match.group(1).replace(',', '')
                logger.info(f"æ‰¾åˆ°ç›®æ ‡é‡‘é¢: Â¥{goal_money}")
                break

        # ä»é¡µé¢æ–‡æœ¬ä¸­æå–å®Œæˆç™¾åˆ†æ¯”
        percent_match = re.search(r'([0-9.]+)%', page_text)
        if percent_match:
            percent = percent_match.group(1)
            logger.info(f"æ‰¾åˆ°å®Œæˆç™¾åˆ†æ¯”: {percent}%")

        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç›®æ ‡é‡‘é¢ï¼Œå°è¯•ä»ç™¾åˆ†æ¯”åæ¨
        if goal_money == "0" and money != "0" and percent != "0":
            try:
                calculated_goal = float(money) * 100 / float(percent)
                goal_money = str(int(calculated_goal))
                logger.info(f"ä»ç™¾åˆ†æ¯”åæ¨ç›®æ ‡é‡‘é¢: Â¥{goal_money} (è®¡ç®—: {money} Ã— 100 Ã· {percent})")
            except Exception as e:
                logger.warning(f"ç›®æ ‡é‡‘é¢åæ¨è®¡ç®—å¤±è´¥: {e}")

        # ä»é¡µé¢æ–‡æœ¬ä¸­æå–æ”¯æŒè€…æ•°é‡
        supporter_patterns = [
            r'(\d+)äºº\s*æ”¯æŒäººæ•°',  # æ­£å¸¸ç¼–ç 
            r'(\d+)Ã¤ÂºÂº\s*Ã¦Â¯Ã¦Ã¤ÂºÂºÃ¦Â°',  # ç¼–ç åçš„ä¸­æ–‡
            r'æ”¯æŒäººæ•°\s*(\d+)',
            r'Ã¦Â¯Ã¦Ã¤ÂºÂºÃ¦Â°\s*(\d+)',
            r'(\d+)\s*äºº\s*æ”¯æŒ',
            r'(\d+)\s*Ã¤ÂºÂº\s*Ã¦Â¯Ã¦',
            r'æ”¯æŒè€…\s*(\d+)',
            r'Ã¦Â¯Ã¦Ã¨\s*(\d+)',
            r'(\d+)\s*æ”¯æŒè€…',
            r'(\d+)\s*Ã¦Â¯Ã¦Ã¨',
            r'(\d+)\s*äºº$',  # ç®€åŒ–æ ¼å¼
            r'(\d+)\s*Ã¤ÂºÂº$'  # ç¼–ç åç®€åŒ–æ ¼å¼
        ]

        for pattern in supporter_patterns:
            supporter_match = re.search(pattern, page_text)
            if supporter_match:
                sponsor_num = supporter_match.group(1)
                logger.info(f"æ‰¾åˆ°æ”¯æŒè€…æ•°é‡: {sponsor_num}äºº")
                break

        # éªŒè¯æ•°æ®åˆç†æ€§
        if money != "0" and goal_money != "0":
            try:
                calculated_percent = (float(money) / float(goal_money)) * 100
                if percent == "0":
                    percent = f"{calculated_percent:.1f}"
            except:
                pass

        logger.info(f"è§£æä¼—ç­¹ä¿¡æ¯: å·²ç­¹Â¥{money}, ç›®æ ‡Â¥{goal_money}, å®Œæˆç‡{percent}%, æ”¯æŒè€…{sponsor_num}äºº")

        data.extend([money, percent, goal_money, sponsor_num])
        data.append(true_authorid_from_re) # The ID extracted from sponsor_href
        data.extend(parsed_author_page_details) # fans_num, notice_number, love_number, detail_result_str, other_result_str, userhome_url_confirmation
        return data

    def parse_main_left_content(self, soup_detail_page):
        """è§£æé¡¹ç›®å·¦ä¾§å†…å®¹ï¼ˆå›¾ç‰‡å’Œè§†é¢‘ï¼‰- ä¿®å¤ç‰ˆ"""
        data = []
        img_list = []
        video_list = []

        # ğŸ”§ ä¼˜åŒ–åª’ä½“å†…å®¹æå– - æŸ¥æ‰¾æ‰€æœ‰ç›¸å…³åŒºåŸŸ
        content_areas = [
            soup_detail_page.find('div', {'class': 'main-left'}),
            soup_detail_page.find('div', {'class': 'project-content'}),
            soup_detail_page.find('div', {'class': 'content-wrap'}),
            soup_detail_page.find('div', {'id': 'projectDetail'}),
            soup_detail_page.find('section', {'class': 'project-detail'})
        ]

        for area in content_areas:
            if area:
                # æŸ¥æ‰¾å›¾ç‰‡
                for img_tag in area.find_all('img'):
                    src = img_tag.get('src')
                    if src and src.strip():
                        # è¿‡æ»¤æ‰å¤´åƒã€å›¾æ ‡ç­‰æ— å…³å›¾ç‰‡
                        if not any(keyword in src for keyword in [
                            'default_profile', 'icon-', 'avatar', 'logo',
                            'headPic', 'default_1x1', 'video-play'
                        ]):
                            # ç¡®ä¿URLå®Œæ•´
                            if src.startswith('//'):
                                src = 'https:' + src
                            elif src.startswith('/'):
                                src = 'https://zhongchou.modian.com' + src
                            img_list.append(src)

                # æŸ¥æ‰¾è§†é¢‘
                for video_tag in area.find_all('video'):
                    if video_tag.get('src'):
                        video_src = video_tag.get('src')
                        if video_src.startswith('//'):
                            video_src = 'https:' + video_src
                        elif video_src.startswith('/'):
                            video_src = 'https://zhongchou.modian.com' + video_src
                        video_list.append(video_src)
                    else:
                        # æŸ¥æ‰¾sourceæ ‡ç­¾
                        for source_tag in video_tag.find_all('source'):
                            if source_tag.get('src'):
                                video_src = source_tag.get('src')
                                if video_src.startswith('//'):
                                    video_src = 'https:' + video_src
                                elif video_src.startswith('/'):
                                    video_src = 'https://zhongchou.modian.com' + video_src
                                video_list.append(video_src)
                                break

                # æŸ¥æ‰¾iframeä¸­çš„è§†é¢‘ï¼ˆå¦‚Bç«™ã€ä¼˜é…·ç­‰ï¼‰
                for iframe_tag in area.find_all('iframe'):
                    src = iframe_tag.get('src')
                    if src and any(domain in src for domain in [
                        'bilibili', 'youku', 'qq.com', 'iqiyi', 'tudou', 'youtube'
                    ]):
                        if src.startswith('//'):
                            src = 'https:' + src
                        video_list.append(src)

        # ğŸ”§ ä»é¡µé¢æ–‡æœ¬ä¸­æŸ¥æ‰¾è§†é¢‘é“¾æ¥
        page_text = soup_detail_page.get_text()
        video_url_patterns = [
            r'https?://[^\s]*\.mp4',
            r'https?://[^\s]*\.avi',
            r'https?://[^\s]*\.mov',
            r'https?://[^\s]*\.wmv',
            r'https?://mediac\d+\.modian\.com/[^\s]*\.mp4'
        ]

        for pattern in video_url_patterns:
            video_matches = re.findall(pattern, page_text)
            for match in video_matches:
                video_list.append(match)

        # å»é‡å¹¶è¿‡æ»¤
        img_list = list(set([img for img in img_list if img and len(img) > 10]))
        video_list = list(set([video for video in video_list if video and len(video) > 10]))

        logger.info(f"æ‰¾åˆ°é¡¹ç›®åª’ä½“: {len(img_list)}å¼ å›¾ç‰‡, {len(video_list)}ä¸ªè§†é¢‘")

        data.extend([len(img_list), str(img_list), len(video_list), str(video_list)])
        return data

    def parse_main_right_rewards(self, soup_detail_page):
        """è§£æé¡¹ç›®å³ä¾§å›æŠ¥ä¿¡æ¯"""
        rewards_data_list_str = [] # List of strings, each representing a reward tier
        main_right_div = soup_detail_page.find('div', {'class': 'main-right'})
        if main_right_div:
            payback_lists_div = main_right_div.find('div', {'class': 'payback-lists margin36'})
            if payback_lists_div:
                for reward_item_div in payback_lists_div.find_all('div', class_=lambda x: x and 'back-list' in x):
                    single_reward_details = []

                    back_money = "0"
                    head_div = reward_item_div.find('div', {'class': 'head'})
                    if head_div and head_div.find('span'):
                        back_money_text = head_div.find('span').text.strip().replace('ï¿¥','')
                        if back_money_text.isdigit(): back_money = back_money_text

                    backsponsor = "0"
                    if head_div and head_div.find('em'):
                        em_text = head_div.find('em').text.strip() # e.g., "23 äººæ”¯æŒ" or "å·²æ»¡"
                        parts = em_text.split()
                        if parts and parts[0].isdigit(): backsponsor = parts[0]
                        elif "å·²æ»¡" in em_text: backsponsor = "å·²æ»¡"


                    sign_logo = "0" # Default, might be "é™é‡" or number
                    zc_subhead_div = reward_item_div.find('div', {'class': 'zc-subhead'})
                    if zc_subhead_div and zc_subhead_div.find('span'):
                        sign_logo_text = zc_subhead_div.find('span').text.strip()
                        if "é™é‡" in sign_logo_text:
                            num_part = sign_logo_text.replace("é™é‡","").replace("ä»½","").strip()
                            sign_logo = f"é™é‡ {num_part}" if num_part.isdigit() else "é™é‡"
                        elif sign_logo_text.isdigit(): # Unlikely based on old regex, but good to check
                            sign_logo = sign_logo_text


                    backtitle = "none"
                    backtext = "none"
                    backtime = "none"

                    back_content_div = reward_item_div.find('div', {'class': 'back-content'})
                    if back_content_div:
                        title_div = back_content_div.find('div', {'class': 'back-sub-title'})
                        if title_div: backtitle = title_div.text.strip()

                        detail_div = back_content_div.find('div', {'class': 'back-detail'})
                        if detail_div: backtext = detail_div.text.strip()

                        time_div = back_content_div.find('div', {'class': 'back-time'})
                        if time_div: backtime = time_div.text.strip()

                    single_reward_details.extend([backtitle, sign_logo, back_money, backsponsor, backtime, backtext])
                    rewards_data_list_str.append(str(single_reward_details))

        return [str(rewards_data_list_str), len(rewards_data_list_str)]

    def parse_main_middle_nav_info(self, soup_detail_page, project_status):
        """è§£æé¡¹ç›®ä¸­é—´å¯¼èˆªä¿¡æ¯ï¼ˆæ›´æ–°æ•°ã€è¯„è®ºæ•°ã€æ”¯æŒè€…æ•°ã€æ”¶è—æ•°ï¼‰- æ·±åº¦ä¿®å¤ç‰ˆ"""
        data = []
        update_number = "0"
        comment_number = "0"
        userlist_num = "0"
        collect_number = "0"

        # ğŸ”§ ä¼˜åŒ–ç­–ç•¥ï¼šä»é¡µé¢æ–‡æœ¬ä¸­ç›´æ¥æå–æ•°å­—ä¿¡æ¯
        page_text = soup_detail_page.get_text()

        # æå–æ›´æ–°æ•° - æŸ¥æ‰¾"é¡¹ç›®æ›´æ–° X"æ¨¡å¼
        update_patterns = [
            r'é¡¹ç›®æ›´æ–°\s*(\d+)',
            r'æ›´æ–°\s*(\d+)',
            r'(\d+)\s*æ¬¡æ›´æ–°',
            r'æ›´æ–°.*?(\d+)'
        ]

        for pattern in update_patterns:
            update_match = re.search(pattern, page_text)
            if update_match:
                update_number = update_match.group(1)
                logger.info(f"æ‰¾åˆ°æ›´æ–°æ•°: {update_number}")
                break

        # æå–è¯„è®ºæ•° - æŸ¥æ‰¾"è¯„è®º X"æ¨¡å¼
        comment_patterns = [
            r'è¯„è®º\s*(\d+)',
            r'(\d+)\s*æ¡è¯„è®º',
            r'è¯„è®º.*?(\d+)',
            r'(\d+)\s*è¯„è®º'
        ]

        for pattern in comment_patterns:
            comment_match = re.search(pattern, page_text)
            if comment_match:
                comment_number = comment_match.group(1)
                logger.info(f"æ‰¾åˆ°è¯„è®ºæ•°: {comment_number}")
                break

        # æå–æ”¯æŒè€…æ•° - æŸ¥æ‰¾"æ”¯æŒè€… X"æˆ–"Xäºº"æ¨¡å¼
        supporter_patterns = [
            r'æ”¯æŒè€…\s*(\d+)',
            r'(\d+)\s*äºº\s*æ”¯æŒ',
            r'(\d+)\s*æ”¯æŒè€…',
            r'æ”¯æŒäººæ•°.*?(\d+)',
            r'(\d+)\s*äºº$'  # è¡Œæœ«çš„æ•°å­—+äºº
        ]

        for pattern in supporter_patterns:
            supporter_match = re.search(pattern, page_text)
            if supporter_match:
                userlist_num = supporter_match.group(1)
                logger.info(f"æ‰¾åˆ°æ”¯æŒè€…æ•°: {userlist_num}")
                break

        # æå–æ”¶è—æ•° - æŸ¥æ‰¾æ”¶è—ç›¸å…³æ•°å­—
        collect_patterns = [
            r'æ”¶è—\s*(\d+)',
            r'(\d+)\s*æ”¶è—',
            r'å…³æ³¨\s*(\d+)',
            r'(\d+)\s*å…³æ³¨'
        ]

        for pattern in collect_patterns:
            collect_match = re.search(pattern, page_text)
            if collect_match:
                collect_number = collect_match.group(1)
                logger.info(f"æ‰¾åˆ°æ”¶è—æ•°: {collect_number}")
                break

        # ğŸ”§ å›é€€åˆ°ä¼ ç»ŸDOMè§£æï¼ˆå¦‚æœæ–‡æœ¬è§£æå¤±è´¥ï¼‰
        if all(x == "0" for x in [update_number, comment_number, userlist_num, collect_number]):
            logger.info("æ–‡æœ¬è§£æå¤±è´¥ï¼Œå›é€€åˆ°DOMè§£æ")
            nav_wrap_inner = soup_detail_page.find('div', {'class': 'nav-wrap-inner'})
            if nav_wrap_inner:
                nav_left = nav_wrap_inner.find('ul', {'class': 'nav-left'})
                if nav_left:
                    # æ›´æ–°æ•°
                    update_li = nav_left.find('li', {'class': 'pro-gengxin'})
                    if update_li:
                        li_text = update_li.get_text()
                        numbers = re.findall(r'\d+', li_text)
                        if numbers:
                            update_number = numbers[-1]

                    # è¯„è®ºæ•°
                    comment_li = nav_left.find('li', {'class': 'nav-comment'})
                    if comment_li:
                        li_text = comment_li.get_text()
                        numbers = re.findall(r'\d+', li_text)
                        if numbers:
                            comment_number = numbers[-1]

                    # æ”¯æŒè€…æ•°
                    userlist_li = nav_left.find('li', class_='dialog_user_list')
                    if userlist_li:
                        li_text = userlist_li.get_text()
                        numbers = re.findall(r'\d+', li_text)
                        if numbers:
                            userlist_num = numbers[-1]

                # æ”¶è—æ•°
                nav_right = nav_wrap_inner.find('ul', {'class': 'nav-right'})
                if nav_right:
                    atten_li = nav_right.find('li', {'class': 'atten'})
                    if atten_li:
                        li_text = atten_li.get_text()
                        numbers = re.findall(r'\d+', li_text)
                        if numbers:
                            collect_number = numbers[-1]

        # ğŸ”§ æ•°æ®æ¸…ç†å’ŒéªŒè¯
        def clean_number(num_str, field_name=""):
            """å¢å¼ºçš„æ•°å­—æ¸…ç†å‡½æ•°"""
            if not num_str:
                return "0"

            num_str = str(num_str).strip()
            cleaned = re.sub(r'[^\d]', '', num_str)

            if cleaned and cleaned.isdigit():
                return cleaned
            else:
                return "0"

        update_number = clean_number(update_number, "æ›´æ–°æ•°")
        comment_number = clean_number(comment_number, "è¯„è®ºæ•°")
        userlist_num = clean_number(userlist_num, "æ”¯æŒè€…æ•°")
        collect_number = clean_number(collect_number, "æ”¶è—æ•°")

        logger.info(f"å¯¼èˆªä¿¡æ¯è§£æç»“æœ: æ›´æ–°æ•°={update_number}, è¯„è®ºæ•°={comment_number}, æ”¯æŒè€…æ•°={userlist_num}, æ”¶è—æ•°={collect_number}")

        data.extend([update_number, comment_number, userlist_num, collect_number])
        return data

    def parse_project_detail_page(self, html_content):
        """è§£æé¡¹ç›®è¯¦æƒ…é¡µé¢"""
        soup = BeautifulSoup(html_content, "html.parser")
        project_data = []

        project_status = self.get_project_status_info(soup)

        # Upper items: time, author, basic funding info
        upper_items_data = self.parse_upper_items(soup, project_status)
        project_data.extend(upper_items_data)

        # Right items: Rewards
        main_right_data = self.parse_main_right_rewards(soup)
        project_data.extend(main_right_data)

        # Middle items: Nav counts (updates, comments, supporters/likes, collections)
        main_middle_data = self.parse_main_middle_nav_info(soup, project_status)
        project_data.extend(main_middle_data)

        # Left items: Project content images/videos
        main_left_data = self.parse_main_left_content(soup)
        project_data.extend(main_left_data)

        return project_data

    def parse_main_listing_page(self, html_content, current_excel_index_ref):
        """è§£æä¸»åˆ—è¡¨é¡µé¢"""
        soup = BeautifulSoup(html_content, "html.parser")
        datalist_batch = []

        pro_field_div = soup.find('div', {'class': 'pro_field'})
        if not pro_field_div:
            logger.warning("No 'pro_field' div found on listing page.")
            return datalist_batch, current_excel_index_ref

        # Find all project items - they are in <li> elements within the pro_field div
        project_items = pro_field_div.find_all('li')
        if not project_items:
            logger.warning("No project items found in pro_field div.")
            return datalist_batch, current_excel_index_ref

        for item_li in project_items:
            current_excel_index_ref += 1 # Increment shared index
            single_project_base_data = [current_excel_index_ref] # Start with new auto-incremented index

            # Find the project link - look for any <a> tag with href containing "/item/"
            link_tag = None
            item_link = "none"
            item_id = ""

            # Try to find the main project link
            for a_tag in item_li.find_all('a'):
                href = a_tag.get('href', '')
                if '/item/' in href:
                    link_tag = a_tag
                    item_link = href
                    break

            if link_tag and item_link != "none":
                if not item_link.startswith("http"):
                    item_link = "https://zhongchou.modian.com" + item_link

                id_match = self.config.LINK_ID_PATTERN.search(item_link)
                if id_match:
                    item_id = id_match.group(1)

            single_project_base_data.append(item_link)
            single_project_base_data.append(item_id)

            # Find the project title
            title = "none"
            title_h3 = item_li.find('h3', class_='pro_title')
            if title_h3:
                title = title_h3.text.strip()
            elif link_tag and link_tag.text:
                title = link_tag.text.strip()

            # Skip specific items as per original logic
            if "å¯æ±—æ¸¸æˆå¤§ä¼š" in title:
                current_excel_index_ref -= 1 # Decrement back as we are skipping
                continue

            single_project_base_data.append(title)

            # Find the project image
            img_src = "none"
            img_tag = item_li.find('img')
            if img_tag and img_tag.get('src'):
                img_src = img_tag.get('src')
            single_project_base_data.append(img_src)

            logger.info(f"Processing: {current_excel_index_ref} - {title} ({item_link})")

            # Fetch and parse detail page
            if item_link != "none" and item_id: # Only proceed if we have a valid link/ID
                detail_page_html = self.askURL(item_link)
                if detail_page_html:
                    detail_page_data = self.parse_project_detail_page(detail_page_html)
                    single_project_base_data.extend(detail_page_data)
                    self.stats.projects_processed += 1
                else:
                    logger.warning(f"Failed to fetch detail page for {item_link}")
                    # Add placeholders for detail_data if fetch fails, to maintain column consistency
                    # Number of placeholders should match expected fields from parse_project_detail_page
                    # upper (11) + right (2) + middle (4) + left (4) = 21 placeholders
                    single_project_base_data.extend(["error_fetching_details"] * 21)
            else:
                logger.warning(f"Skipping detail fetch for item without link/ID: {title}")
                single_project_base_data.extend(["no_link_for_details"] * 21)

            datalist_batch.append(single_project_base_data)

        return datalist_batch, current_excel_index_ref

    def save_data_to_excel(self, workbook, sheet, data_rows_list, start_row_idx):
        """ä¿å­˜æ•°æ®åˆ°Excelæ–‡ä»¶"""
        logger.info(f"Saving {len(data_rows_list)} rows to Excel, starting at sheet row {start_row_idx + 1}...")

        col_headers = (
            "åºå·", "é¡¹ç›®link", "é¡¹ç›®6ä½id", "é¡¹ç›®åç§°", "é¡¹ç›®å›¾",  # Base info from listing (5)
            "å¼€å§‹æ—¶é—´", "ç»“æŸæ—¶é—´", "é¡¹ç›®ç»“æœ",  # from upper (3)
            "ç”¨æˆ·ä¸»é¡µ(é“¾æ¥)", "ç”¨æˆ·å¤´åƒ(å›¾ç‰‡é“¾æ¥)", "åˆ†ç±»", "ç”¨æˆ·å", "ç”¨æˆ·UID(data-username)", # from upper (5)
            "å·²ç­¹é‡‘é¢", "ç™¾åˆ†æ¯”", "ç›®æ ‡é‡‘é¢", "æ”¯æŒè€…(æ•°é‡)", # from upper (4)
            "çœŸå®ç”¨æˆ·ID(é“¾æ¥æå–)", "ä½œè€…é¡µ-ç²‰ä¸æ•°", "ä½œè€…é¡µ-å…³æ³¨æ•°", "ä½œè€…é¡µ-è·èµæ•°", "ä½œè€…é¡µ-è¯¦æƒ…", "ä½œè€…é¡µ-å…¶ä»–ä¿¡æ¯", "ä½œè€…é¡µ-ä¸»é¡µç¡®è®¤", # from upper's author parse (7)
            "å›æŠ¥åˆ—è¡¨ä¿¡æ¯(å­—ç¬¦ä¸²)", "å›æŠ¥åˆ—è¡¨é¡¹ç›®æ•°",  # from right (2)
            "é¡¹ç›®æ›´æ–°æ•°", "è¯„è®ºæ•°", "é¡¹ç›®æ”¯æŒè€…/ç‚¹èµæ•°", "æ”¶è—æ•°",  # from middle (4)
            "é¡¹ç›®è¯¦æƒ…-å›¾ç‰‡æ•°é‡", "é¡¹ç›®è¯¦æƒ…-å›¾ç‰‡(åˆ—è¡¨å­—ç¬¦ä¸²)", "é¡¹ç›®è¯¦æƒ…-è§†é¢‘æ•°é‡", "é¡¹ç›®è¯¦æƒ…-è§†é¢‘(åˆ—è¡¨å­—ç¬¦ä¸²)"  # from left (4)
        ) # Total 34 columns

        # Write headers only once
        if start_row_idx == 0:
            for i, header_name in enumerate(col_headers):
                sheet.write(0, i, header_name)

        # Process each row of data
        for row_idx, row_data in enumerate(data_rows_list):
            if not row_data:
                continue

            # Calculate the actual Excel row number
            excel_row_num = start_row_idx + row_idx + 1  # +1 because headers are at row 0

            # Ensure row_data is a list and has enough elements
            if not isinstance(row_data, list):
                row_data = list(row_data) if hasattr(row_data, '__iter__') else [row_data]

            # Pad with empty strings if not enough columns
            padded_row_data = row_data + [""] * (len(col_headers) - len(row_data))

            # Write each cell in the row
            for col_idx, cell_value in enumerate(padded_row_data):
                cell_str = str(cell_value) if cell_value is not None else ""

                # Handle Excel cell character limit
                if len(cell_str) > 32767:
                    cell_str = cell_str[:32767] + "...TRUNCATED"

                try:
                    sheet.write(excel_row_num, col_idx, cell_str)
                except Exception as e:
                    logger.error(f"Error writing to Excel cell ({excel_row_num},{col_idx}): {e}. Value: {cell_str[:50]}")
                    try:
                        sheet.write(excel_row_num, col_idx, "ERROR_WRITING_CELL")
                    except:
                        pass  # Skip if even error message can't be written

        # Save the workbook
        try:
            excel_path = self.config.OUTPUT_DIR / self.config.EXCEL_FILENAME
            workbook.save(excel_path)
            logger.info(f"Data saved to {excel_path}")
        except Exception as e:
            logger.error(f"Error saving workbook: {e}")

    def save_data_to_multiple_formats(self, projects_data: List[List[Any]], suffix: str = "",
                                     stats: Optional[Dict[str, Any]] = None):
        """ä¿å­˜æ•°æ®åˆ°å¤šç§æ ¼å¼ï¼ˆå¢å¼ºç‰ˆï¼‰"""
        if not projects_data:
            logger.warning("æ²¡æœ‰æ•°æ®éœ€è¦ä¿å­˜")
            return {}

        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        base_filename = f"modian_projects_{timestamp}"
        if suffix:
            base_filename += f"_{suffix}"

        saved_files = {}

        # æ•°æ®è´¨é‡åˆ†æ
        quality_report = self._analyze_data_quality(projects_data)

        try:
            # 1. ä¿å­˜ä¸ºå¢å¼ºJSONæ ¼å¼ï¼ˆä¼˜åŒ–AIåˆ†æï¼‰
            json_file = self._save_enhanced_json(projects_data, base_filename, quality_report, stats)
            saved_files['json'] = json_file

            # 2. ä¿å­˜ä¸ºCSVæ ¼å¼
            csv_file = self._save_enhanced_csv(projects_data, base_filename)
            saved_files['csv'] = csv_file

            # 3. ä¿å­˜ä¸ºExcelæ ¼å¼ï¼ˆå…¼å®¹åŸæœ‰æ ¼å¼ï¼‰
            excel_file = self._save_enhanced_excel(projects_data, base_filename)
            saved_files['excel'] = excel_file

            # 4. ç”Ÿæˆæ•°æ®è´¨é‡æŠ¥å‘Š
            report_file = self._save_quality_report(quality_report, base_filename, stats)
            saved_files['quality_report'] = report_file

            logger.info(f"âœ… æ•°æ®å·²ä¿å­˜ä¸ºå¤šç§æ ¼å¼:")
            for format_type, file_path in saved_files.items():
                logger.info(f"  {format_type.upper()}: {file_path}")

            return saved_files

        except Exception as e:
            logger.error(f"ä¿å­˜å¤šæ ¼å¼æ–‡ä»¶å¤±è´¥: {e}")
            return saved_files

    def _analyze_data_quality(self, projects_data: List[List[Any]]) -> Dict[str, Any]:
        """åˆ†ææ•°æ®è´¨é‡"""
        if not projects_data:
            return {"total_projects": 0, "empty_fields": {}, "data_completeness": 0}

        # å®šä¹‰å­—æ®µåç§°ï¼ˆä¸Excelåˆ—å¯¹åº”ï¼‰
        field_names = [
            "åºå·", "é¡¹ç›®link", "é¡¹ç›®6ä½id", "é¡¹ç›®åç§°", "é¡¹ç›®å›¾",
            "å¼€å§‹æ—¶é—´", "ç»“æŸæ—¶é—´", "é¡¹ç›®ç»“æœ",
            "ç”¨æˆ·ä¸»é¡µ(é“¾æ¥)", "ç”¨æˆ·å¤´åƒ(å›¾ç‰‡é“¾æ¥)", "åˆ†ç±»", "ç”¨æˆ·å", "ç”¨æˆ·UID(data-username)",
            "å·²ç­¹é‡‘é¢", "ç™¾åˆ†æ¯”", "ç›®æ ‡é‡‘é¢", "æ”¯æŒè€…(æ•°é‡)",
            "çœŸå®ç”¨æˆ·ID(é“¾æ¥æå–)", "ä½œè€…é¡µ-ç²‰ä¸æ•°", "ä½œè€…é¡µ-å…³æ³¨æ•°", "ä½œè€…é¡µ-è·èµæ•°",
            "ä½œè€…é¡µ-è¯¦æƒ…", "ä½œè€…é¡µ-å…¶ä»–ä¿¡æ¯", "ä½œè€…é¡µ-ä¸»é¡µç¡®è®¤",
            "å›æŠ¥åˆ—è¡¨ä¿¡æ¯(å­—ç¬¦ä¸²)", "å›æŠ¥åˆ—è¡¨é¡¹ç›®æ•°",
            "é¡¹ç›®æ›´æ–°æ•°", "è¯„è®ºæ•°", "é¡¹ç›®æ”¯æŒè€…/ç‚¹èµæ•°", "æ”¶è—æ•°",
            "é¡¹ç›®è¯¦æƒ…-å›¾ç‰‡æ•°é‡", "é¡¹ç›®è¯¦æƒ…-å›¾ç‰‡(åˆ—è¡¨å­—ç¬¦ä¸²)",
            "é¡¹ç›®è¯¦æƒ…-è§†é¢‘æ•°é‡", "é¡¹ç›®è¯¦æƒ…-è§†é¢‘(åˆ—è¡¨å­—ç¬¦ä¸²)"
        ]

        total_projects = len(projects_data)
        empty_fields = {}
        error_fields = {}

        # åˆ†ææ¯ä¸ªå­—æ®µçš„æ•°æ®è´¨é‡
        for field_idx, field_name in enumerate(field_names):
            empty_count = 0
            error_count = 0

            for project in projects_data:
                if field_idx >= len(project):
                    empty_count += 1
                else:
                    value = project[field_idx]
                    if value is None or str(value).strip() in ["", "none", "0", "error_fetching_details", "no_link_for_details"]:
                        empty_count += 1
                    elif "error" in str(value).lower():
                        error_count += 1

            if empty_count > 0:
                empty_fields[field_name] = {
                    "empty_count": empty_count,
                    "empty_percentage": round((empty_count / total_projects) * 100, 2)
                }

            if error_count > 0:
                error_fields[field_name] = {
                    "error_count": error_count,
                    "error_percentage": round((error_count / total_projects) * 100, 2)
                }

        # è®¡ç®—æ•´ä½“æ•°æ®å®Œæ•´æ€§
        total_fields = len(field_names) * total_projects
        total_empty = sum(field["empty_count"] for field in empty_fields.values())
        data_completeness = round(((total_fields - total_empty) / total_fields) * 100, 2) if total_fields > 0 else 0

        return {
            "total_projects": total_projects,
            "total_fields": len(field_names),
            "empty_fields": empty_fields,
            "error_fields": error_fields,
            "data_completeness": data_completeness,
            "analysis_time": datetime.datetime.now().isoformat()
        }

    def _save_enhanced_json(self, projects_data: List[List[Any]], base_filename: str,
                           quality_report: Dict[str, Any], stats: Optional[Dict[str, Any]] = None) -> str:
        """ä¿å­˜å¢å¼ºçš„JSONæ ¼å¼ï¼ˆä¼˜åŒ–AIåˆ†æï¼‰"""
        json_file = self.config.OUTPUT_DIR / f"{base_filename}.json"

        # å­—æ®µåç§°æ˜ å°„
        field_names = [
            "åºå·", "é¡¹ç›®link", "é¡¹ç›®6ä½id", "é¡¹ç›®åç§°", "é¡¹ç›®å›¾",
            "å¼€å§‹æ—¶é—´", "ç»“æŸæ—¶é—´", "é¡¹ç›®ç»“æœ",
            "ç”¨æˆ·ä¸»é¡µ(é“¾æ¥)", "ç”¨æˆ·å¤´åƒ(å›¾ç‰‡é“¾æ¥)", "åˆ†ç±»", "ç”¨æˆ·å", "ç”¨æˆ·UID(data-username)",
            "å·²ç­¹é‡‘é¢", "ç™¾åˆ†æ¯”", "ç›®æ ‡é‡‘é¢", "æ”¯æŒè€…(æ•°é‡)",
            "çœŸå®ç”¨æˆ·ID(é“¾æ¥æå–)", "ä½œè€…é¡µ-ç²‰ä¸æ•°", "ä½œè€…é¡µ-å…³æ³¨æ•°", "ä½œè€…é¡µ-è·èµæ•°",
            "ä½œè€…é¡µ-è¯¦æƒ…", "ä½œè€…é¡µ-å…¶ä»–ä¿¡æ¯", "ä½œè€…é¡µ-ä¸»é¡µç¡®è®¤",
            "å›æŠ¥åˆ—è¡¨ä¿¡æ¯(å­—ç¬¦ä¸²)", "å›æŠ¥åˆ—è¡¨é¡¹ç›®æ•°",
            "é¡¹ç›®æ›´æ–°æ•°", "è¯„è®ºæ•°", "é¡¹ç›®æ”¯æŒè€…/ç‚¹èµæ•°", "æ”¶è—æ•°",
            "é¡¹ç›®è¯¦æƒ…-å›¾ç‰‡æ•°é‡", "é¡¹ç›®è¯¦æƒ…-å›¾ç‰‡(åˆ—è¡¨å­—ç¬¦ä¸²)",
            "é¡¹ç›®è¯¦æƒ…-è§†é¢‘æ•°é‡", "é¡¹ç›®è¯¦æƒ…-è§†é¢‘(åˆ—è¡¨å­—ç¬¦ä¸²)"
        ]

        # è½¬æ¢ä¸ºç»“æ„åŒ–JSON
        projects_json = []
        for project_data in projects_data:
            project_dict = {}

            # åŸºæœ¬ä¿¡æ¯
            project_dict["basic_info"] = {
                "id": project_data[0] if len(project_data) > 0 else "",
                "link": project_data[1] if len(project_data) > 1 else "",
                "project_id": project_data[2] if len(project_data) > 2 else "",
                "title": project_data[3] if len(project_data) > 3 else "",
                "image": project_data[4] if len(project_data) > 4 else ""
            }

            # æ—¶é—´ä¿¡æ¯
            project_dict["time_info"] = {
                "start_time": project_data[5] if len(project_data) > 5 else "",
                "end_time": project_data[6] if len(project_data) > 6 else "",
                "status": project_data[7] if len(project_data) > 7 else ""
            }

            # ä½œè€…ä¿¡æ¯
            project_dict["author_info"] = {
                "homepage": project_data[8] if len(project_data) > 8 else "",
                "avatar": project_data[9] if len(project_data) > 9 else "",
                "category": project_data[10] if len(project_data) > 10 else "",
                "name": project_data[11] if len(project_data) > 11 else "",
                "uid": project_data[12] if len(project_data) > 12 else "",
                "real_id": project_data[17] if len(project_data) > 17 else "",
                "fans_count": self._safe_int(project_data[18] if len(project_data) > 18 else 0),
                "following_count": self._safe_int(project_data[19] if len(project_data) > 19 else 0),
                "likes_count": self._safe_int(project_data[20] if len(project_data) > 20 else 0),
                "details": project_data[21] if len(project_data) > 21 else "",
                "other_info": project_data[22] if len(project_data) > 22 else "",
                "homepage_confirm": project_data[23] if len(project_data) > 23 else ""
            }

            # ä¼—ç­¹ä¿¡æ¯
            project_dict["funding_info"] = {
                "raised_amount": self._safe_float(project_data[13] if len(project_data) > 13 else 0),
                "completion_rate": self._safe_float(project_data[14] if len(project_data) > 14 else 0),
                "target_amount": self._safe_float(project_data[15] if len(project_data) > 15 else 0),
                "backer_count": self._safe_int(project_data[16] if len(project_data) > 16 else 0)
            }

            # å›æŠ¥ä¿¡æ¯
            project_dict["reward_info"] = {
                "rewards_list": project_data[24] if len(project_data) > 24 else "",
                "rewards_count": self._safe_int(project_data[25] if len(project_data) > 25 else 0)
            }

            # äº’åŠ¨ä¿¡æ¯
            project_dict["engagement_info"] = {
                "updates_count": self._safe_int(project_data[26] if len(project_data) > 26 else 0),
                "comments_count": self._safe_int(project_data[27] if len(project_data) > 27 else 0),
                "supporters_likes_count": self._safe_int(project_data[28] if len(project_data) > 28 else 0),
                "collections_count": self._safe_int(project_data[29] if len(project_data) > 29 else 0)
            }

            # å†…å®¹ä¿¡æ¯
            project_dict["content_info"] = {
                "images_count": self._safe_int(project_data[30] if len(project_data) > 30 else 0),
                "images_list": project_data[31] if len(project_data) > 31 else "",
                "videos_count": self._safe_int(project_data[32] if len(project_data) > 32 else 0),
                "videos_list": project_data[33] if len(project_data) > 33 else ""
            }

            projects_json.append(project_dict)

        # æ„å»ºå®Œæ•´çš„JSONç»“æ„
        json_data = {
            "metadata": {
                "export_time": datetime.datetime.now().isoformat(),
                "total_projects": len(projects_json),
                "format_version": "2.0",
                "data_source": "modian_crowdfunding",
                "fields_count": len(field_names),
                "quality_report": quality_report
            },
            "statistics": stats if stats else {},
            "projects": projects_json
        }

        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(json_data, f, ensure_ascii=False, indent=2)

        logger.info(f"ğŸ“„ å¢å¼ºJSONæ–‡ä»¶å·²ä¿å­˜: {json_file}")
        return str(json_file)

    def _safe_int(self, value) -> int:
        """å®‰å…¨è½¬æ¢ä¸ºæ•´æ•°"""
        try:
            if isinstance(value, str):
                # ç§»é™¤å¸¸è§çš„éæ•°å­—å­—ç¬¦
                cleaned = value.replace(',', '').replace('äººæ”¯æŒ', '').replace('äººè®¢é˜…', '').strip()
                if cleaned.isdigit():
                    return int(cleaned)
            elif isinstance(value, (int, float)):
                return int(value)
            return 0
        except (ValueError, TypeError):
            return 0

    def _safe_float(self, value) -> float:
        """å®‰å…¨è½¬æ¢ä¸ºæµ®ç‚¹æ•°"""
        try:
            if isinstance(value, str):
                # ç§»é™¤å¸¸è§çš„éæ•°å­—å­—ç¬¦
                cleaned = value.replace(',', '').replace('ï¿¥', '').replace('%', '').strip()
                if cleaned.replace('.', '').isdigit():
                    return float(cleaned)
            elif isinstance(value, (int, float)):
                return float(value)
            return 0.0
        except (ValueError, TypeError):
            return 0.0

    def _save_enhanced_csv(self, projects_data: List[List[Any]], base_filename: str) -> str:
        """ä¿å­˜å¢å¼ºçš„CSVæ ¼å¼"""
        csv_file = self.config.OUTPUT_DIR / f"{base_filename}.csv"

        # å­—æ®µåç§°
        field_names = [
            "åºå·", "é¡¹ç›®link", "é¡¹ç›®6ä½id", "é¡¹ç›®åç§°", "é¡¹ç›®å›¾",
            "å¼€å§‹æ—¶é—´", "ç»“æŸæ—¶é—´", "é¡¹ç›®ç»“æœ",
            "ç”¨æˆ·ä¸»é¡µ(é“¾æ¥)", "ç”¨æˆ·å¤´åƒ(å›¾ç‰‡é“¾æ¥)", "åˆ†ç±»", "ç”¨æˆ·å", "ç”¨æˆ·UID(data-username)",
            "å·²ç­¹é‡‘é¢", "ç™¾åˆ†æ¯”", "ç›®æ ‡é‡‘é¢", "æ”¯æŒè€…(æ•°é‡)",
            "çœŸå®ç”¨æˆ·ID(é“¾æ¥æå–)", "ä½œè€…é¡µ-ç²‰ä¸æ•°", "ä½œè€…é¡µ-å…³æ³¨æ•°", "ä½œè€…é¡µ-è·èµæ•°",
            "ä½œè€…é¡µ-è¯¦æƒ…", "ä½œè€…é¡µ-å…¶ä»–ä¿¡æ¯", "ä½œè€…é¡µ-ä¸»é¡µç¡®è®¤",
            "å›æŠ¥åˆ—è¡¨ä¿¡æ¯(å­—ç¬¦ä¸²)", "å›æŠ¥åˆ—è¡¨é¡¹ç›®æ•°",
            "é¡¹ç›®æ›´æ–°æ•°", "è¯„è®ºæ•°", "é¡¹ç›®æ”¯æŒè€…/ç‚¹èµæ•°", "æ”¶è—æ•°",
            "é¡¹ç›®è¯¦æƒ…-å›¾ç‰‡æ•°é‡", "é¡¹ç›®è¯¦æƒ…-å›¾ç‰‡(åˆ—è¡¨å­—ç¬¦ä¸²)",
            "é¡¹ç›®è¯¦æƒ…-è§†é¢‘æ•°é‡", "é¡¹ç›®è¯¦æƒ…-è§†é¢‘(åˆ—è¡¨å­—ç¬¦ä¸²)"
        ]

        try:
            with open(csv_file, 'w', newline='', encoding='utf-8-sig') as f:
                writer = csv.writer(f)

                # å†™å…¥è¡¨å¤´
                writer.writerow(field_names)

                # å†™å…¥æ•°æ®
                for project_data in projects_data:
                    # ç¡®ä¿æ•°æ®é•¿åº¦åŒ¹é…è¡¨å¤´ï¼Œå¡«å……ç©ºå€¼
                    padded_data = list(project_data) + [""] * (len(field_names) - len(project_data))

                    # æ¸…ç†æ•°æ®
                    cleaned_row = []
                    for i, cell in enumerate(padded_data[:len(field_names)]):
                        cell_str = str(cell) if cell is not None else ""
                        # æ¸…ç†CSVä¸­çš„ç‰¹æ®Šå­—ç¬¦
                        cell_str = cell_str.replace('\n', ' ').replace('\r', ' ').replace('\t', ' ')
                        cleaned_row.append(cell_str)

                    writer.writerow(cleaned_row)

            logger.info(f"ğŸ“Š å¢å¼ºCSVæ–‡ä»¶å·²ä¿å­˜: {csv_file}")
            return str(csv_file)

        except Exception as e:
            logger.error(f"ä¿å­˜CSVæ–‡ä»¶å¤±è´¥: {e}")
            raise

    def _save_enhanced_excel(self, projects_data: List[List[Any]], base_filename: str) -> str:
        """ä¿å­˜å¢å¼ºçš„Excelæ ¼å¼ï¼ˆå…¼å®¹åŸæœ‰æ ¼å¼ï¼‰"""
        excel_file = self.config.OUTPUT_DIR / f"{base_filename}.xlsx"

        try:
            # ä½¿ç”¨openpyxlåˆ›å»ºå·¥ä½œç°¿
            from openpyxl import Workbook
            wb = Workbook()
            ws = wb.active
            ws.title = "æ‘©ç‚¹ä¼—ç­¹é¡¹ç›®æ•°æ®"

            # å­—æ®µåç§°
            field_names = [
                "åºå·", "é¡¹ç›®link", "é¡¹ç›®6ä½id", "é¡¹ç›®åç§°", "é¡¹ç›®å›¾",
                "å¼€å§‹æ—¶é—´", "ç»“æŸæ—¶é—´", "é¡¹ç›®ç»“æœ",
                "ç”¨æˆ·ä¸»é¡µ(é“¾æ¥)", "ç”¨æˆ·å¤´åƒ(å›¾ç‰‡é“¾æ¥)", "åˆ†ç±»", "ç”¨æˆ·å", "ç”¨æˆ·UID(data-username)",
                "å·²ç­¹é‡‘é¢", "ç™¾åˆ†æ¯”", "ç›®æ ‡é‡‘é¢", "æ”¯æŒè€…(æ•°é‡)",
                "çœŸå®ç”¨æˆ·ID(é“¾æ¥æå–)", "ä½œè€…é¡µ-ç²‰ä¸æ•°", "ä½œè€…é¡µ-å…³æ³¨æ•°", "ä½œè€…é¡µ-è·èµæ•°",
                "ä½œè€…é¡µ-è¯¦æƒ…", "ä½œè€…é¡µ-å…¶ä»–ä¿¡æ¯", "ä½œè€…é¡µ-ä¸»é¡µç¡®è®¤",
                "å›æŠ¥åˆ—è¡¨ä¿¡æ¯(å­—ç¬¦ä¸²)", "å›æŠ¥åˆ—è¡¨é¡¹ç›®æ•°",
                "é¡¹ç›®æ›´æ–°æ•°", "è¯„è®ºæ•°", "é¡¹ç›®æ”¯æŒè€…/ç‚¹èµæ•°", "æ”¶è—æ•°",
                "é¡¹ç›®è¯¦æƒ…-å›¾ç‰‡æ•°é‡", "é¡¹ç›®è¯¦æƒ…-å›¾ç‰‡(åˆ—è¡¨å­—ç¬¦ä¸²)",
                "é¡¹ç›®è¯¦æƒ…-è§†é¢‘æ•°é‡", "é¡¹ç›®è¯¦æƒ…-è§†é¢‘(åˆ—è¡¨å­—ç¬¦ä¸²)"
            ]

            # å†™å…¥è¡¨å¤´
            for col_idx, header in enumerate(field_names, 1):
                ws.cell(row=1, column=col_idx, value=header)

            # å†™å…¥æ•°æ®
            for row_idx, project_data in enumerate(projects_data, 2):
                for col_idx, cell_value in enumerate(project_data, 1):
                    if col_idx > len(field_names):
                        break

                    # å¤„ç†å•å…ƒæ ¼å€¼
                    if cell_value is None:
                        cell_value = ""
                    else:
                        cell_str = str(cell_value)
                        # Excelå•å…ƒæ ¼å­—ç¬¦é™åˆ¶
                        if len(cell_str) > 32767:
                            cell_str = cell_str[:32764] + "..."
                        cell_value = cell_str

                    ws.cell(row=row_idx, column=col_idx, value=cell_value)

            # ä¿å­˜æ–‡ä»¶
            wb.save(excel_file)
            logger.info(f"ğŸ“ˆ å¢å¼ºExcelæ–‡ä»¶å·²ä¿å­˜: {excel_file}")
            return str(excel_file)

        except Exception as e:
            logger.error(f"ä¿å­˜Excelæ–‡ä»¶å¤±è´¥: {e}")
            # å›é€€åˆ°xlwtæ ¼å¼
            try:
                excel_file_xls = self.config.OUTPUT_DIR / f"{base_filename}.xls"
                workbook = xlwt.Workbook(encoding="utf-8", style_compression=0)
                sheet = workbook.add_sheet('projects', cell_overwrite_ok=True)

                # ä½¿ç”¨åŸæœ‰çš„ä¿å­˜æ–¹æ³•
                self.save_data_to_excel(workbook, sheet, projects_data, 0)
                logger.info(f"ğŸ“ˆ Excelæ–‡ä»¶å·²ä¿å­˜(XLSæ ¼å¼): {excel_file_xls}")
                return str(excel_file_xls)
            except Exception as e2:
                logger.error(f"ä¿å­˜XLSæ–‡ä»¶ä¹Ÿå¤±è´¥: {e2}")
                raise

    def _save_quality_report(self, quality_report: Dict[str, Any], base_filename: str,
                           stats: Optional[Dict[str, Any]] = None) -> str:
        """ä¿å­˜æ•°æ®è´¨é‡æŠ¥å‘Š"""
        report_file = self.config.OUTPUT_DIR / f"{base_filename}_quality_report.txt"

        try:
            with open(report_file, 'w', encoding='utf-8') as f:
                f.write("=" * 80 + "\n")
                f.write("æ‘©ç‚¹ä¼—ç­¹çˆ¬è™« - æ•°æ®è´¨é‡åˆ†ææŠ¥å‘Š\n")
                f.write("=" * 80 + "\n\n")

                # åŸºæœ¬ä¿¡æ¯
                f.write(f"ğŸ“Š æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"ğŸ“ˆ æ•°æ®é¡¹ç›®æ€»æ•°: {quality_report.get('total_projects', 0)}\n")
                f.write(f"ğŸ“‹ æ•°æ®å­—æ®µæ€»æ•°: {quality_report.get('total_fields', 0)}\n")
                f.write(f"âœ… æ•°æ®å®Œæ•´æ€§: {quality_report.get('data_completeness', 0)}%\n\n")

                # çˆ¬è™«ç»Ÿè®¡ä¿¡æ¯
                if stats:
                    f.write("ğŸš€ çˆ¬è™«è¿è¡Œç»Ÿè®¡:\n")
                    f.write("-" * 40 + "\n")
                    f.write(f"  è¿è¡Œæ—¶é—´: {stats.get('elapsed_time', 0):.2f} ç§’\n")
                    f.write(f"  å¤„ç†é¡µé¢: {stats.get('pages_processed', 0)}\n")
                    f.write(f"  æ€»è¯·æ±‚æ•°: {stats.get('total_requests', 0)}\n")
                    f.write(f"  æˆåŠŸè¯·æ±‚: {stats.get('successful_requests', 0)}\n")
                    f.write(f"  å¤±è´¥è¯·æ±‚: {stats.get('failed_requests', 0)}\n")
                    f.write(f"  æˆåŠŸç‡: {stats.get('success_rate', 0):.1f}%\n")
                    f.write(f"  å‘ç°é¡¹ç›®: {stats.get('projects_found', 0)}\n")
                    f.write(f"  å¤„ç†é¡¹ç›®: {stats.get('projects_processed', 0)}\n\n")

                # æ•°æ®ç¼ºå¤±åˆ†æ
                empty_fields = quality_report.get('empty_fields', {})
                if empty_fields:
                    f.write("âš ï¸  æ•°æ®ç¼ºå¤±å­—æ®µåˆ†æ:\n")
                    f.write("-" * 40 + "\n")

                    # æŒ‰ç¼ºå¤±ç‡æ’åº
                    sorted_empty = sorted(empty_fields.items(),
                                        key=lambda x: x[1]['empty_percentage'], reverse=True)

                    for field_name, field_info in sorted_empty:
                        f.write(f"  {field_name}:\n")
                        f.write(f"    ç¼ºå¤±æ•°é‡: {field_info['empty_count']}\n")
                        f.write(f"    ç¼ºå¤±ç‡: {field_info['empty_percentage']}%\n\n")
                else:
                    f.write("âœ… æ‰€æœ‰å­—æ®µæ•°æ®å®Œæ•´ï¼Œæ— ç¼ºå¤±æ•°æ®\n\n")

                # é”™è¯¯å­—æ®µåˆ†æ
                error_fields = quality_report.get('error_fields', {})
                if error_fields:
                    f.write("âŒ æ•°æ®é”™è¯¯å­—æ®µåˆ†æ:\n")
                    f.write("-" * 40 + "\n")

                    for field_name, field_info in error_fields.items():
                        f.write(f"  {field_name}:\n")
                        f.write(f"    é”™è¯¯æ•°é‡: {field_info['error_count']}\n")
                        f.write(f"    é”™è¯¯ç‡: {field_info['error_percentage']}%\n\n")
                else:
                    f.write("âœ… æ— æ•°æ®é”™è¯¯å­—æ®µ\n\n")

                # æ•°æ®è´¨é‡å»ºè®®
                f.write("ğŸ’¡ æ•°æ®è´¨é‡æ”¹è¿›å»ºè®®:\n")
                f.write("-" * 40 + "\n")

                completeness = quality_report.get('data_completeness', 0)
                if completeness >= 90:
                    f.write("  âœ… æ•°æ®è´¨é‡ä¼˜ç§€ï¼Œå®Œæ•´æ€§è¶…è¿‡90%\n")
                elif completeness >= 80:
                    f.write("  âš ï¸  æ•°æ®è´¨é‡è‰¯å¥½ï¼Œå»ºè®®ä¼˜åŒ–ç¼ºå¤±ç‡è¾ƒé«˜çš„å­—æ®µ\n")
                elif completeness >= 70:
                    f.write("  âš ï¸  æ•°æ®è´¨é‡ä¸€èˆ¬ï¼Œéœ€è¦é‡ç‚¹ä¼˜åŒ–æ•°æ®é‡‡é›†é€»è¾‘\n")
                else:
                    f.write("  âŒ æ•°æ®è´¨é‡è¾ƒå·®ï¼Œå»ºè®®å…¨é¢æ£€æŸ¥çˆ¬è™«é€»è¾‘\n")

                # é’ˆå¯¹é«˜ç¼ºå¤±ç‡å­—æ®µçš„å»ºè®®
                high_missing_fields = [name for name, info in empty_fields.items()
                                     if info['empty_percentage'] > 50]
                if high_missing_fields:
                    f.write(f"\n  ğŸ”§ é«˜ç¼ºå¤±ç‡å­—æ®µ({len(high_missing_fields)}ä¸ª)éœ€è¦ä¼˜åŒ–:\n")
                    for field in high_missing_fields[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                        f.write(f"    - {field}\n")

                f.write("\n" + "=" * 80 + "\n")
                f.write("æŠ¥å‘Šç»“æŸ\n")

            logger.info(f"ğŸ“‹ æ•°æ®è´¨é‡æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
            return str(report_file)

        except Exception as e:
            logger.error(f"ä¿å­˜è´¨é‡æŠ¥å‘Šå¤±è´¥: {e}")
            raise

    def print_stats(self):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        stats = self.stats.get_summary()

        print("\n" + "="*60)
        print("ğŸ“Š çˆ¬è™«è¿è¡Œç»Ÿè®¡")
        print("="*60)
        print(f"è¿è¡Œæ—¶é—´: {stats['elapsed_time']:.2f} ç§’")
        print(f"å¤„ç†é¡µé¢: {stats['pages_processed']}")
        print(f"æ€»è¯·æ±‚æ•°: {stats['total_requests']}")
        print(f"æˆåŠŸè¯·æ±‚: {stats['successful_requests']}")
        print(f"å¤±è´¥è¯·æ±‚: {stats['failed_requests']}")
        print(f"æˆåŠŸç‡: {stats['success_rate']:.1f}%")
        print(f"å‘ç°é¡¹ç›®: {stats['projects_found']}")
        print(f"å¤„ç†é¡¹ç›®: {stats['projects_processed']}")
        print(f"å¹³å‡æ¯é¡µè€—æ—¶: {stats['avg_time_per_page']:.2f} ç§’")

        if stats['errors']:
            print("\né”™è¯¯ç»Ÿè®¡:")
            for error_type, messages in stats['errors'].items():
                print(f"  {error_type}: {len(messages)} æ¬¡")

        print("="*60)

    def run_scraper(self):
        """è¿è¡Œå®Œæ•´çš„çˆ¬è™«æµç¨‹"""
        logger.info("å¼€å§‹è¿è¡Œæ‘©ç‚¹ä¼—ç­¹çˆ¬è™«")

        # åˆ›å»ºExcelå·¥ä½œç°¿
        main_workbook = xlwt.Workbook(encoding="utf-8", style_compression=0)
        main_sheet = main_workbook.add_sheet('all_projects', cell_overwrite_ok=True)

        # å…¨å±€Excelè¡Œç´¢å¼•è®¡æ•°å™¨
        excel_index_counter = 0 # Start at 0 because headers are row 0, data starts at row 1

        # å†™å…¥è¡¨å¤´
        self.save_data_to_excel(main_workbook, main_sheet, [], 0) # Pass empty data list to just write headers

        # çˆ¬å–æ•°æ®
        total_projects_processed = 0

        try:
            for page_num in range(1, self.config.MAX_PAGES + 1):
                logger.info(f"\n--- æ­£åœ¨å¤„ç†ç¬¬ {page_num} é¡µ ---")
                current_page_url = self.config.get_page_url(page_num)
                page_html = self.askURL(current_page_url)

                if not page_html:
                    logger.warning(f"Failed to get HTML for page {page_num}. Skipping.")
                    continue

                # è§£æé¡µé¢é¡¹ç›®
                projects_on_page, excel_index_counter = self.parse_main_listing_page(page_html, excel_index_counter)

                if projects_on_page:
                    # ä¿å­˜åˆ°Excel
                    self.save_data_to_excel(main_workbook, main_sheet, projects_on_page, 0)
                    total_projects_processed += len(projects_on_page)
                    logger.info(f"Processed {len(projects_on_page)} projects from page {page_num}. Total processed: {total_projects_processed}")

                    # æ›´æ–°ç»Ÿè®¡
                    self.stats.projects_found += len(projects_on_page)
                else:
                    logger.warning(f"No projects found or processed on page {page_num}.")

                self.stats.pages_processed += 1

                # å®šæœŸä¿å­˜
                if page_num % self.config.SAVE_INTERVAL == 0:
                    logger.info(f"Intermediate save at page {page_num}...")
                    excel_path = self.config.OUTPUT_DIR / self.config.EXCEL_FILENAME
                    main_workbook.save(excel_path)

                # æ·»åŠ å»¶è¿Ÿé¿å…è¿‡äºé¢‘ç¹çš„è¯·æ±‚
                time.sleep(random.uniform(1, 3))

            # æœ€ç»ˆä¿å­˜ - ä¼ ç»ŸExcelæ ¼å¼
            excel_path = self.config.OUTPUT_DIR / self.config.EXCEL_FILENAME
            main_workbook.save(excel_path)
            logger.info(f"ä¼ ç»ŸExcelæ–‡ä»¶å·²ä¿å­˜: {excel_path}")

            # æ”¶é›†æ‰€æœ‰é¡¹ç›®æ•°æ®ç”¨äºå¤šæ ¼å¼è¾“å‡º
            all_projects_data = []
            for page_num in range(1, min(self.config.MAX_PAGES + 1, self.stats.pages_processed + 1)):
                current_page_url = self.config.get_page_url(page_num)
                page_html = self.askURL(current_page_url)
                if page_html:
                    projects_on_page, _ = self.parse_main_listing_page(page_html, 0)
                    all_projects_data.extend(projects_on_page)

            # ç”Ÿæˆå¤šæ ¼å¼è¾“å‡º
            if all_projects_data:
                stats_summary = self.stats.get_summary()
                saved_files = self.save_data_to_multiple_formats(
                    all_projects_data,
                    suffix="final",
                    stats=stats_summary
                )

                logger.info(f"\nğŸ‰ å¤šæ ¼å¼æ–‡ä»¶ç”Ÿæˆå®Œæˆ:")
                for format_type, file_path in saved_files.items():
                    logger.info(f"  {format_type.upper()}: {file_path}")

            # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
            self.print_stats()

            logger.info(f"\n--- çˆ¬å–å®Œæˆ ---")
            logger.info(f"Total projects processed and attempted to save: {total_projects_processed}")
            logger.info(f"Final global excel row index reached: {excel_index_counter}")

            return True

        except Exception as e:
            logger.error(f"çˆ¬è™«è¿è¡Œå¤±è´¥: {e}")
            self.stats.record_error("runtime_error", str(e))
            return False

    def run(self) -> bool:
        """è¿è¡Œçˆ¬è™«ï¼ˆå¢å¼ºç‰ˆæ¥å£ï¼‰"""
        logger.info("å¼€å§‹è¿è¡Œæ‘©ç‚¹ä¼—ç­¹çˆ¬è™« - å¢å¼ºç‰ˆæ¥å£")

        all_projects = []

        try:
            for page_num in range(1, self.config.MAX_PAGES + 1):
                logger.info(f"æ­£åœ¨å¤„ç†ç¬¬ {page_num} é¡µ...")

                page_url = self.config.get_page_url(page_num)
                html = self.make_request(page_url)

                if not html:
                    logger.warning(f"ç¬¬ {page_num} é¡µè·å–å¤±è´¥ï¼Œè·³è¿‡")
                    continue

                # ç®€åŒ–ç‰ˆè§£æï¼ˆç”¨äºå¢å¼ºç‰ˆæ¥å£ï¼‰
                projects = self.parse_listing_page_simple(html)
                all_projects.extend(projects)

                self.stats.pages_processed += 1

                logger.info(f"ç¬¬ {page_num} é¡µå¤„ç†å®Œæˆï¼Œæ‰¾åˆ° {len(projects)} ä¸ªé¡¹ç›®")

                # å®šæœŸä¿å­˜
                if page_num % self.config.SAVE_INTERVAL == 0:
                    self.save_data_to_multiple_formats(all_projects, f"intermediate_page_{page_num}")

                # æ·»åŠ å»¶è¿Ÿé¿å…è¿‡äºé¢‘ç¹çš„è¯·æ±‚
                time.sleep(random.uniform(1, 3))

            # æœ€ç»ˆä¿å­˜ - ä½¿ç”¨å¢å¼ºçš„å¤šæ ¼å¼è¾“å‡º
            if all_projects:
                # è½¬æ¢ç®€åŒ–æ ¼å¼ä¸ºå®Œæ•´æ ¼å¼ï¼ˆç”¨äºå…¼å®¹ï¼‰
                projects_data = []
                for i, project in enumerate(all_projects, 1):
                    project_row = [
                        i,  # åºå·
                        project.get("link", ""),  # é¡¹ç›®link
                        project.get("id", ""),    # é¡¹ç›®6ä½id
                        project.get("title", ""), # é¡¹ç›®åç§°
                        project.get("image", "")  # é¡¹ç›®å›¾
                    ]
                    # å¡«å……å…¶ä»–å­—æ®µä¸ºç©ºå€¼ï¼ˆç®€åŒ–ç‰ˆæ²¡æœ‰è¯¦ç»†ä¿¡æ¯ï¼‰
                    project_row.extend([""] * 29)  # è¡¥å……åˆ°34ä¸ªå­—æ®µ
                    projects_data.append(project_row)

                stats_summary = self.stats.get_summary()
                saved_files = self.save_data_to_multiple_formats(
                    projects_data,
                    suffix="enhanced_final",
                    stats=stats_summary
                )

                logger.info(f"ğŸ‰ çˆ¬å–å®Œæˆï¼å…±å¤„ç† {len(all_projects)} ä¸ªé¡¹ç›®")
                logger.info(f"ğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
                for format_type, file_path in saved_files.items():
                    logger.info(f"  {format_type.upper()}: {file_path}")
            else:
                logger.warning("æœªè·å–åˆ°ä»»ä½•é¡¹ç›®æ•°æ®")

            # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
            self.print_stats()

            return True

        except Exception as e:
            logger.error(f"çˆ¬è™«è¿è¡Œå¤±è´¥: {e}")
            self.stats.record_error("runtime_error", str(e))
            return False

    def parse_listing_page_simple(self, html: str) -> List[Dict[str, Any]]:
        """ç®€åŒ–ç‰ˆåˆ—è¡¨é¡µé¢è§£æï¼ˆç”¨äºå¢å¼ºç‰ˆæ¥å£ï¼‰"""
        projects = []

        try:
            soup = BeautifulSoup(html, "html.parser")
            pro_field_div = soup.find('div', {'class': 'pro_field'})

            if not pro_field_div:
                logger.warning("æœªæ‰¾åˆ°é¡¹ç›®åˆ—è¡¨å®¹å™¨")
                return projects

            project_items = pro_field_div.find_all('li')
            logger.info(f"æ‰¾åˆ° {len(project_items)} ä¸ªé¡¹ç›®")

            for item_li in project_items:
                project = self.parse_project_item_simple(item_li)
                if project:
                    projects.append(project)
                    self.stats.projects_found += 1

        except Exception as e:
            logger.error(f"è§£æåˆ—è¡¨é¡µé¢å¤±è´¥: {e}")
            self.stats.record_error("parse_error", str(e))

        return projects

    def parse_project_item_simple(self, item_li) -> Optional[Dict[str, Any]]:
        """ç®€åŒ–ç‰ˆé¡¹ç›®é¡¹è§£æ"""
        try:
            # æŸ¥æ‰¾é¡¹ç›®é“¾æ¥
            item_link = ""

            for a_tag in item_li.find_all('a'):
                href = a_tag.get('href', '')
                if '/item/' in href:
                    item_link = href
                    break

            if not item_link:
                return None

            if not item_link.startswith("http"):
                item_link = "https://zhongchou.modian.com" + item_link

            # æå–é¡¹ç›®ä¿¡æ¯
            project_id = ""
            id_match = self.config.LINK_ID_PATTERN.search(item_link)
            if id_match:
                project_id = id_match.group(1)

            # é¡¹ç›®æ ‡é¢˜
            title = ""
            title_h3 = item_li.find('h3', class_='pro_title')
            if title_h3:
                title = title_h3.text.strip()

            # é¡¹ç›®å›¾ç‰‡
            img_src = ""
            img_tag = item_li.find('img')
            if img_tag and img_tag.get('src'):
                img_src = img_tag.get('src')

            # è·³è¿‡ç‰¹å®šé¡¹ç›®
            if "å¯æ±—æ¸¸æˆå¤§ä¼š" in title:
                return None

            return {
                "link": item_link,
                "id": project_id,
                "title": title,
                "image": img_src
            }

        except Exception as e:
            logger.error(f"è§£æé¡¹ç›®é¡¹å¤±è´¥: {e}")
            return None


def main(category: str = "all"):
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¯åŠ¨æ‘©ç‚¹çˆ¬è™«ç®¡ç†ç³»ç»Ÿ - èåˆç‰ˆ")
    print(f"ğŸ“‚ çˆ¬å–åˆ†ç±»: {category}")

    # åˆ›å»ºé…ç½®
    config = ModianSpiderConfig(category)

    # åˆ›å»ºçˆ¬è™«å®ä¾‹
    spider = ModianSpider(config)

    # è¿è¡Œçˆ¬è™«ï¼ˆä½¿ç”¨å®Œæ•´ç‰ˆæ¥å£ï¼‰
    success = spider.run_scraper()

    if success:
        print("âœ… çˆ¬è™«è¿è¡Œå®Œæˆ")
    else:
        print("âŒ çˆ¬è™«è¿è¡Œå¤±è´¥")

    return success


if __name__ == "__main__":
    main()
    print("çˆ¬å–å®Œæ¯•ï¼")
