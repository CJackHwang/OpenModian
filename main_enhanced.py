#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ¡Œæ¸¸å¸‚åœºè°ƒç ”å·¥å…· - å¢å¼ºç‰ˆçˆ¬è™«
ä¸»è¦æ”¹è¿›ï¼š
1. æ›´å¥½çš„é”™è¯¯å¤„ç†å’Œæ—¥å¿—è®°å½•
2. æ•°æ®éªŒè¯å’Œæ¸…ç†
3. è¿›åº¦æ˜¾ç¤ºå’Œç»Ÿè®¡
4. é…ç½®æ–‡ä»¶æ”¯æŒ
5. å¤šæ ¼å¼è¾“å‡ºï¼ˆExcel + JSON + CSVï¼‰
6. æ€§èƒ½ä¼˜åŒ–
"""

import random
import re
import socket
import time
import ssl
import json
import csv
import logging
import datetime
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple

import requests
import xlwt
import pandas as pd
from bs4 import BeautifulSoup
from openpyxl import Workbook
from openpyxl.utils.exceptions import IllegalCharacterError

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('spider.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class ModianSpiderConfig:
    """çˆ¬è™«é…ç½®ç±»"""
    
    def __init__(self):
        self.BASE_URL = "https://zhongchou.modian.com/all/top_time/all/"
        self.OUTPUT_DIR = Path("output")
        self.CACHE_DIR = Path("cache")
        self.MAX_RETRIES = 5
        self.RETRY_DELAY = 2
        self.REQUEST_TIMEOUT = (10, 20)
        self.MAX_PAGES = 3  # é»˜è®¤æµ‹è¯•èŒƒå›´ï¼Œå¯ä»¥ä¿®æ”¹ä¸ºæ›´å¤§å€¼
        self.SAVE_INTERVAL = 5  # æ¯5é¡µä¿å­˜ä¸€æ¬¡
        
        # è¯·æ±‚å¤´é…ç½®
        self.HEADERS = {
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
            "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
            "Accept-Encoding": "gzip, deflate, br",
            "Connection": "keep-alive",
            "Upgrade-Insecure-Requests": "1"
        }
        
        # æ­£åˆ™è¡¨è¾¾å¼
        self.LINK_ID_PATTERN = re.compile(r'https://zhongchou.modian.com/item/(\d+).html')
        self.USER_ID_PATTERN = re.compile(r'https://me.modian.com/u/detail\?uid=(\d+)')
        
        # åˆ›å»ºå¿…è¦ç›®å½•
        self.OUTPUT_DIR.mkdir(exist_ok=True)
        self.CACHE_DIR.mkdir(exist_ok=True)

class ModianSpiderStats:
    """çˆ¬è™«ç»Ÿè®¡ç±»"""
    
    def __init__(self):
        self.start_time = time.time()
        self.total_requests = 0
        self.successful_requests = 0
        self.failed_requests = 0
        self.projects_found = 0
        self.projects_processed = 0
        self.pages_processed = 0
        self.errors = {}
    
    def record_request(self, success: bool):
        """è®°å½•è¯·æ±‚ç»“æœ"""
        self.total_requests += 1
        if success:
            self.successful_requests += 1
        else:
            self.failed_requests += 1
    
    def record_error(self, error_type: str, message: str):
        """è®°å½•é”™è¯¯"""
        if error_type not in self.errors:
            self.errors[error_type] = []
        self.errors[error_type].append(message)
    
    def get_summary(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡æ‘˜è¦"""
        elapsed_time = time.time() - self.start_time
        return {
            "elapsed_time": elapsed_time,
            "total_requests": self.total_requests,
            "successful_requests": self.successful_requests,
            "failed_requests": self.failed_requests,
            "success_rate": (self.successful_requests / max(self.total_requests, 1)) * 100,
            "projects_found": self.projects_found,
            "projects_processed": self.projects_processed,
            "pages_processed": self.pages_processed,
            "avg_time_per_page": elapsed_time / max(self.pages_processed, 1),
            "errors": self.errors
        }

class ModianSpider:
    """æ‘©ç‚¹ä¼—ç­¹çˆ¬è™«ä¸»ç±»"""
    
    def __init__(self, config: Optional[ModianSpiderConfig] = None):
        self.config = config or ModianSpiderConfig()
        self.stats = ModianSpiderStats()
        self.session = requests.Session()
        self.session.headers.update(self.config.HEADERS)
        
        # SSLé…ç½®
        self.ssl_context = ssl.create_default_context()
        self.ssl_context.check_hostname = False
        self.ssl_context.verify_mode = ssl.CERT_NONE
        
        logger.info("æ‘©ç‚¹ä¼—ç­¹çˆ¬è™«åˆå§‹åŒ–å®Œæˆ")
    
    def make_request(self, url: str) -> Optional[str]:
        """å‘èµ·ç½‘ç»œè¯·æ±‚"""
        for attempt in range(self.config.MAX_RETRIES):
            try:
                timeout = random.randint(*self.config.REQUEST_TIMEOUT)

                # ä½¿ç”¨requestsè¿›è¡Œè¯·æ±‚ï¼Œè‡ªåŠ¨å¤„ç†gzipç­‰ç¼–ç 
                response = self.session.get(url, timeout=timeout, verify=False)
                response.raise_for_status()
                response.encoding = 'utf-8'

                self.stats.record_request(True)
                return response.text

            except Exception as e:
                logger.warning(f"ç¬¬{attempt + 1}æ¬¡è¯·æ±‚å¤±è´¥ {url}: {e}")
                self.stats.record_request(False)
                self.stats.record_error("network_error", str(e))

                if attempt == self.config.MAX_RETRIES - 1:
                    logger.error(f"è¯·æ±‚æœ€ç»ˆå¤±è´¥ {url}")
                    return None

                wait_time = self.config.RETRY_DELAY * (attempt + 1)
                logger.info(f"ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                time.sleep(wait_time)

        return None
    
    def extract_project_id(self, url: str) -> str:
        """æå–é¡¹ç›®ID"""
        match = self.config.LINK_ID_PATTERN.search(url)
        return match.group(1) if match else ""
    
    def extract_user_id(self, url: str) -> str:
        """æå–ç”¨æˆ·ID"""
        match = self.config.USER_ID_PATTERN.search(url)
        return match.group(1) if match else ""
    
    def clean_text(self, text: str, max_length: int = 1000) -> str:
        """æ¸…ç†æ–‡æœ¬"""
        if not text:
            return ""
        
        # ç§»é™¤å¤šä½™ç©ºç™½å­—ç¬¦
        cleaned = re.sub(r'\s+', ' ', text.strip())
        
        # é™åˆ¶é•¿åº¦
        if len(cleaned) > max_length:
            cleaned = cleaned[:max_length] + "..."
        
        return cleaned
    
    def parse_listing_page(self, html: str) -> List[Dict[str, Any]]:
        """è§£æåˆ—è¡¨é¡µé¢"""
        projects = []
        
        try:
            soup = BeautifulSoup(html, "html.parser")
            pro_field_div = soup.find('div', {'class': 'pro_field'})
            
            if not pro_field_div:
                logger.warning("æœªæ‰¾åˆ°é¡¹ç›®åˆ—è¡¨å®¹å™¨")
                return projects
            
            project_items = pro_field_div.find_all('li')
            logger.info(f"æ‰¾åˆ° {len(project_items)} ä¸ªé¡¹ç›®")
            
            for item_li in project_items:
                project = self.parse_project_item(item_li)
                if project:
                    projects.append(project)
                    self.stats.projects_found += 1
            
        except Exception as e:
            logger.error(f"è§£æåˆ—è¡¨é¡µé¢å¤±è´¥: {e}")
            self.stats.record_error("parse_error", str(e))
        
        return projects
    
    def parse_project_item(self, item_li) -> Optional[Dict[str, Any]]:
        """è§£æå•ä¸ªé¡¹ç›®é¡¹"""
        try:
            # æŸ¥æ‰¾é¡¹ç›®é“¾æ¥
            link_tag = None
            item_link = ""
            
            for a_tag in item_li.find_all('a'):
                href = a_tag.get('href', '')
                if '/item/' in href:
                    link_tag = a_tag
                    item_link = href
                    break
            
            if not item_link:
                return None
            
            if not item_link.startswith("http"):
                item_link = "https://zhongchou.modian.com" + item_link
            
            # æå–é¡¹ç›®ä¿¡æ¯
            project_id = self.extract_project_id(item_link)
            
            # é¡¹ç›®æ ‡é¢˜
            title = ""
            title_h3 = item_li.find('h3', class_='pro_title')
            if title_h3:
                title = self.clean_text(title_h3.text)
            
            # é¡¹ç›®å›¾ç‰‡
            img_src = ""
            img_tag = item_li.find('img')
            if img_tag and img_tag.get('src'):
                img_src = img_tag.get('src')
            
            # è·³è¿‡ç‰¹å®šé¡¹ç›®
            if "å¯æ±—æ¸¸æˆå¤§ä¼š" in title:
                return None
            
            return {
                "link": item_link,
                "id": project_id,
                "title": title,
                "image": img_src
            }
            
        except Exception as e:
            logger.error(f"è§£æé¡¹ç›®é¡¹å¤±è´¥: {e}")
            return None
    
    def run(self) -> bool:
        """è¿è¡Œçˆ¬è™«"""
        logger.info("å¼€å§‹è¿è¡Œæ‘©ç‚¹ä¼—ç­¹çˆ¬è™«")
        
        all_projects = []
        
        try:
            for page_num in range(1, self.config.MAX_PAGES + 1):
                logger.info(f"æ­£åœ¨å¤„ç†ç¬¬ {page_num} é¡µ...")
                
                page_url = f"{self.config.BASE_URL}{page_num}"
                html = self.make_request(page_url)
                
                if not html:
                    logger.warning(f"ç¬¬ {page_num} é¡µè·å–å¤±è´¥ï¼Œè·³è¿‡")
                    continue
                
                projects = self.parse_listing_page(html)
                all_projects.extend(projects)
                
                self.stats.pages_processed += 1
                
                logger.info(f"ç¬¬ {page_num} é¡µå¤„ç†å®Œæˆï¼Œæ‰¾åˆ° {len(projects)} ä¸ªé¡¹ç›®")
                
                # å®šæœŸä¿å­˜
                if page_num % self.config.SAVE_INTERVAL == 0:
                    self.save_data(all_projects, f"intermediate_page_{page_num}")
                
                # æ·»åŠ å»¶è¿Ÿé¿å…è¿‡äºé¢‘ç¹çš„è¯·æ±‚
                time.sleep(random.uniform(1, 3))
            
            # æœ€ç»ˆä¿å­˜
            if all_projects:
                self.save_data(all_projects, "final")
                logger.info(f"çˆ¬å–å®Œæˆï¼å…±å¤„ç† {len(all_projects)} ä¸ªé¡¹ç›®")
            else:
                logger.warning("æœªè·å–åˆ°ä»»ä½•é¡¹ç›®æ•°æ®")
            
            # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
            self.print_stats()
            
            return True
            
        except Exception as e:
            logger.error(f"çˆ¬è™«è¿è¡Œå¤±è´¥: {e}")
            self.stats.record_error("runtime_error", str(e))
            return False
    
    def save_data(self, projects: List[Dict[str, Any]], suffix: str = ""):
        """ä¿å­˜æ•°æ®åˆ°å¤šç§æ ¼å¼"""
        if not projects:
            return
        
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        base_filename = f"modian_projects_{timestamp}"
        if suffix:
            base_filename += f"_{suffix}"
        
        # ä¿å­˜ä¸ºJSON
        json_file = self.config.OUTPUT_DIR / f"{base_filename}.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(projects, f, ensure_ascii=False, indent=2)
        logger.info(f"æ•°æ®å·²ä¿å­˜ä¸ºJSON: {json_file}")
        
        # ä¿å­˜ä¸ºCSV
        csv_file = self.config.OUTPUT_DIR / f"{base_filename}.csv"
        if projects:
            df = pd.DataFrame(projects)
            df.to_csv(csv_file, index=False, encoding='utf-8-sig')
            logger.info(f"æ•°æ®å·²ä¿å­˜ä¸ºCSV: {csv_file}")
        
        # ä¿å­˜ä¸ºExcelï¼ˆç®€åŒ–ç‰ˆï¼‰
        excel_file = self.config.OUTPUT_DIR / f"{base_filename}.xlsx"
        try:
            df = pd.DataFrame(projects)
            df.to_excel(excel_file, index=False, engine='openpyxl')
            logger.info(f"æ•°æ®å·²ä¿å­˜ä¸ºExcel: {excel_file}")
        except Exception as e:
            logger.error(f"ä¿å­˜Excelæ–‡ä»¶å¤±è´¥: {e}")
    
    def print_stats(self):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        stats = self.stats.get_summary()
        
        print("\n" + "="*60)
        print("ğŸ“Š çˆ¬è™«è¿è¡Œç»Ÿè®¡")
        print("="*60)
        print(f"è¿è¡Œæ—¶é—´: {stats['elapsed_time']:.2f} ç§’")
        print(f"å¤„ç†é¡µé¢: {stats['pages_processed']}")
        print(f"æ€»è¯·æ±‚æ•°: {stats['total_requests']}")
        print(f"æˆåŠŸè¯·æ±‚: {stats['successful_requests']}")
        print(f"å¤±è´¥è¯·æ±‚: {stats['failed_requests']}")
        print(f"æˆåŠŸç‡: {stats['success_rate']:.1f}%")
        print(f"å‘ç°é¡¹ç›®: {stats['projects_found']}")
        print(f"å¹³å‡æ¯é¡µè€—æ—¶: {stats['avg_time_per_page']:.2f} ç§’")
        
        if stats['errors']:
            print("\né”™è¯¯ç»Ÿè®¡:")
            for error_type, messages in stats['errors'].items():
                print(f"  {error_type}: {len(messages)} æ¬¡")
        
        print("="*60)

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¯åŠ¨æ¡Œæ¸¸å¸‚åœºè°ƒç ”å·¥å…· - å¢å¼ºç‰ˆ")
    
    # åˆ›å»ºé…ç½®
    config = ModianSpiderConfig()
    
    # åˆ›å»ºçˆ¬è™«å®ä¾‹
    spider = ModianSpider(config)
    
    # è¿è¡Œçˆ¬è™«
    success = spider.run()
    
    if success:
        print("âœ… çˆ¬è™«è¿è¡Œå®Œæˆ")
    else:
        print("âŒ çˆ¬è™«è¿è¡Œå¤±è´¥")
    
    return success

if __name__ == "__main__":
    main()
